Topic,Paper_Title,Paper_Authors,Paper_Abstract,Dataset_List,Label_Dataset,MainId,Dataset_Name,Dataset_Authors,Dataset_Description
Entity Matching,"ANALISIS ENTITY MATCHING PADA DATASET SMARTPHONE MENGGUNAKAN METODE SIF, RNN, ATTENTION, DAN HYBRID","Rahmat Hidayat, Rivanda Putra Pratama, Nur Aini Rakhmawati","Penerapan teknologi informasi saat ini berdampak pada kecepatan
dan efektivitas suatu perusahaan atau masyarakat. Perkembangan dan
kecepatan data saat ini sangat krusial, perusahaan terus berupaya untuk
mempercepat analisis data perusahaan mereka. Dalam pelaksanaan
pengolahan data, entity matching berperan mencocokkan dua entitas. Masalah
dengan entity matching adalah bahwa kecocokan dalam pengenal unik
terdistribusi jarang terjadi dan sering kali terkait dengan masalah privasi.
Oleh karena itu diperlukan langkah untuk mencocokkan dua entitas yang
sama yaitu dengan menggunakan deep learning. Deepmatcher adalah paket
Python yang didasarkan pada arsitektur model Deep learning yang dianggap
berfungsi baik dengan pencocokan entitas. Tujuan dari penelitian ini adalah
untuk mengimplementasikan deepmatcher pada entity matching melalui
pencocokan antara dua dataset smartphone dengan menggunakan model SIF,
RNN, attention, dan hybrid. Hasil pengujian dengan semua model rata-rata
akurat. Model attention dan hybrid cocok untuk proses pelatihan model pada
dataset smartphone karena masing-masing memiliki nilai F1 terbesar yaitu
82,93.",Entity Matching on Dataset Smartphone,0,1,Entity Matching on Dataset Smartphone,"Rahmat Hidayat, Rivanda Putra Pratama, Nur Aini Rakhmawati
","Entity Matching on Dataset Smartphone

"
Entity Matching,Implementasi Deep Learning untuk Entity Matching pada Dataset Obat (Studi Kasus K24 dan Farmaku),"Rahmat Hidayat, Rivanda Putra Pratama, Nur Aini Rakhmawati, Nisrina Fadhilah Fano, Adam Akbar"," Data processing speed in companies is important to speed up their analysis. Entity matching is a computational process that companies can perform in data processing. In conducting data processing, entity matching plays a role in determining two different data but referring to the same entity. Entity matching problems arise when the dataset used in the comparison is large. The deep learning concept is one of the solutions in dealing with entity matching problems. DeepMatcher is a python package based on a deep learning model architecture that can solve entity matching problems. The purpose of this study was to determine the matching between the two datasets with the application of DeepMatcher in entity matching using drug data from farmaku.com and k24klik.com. The comparison model used is the Hybrid model. Based on the test results, the Hybrid model produces accurate numbers, so that the entity matching used in this study runs well. The best accuracy value of the 10th training with an F1 value of 30.30, a precision value of 17.86, and a recall value of 100.

",Entity Matching on Dataset Smartphone,0,0, ,,
Entity Matching,Profiling Entity Matching Benchmark Tasks,"Anna Primpeli, Christian Bizer","Entity matching is a central task in data integration which has been researched for decades. Over this time, a wide range of benchmark tasks for evaluating entity matching methods has been developed. This resource paper systematically complements, profiles, and compares 21 entity matching benchmark tasks. In order to better understand the specific challenges associated with different tasks, we define a set of profiling dimensions which capture central aspects of the matching tasks. Using these dimensions, we create groups of benchmark tasks having similar characteristics. Afterwards, we assess the difficulty of the tasks in each group by computing baseline evaluation results using standard feature engineering together with two common classification methods. In order to enable the exact reproducibility of evaluation results, matching tasks need to contain exactly defined sets of matching and non-matching record pairs, as well as a fixed development and test split. As this is not the case for some widely-used benchmark tasks, we complement these tasks with fixed sets of non-matching pairs, as well as fixed splits, and provide the resulting development and test sets for public download. By profiling and complementing the benchmark tasks, we support researchers to select challenging as well as diverse tasks and to compare matching systems on clearly defined grounds.","Amazon-Google, Augmented Version, Fixed Splits",1,1,"Amazon-Google, Augmented Version, Fixed Splits","Anna Primpeli, Christian Bizer
","An augmented version of the amazon-google products dataset for benchmarking entity matching/record linkage methods found at:
https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution"">https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolutio...
The augmented version adds a fixed set of non-matching pairs to the original dataset. In addition, fixed splits for training, validation and testing as well as their corresponding feature vectors are provided. The feature vectors are built using data type specific similarity metrics.
The dataset contains 1,363 records describing products deriving from amazon which are matched against 3,226 product records from google. The gold standards have manual annotations for 1,298 matching and 6,306 non-matching pairs. The total number of attributes used to decribe the product records are 4 while the attribute density is 0.75.
The augmented dataset enhances the reproducibility of matching methods and the comparability of matching results.
The dataset is part of the CompERBench repository which provides 21 complete benchmark tasks for entity matching for public download:
http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"">http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"
Entity Matching,Graph-Boosted Active Learning for Multi-source Entity Resolution,"Anna Primpeli, Christian Bizer","Supervised entity resolution methods rely on labeled record pairs for learning matching patterns between two or more data sources. Active learning minimizes the labeling effort by selecting informative pairs for labeling. The existing active learning methods for entity resolution all target two-source matching scenarios and ignore signals that only exist in multi-source settings, such as the Web of Data. In this paper, we propose ALMSER, a graph-boosted active learning method for multi-source entity resolution. To the best of our knowledge, ALMSER is the first active learning-based entity resolution method that is especially tailored to the multi-source setting. ALMSER exploits the rich correspondence graph that exists in multi-source settings for selecting informative record pairs. In addition, the correspondence graph is used to derive complementary training data. We evaluate our method using five multi-source matching tasks having different profiling characteristics. The experimental evaluation shows that leveraging graph signals leads to improved results over active learning methods using margin-based and committee-based query strategies in terms of F1 score on all tasks.","Amazon-Google, Augmented Version, Fixed Splits",1,0, ,,
Entity Matching,Frost: Benchmarking and Exploring Data Matching Results,"Martin Graf, Lukas Laskowski, Florian Papsdorf, Florian Sold, Roland Gremmelspacher, Felix Naumann, Fabian Panse","""Bad"" data has a direct impact on 88% of companies, with the average company losing 12% of its revenue due to it. Duplicates - multiple but different representations of the same real-world entities - are among the main reasons for poor data quality, so finding and configuring the right deduplication solution is essential. Existing data matching benchmarks focus on the quality of matching results and neglect other important factors, such as business requirements. Additionally, they often do not support the exploration of data matching results. To address this gap between the mere counting of record pairs vs. a comprehensive means to evaluate data matching solutions, we present the Frost platform. It combines existing benchmarks, established quality metrics, cost and effort metrics, and exploration techniques, making it the first platform to allow systematic exploration to understand matching results. Frost is implemented and published in the open-source application Snowman, which includes the visual exploration of matching results.","Amazon-Google, Augmented Version, Fixed Splits",1,0, ,,
Entity Matching,Machamp: A Generalized Entity Matching Benchmark,"Jin Wang, Yuliang Li, Wataru Hirota","Entity Matching (EM) refers to the problem of determining whether two different data representations refer to the same real-world entity. It has been a long-standing interest of the data management community and many efforts have been paid in creating benchmark tasks as well as in developing advanced matching techniques. However, existing benchmark tasks for EM are limited to the case where the two data collections of entities are structured tables with the same schema. Meanwhile, the data collections for matching could be structured, semi-structured, or unstructured in real-world scenarios of data science. In this paper, we come up with a new research problem -- Generalized Entity Matching to satisfy this requirement and create a benchmark Machamp for it. Machamp consists of seven tasks having diverse characteristics and thus provides good coverage of use cases in real applications. We summarize existing EM benchmark tasks for structured tables and conduct a series of processing and cleaning efforts to transform them into matching tasks between tables with different structures. Based on that, we further conduct comprehensive profiling of the proposed benchmark tasks and evaluate popular entity matching approaches on them. With the help of Machamp, it is the first time that researchers can evaluate EM techniques between data collections with different structures.","Amazon-Google, Augmented Version, Fixed Splits",1,0, ,,
Entity Matching,SocialLink: exploiting graph embeddings to link DBpedia entities to Twitter profiles,"Yaroslav Nechaev, Francesco Corcoglioniti, Claudio Giuliano ","SocialLink is a project designed to match social media profiles on Twitter to corresponding entities in DBpedia. Built to bridge the vibrant Twitter social media world and the Linked Open Data cloud, SocialLink enables knowledge transfer between the two, both assisting Semantic Web practitioners in better harvesting the vast amounts of information available on Twitter and allowing leveraging of DBpedia data for social media analysis tasks. In this paper, we further extend the original SocialLink approach by exploiting graph-based features based on both DBpedia and Twitter, represented as graph embeddings learned from vast amounts of unlabeled data. The introduction of such new features required to redesign our deep neural network-based candidate selection algorithm and, as a result, we experimentally demonstrate a significant improvement of the performances of SocialLink.",SocialLink,2,1,SocialLink,"Yaroslav Nechaev, Francesco Corcoglioniti,Claudio Giuliano
","We present SocialLink, a publicly available Linked Open Data dataset that matches social media accounts on Twitter to the corresponding entities in multiple language chapters of DBpedia. By effectively bridging the Twitter social media world and the Linked Open Data cloud, SocialLink enables knowledge transfer between the two: on the one hand, it supports Semantic Web practitioners in better harvesting the vast amounts of valuable, up-to-date information available in Twitter; on the other hand, it permits Social Media researchers to leverage DBpedia data when processing the noisy, semi-structured data of Twitter."
Entity Matching,A gentle introduction to deep learning for graphs,"Davide Bacciu, Federico Errica, Alessio Micheli, Marco Podda","The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field.",SocialLink,2,0,,,
Entity Matching,Resolving data sparsity and cold start problem in collaborative filtering recommender system using Linked Open Data,"Senthilselvan Natarajan,  Subramaniyaswamy Vairavasundaram, Sivaramakrishnan Natarajan, Amir H.Gandomi","The web contains a huge volume of data, and it's populating every moment to the point that human beings cannot deal with the vast amount of data manually or via traditional tools. Hence an advanced tool is required to filter such massive data and mine the valuable information. Recommender systems are among the most excellent tools for such a purpose in which collaborative filtering is widely used. Collaborative filtering (CF) has been extensively utilized to offer personalized recommendations in electronic business and social network websites. In that, matrix factorization is an efficient technique; however, it depends on past transactions of the users. Hence, there will be a data sparsity problem. Another issue with the collaborative filtering method is the cold start issue, which is due to the deficient information about new entities. A novel method is proposed to overcome the data sparsity and the cold start problem in CF. For cold start issue, Recommender System with Linked Open Data (RS-LOD) model is designed and for data sparsity problem, Matrix Factorization model with Linked Open Data is developed (MF-LOD). A LOD knowledge base “DBpedia” is used to find enough information about new entities for a cold start issue, and an improvement is made on the matrix factorization model to handle data sparsity. Experiments were done on Netflix and MovieLens datasets show that our proposed techniques are superior to other existing methods, which mean recommendation accuracy is improved.",SocialLink,2,0,,,
Entity Matching,Type Prediction Combining Linked Open Data and Social Media,"Yaroslav Nechaev, Francesco Corcoglioniti, Claudio Giuliano","Linked Open Data (LOD) and social media often contain the representations of the same real-world entities, such as persons and organizations. These representations are increasingly interlinked, making it possible to combine and leverage both LOD and social media data in prediction problems, complementing their relative strengths: while LOD knowledge is highly structured but also scarce and obsolete for some entities, social media data provide real-time updates and increased coverage, albeit being mostly unstructured. In this paper, we investigate the feasibility of using social media data to perform type prediction for entities in a LOD knowledge graph. We discuss how to gather training data for such a task, and how to build an efficient domain-independent vector representation of entities based on social media data. Our experiments on several type prediction tasks using DBpedia and Twitter data show the effectiveness of this representation, both alone and combined with knowledge graph-based features, suggesting its potential for ontology population.",SocialLink,2,0,,,
Entity Matching,Predicting Emoji Exploiting Multimodal Data: FBK Participation in ITAmoji Task,"Andrei Catalin Coman, Yaroslav Nechaev, Giacomo Zara","In this paper, we present our approach that has won the ITAmoji task of
the 2018 edition of the EVALITA evaluation campaign1
. ITAmoji is a classification task for predicting the most probable emoji (a total of 25 classes) to go
along with the target tweet written by a
given person in Italian. We demonstrate
that using only textual features is insufficient to achieve reasonable performance
levels on this task and propose a system
that is able to benefit from the multimodal
information contained in the training set,
enabling significant F1 gains and earning
us the first place in the final ranking.",SocialLink,2,0,,,
Entity Matching,Improving link prediction in social networks using local and global features: a clustering-based approach,"S. Ghasemi, A. Zarei ","Link prediction problem has increasingly become prominent in many domains such as social network analyses, bioinformatics experiments, transportation networks, criminal investigations and so forth. A variety of techniques has been developed for link prediction problem, categorized into (1) similarity-based approaches which study a set of features to extract similar nodes; (2) learning-based approaches which extract patterns from the input data; (3) probabilistic statistical approaches which optimize a set of parameters to establish a model which can best compute formation probability. However, existing literatures lack approaches which utilize strength of each approach by integrating them to achieve a much more productive one. To tackle the link prediction problem, we propose an approach based on the combination of first and second group methods; the existing studied works use just one of these categories. Our two-phase developed method firstly determines new features related to the position and dynamic behavior of nodes, which enforce the approach more efficiency compared to approaches using mere measures. Then, a subspace clustering algorithm is applied to group social objects based on the computed similarity measures which differentiate the strength of clusters; basically, the usage of local and global indices and the clustering information plays an imperative role in our link prediction process. Some extensive experiments held on real datasets including Facebook, Brightkite and HepTh indicate good performances of our proposal method. Besides, we have experimentally verified our approach with some previous techniques in the area to prove the supremacy of ours.

",SocialLink,2,0,,,
Entity Matching,Catastrophic Forgetting in Deep Graph Networks: an Introductory Benchmark for Graph Classification,"Antonio Carta, Andrea Cossu, Federico Errica, Davide Bacciu","In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments.",SocialLink,2,0,,,
Entity Matching,"Inferring Degree of Localization of Twitter Persons and Topics through Time, Language, and Location Features","Panasyuk, Aleksey Valeriy.","Identifying authoritative influencers related to a geographic area (geo-influencers) can
aid content recommendation systems and local expert finding. This thesis addresses
this important problem using Twitter data.
A geo-influencer is identified via the locations of its followers. On Twitter, due to
privacy reasons, the location reported by followers is limited to profile via a textual
string or messages with coordinates. However, this textual string is often not possible
to geocode and less than 1% of message traffic provides coordinates. First, the error
rates associated with Google’s geocoder are studied and a classifier is built that gives
a warning for self-reported locations that are likely incorrect. Second, it is shown that
city-level geo-influencers can be identified without geocoding by leveraging the power
of Google search and follower-followee network structure. Third, we illustrate that
the global vs. local influencer, at the timezone level, can be identified using a classifier
using the temporal features of the followers. For global influencers, spatiotemporal
analysis helps understand the evolution of their popularity over time. When applied
over message traffic, the approach can differentiate top trending topics and persons in
different geographical regions. Fourth, we constrain a timezone to a set of possible
countries and use language features for training a high-level geocoder to further
localize an influencer’s geographic area. Finally, we provide a repository of geoinfluencers for applications related to content recommendation. The repository can
be used for filtering influencers based on their audience’s demographics related to
location, time, language, gender, and ethnicity.",SocialLink,2,0,,,
Entity Matching,A Systematic Review on User Identity Linkage across Online Social Networks,Rishabh Kaushal,"Online Social Networks (OSNs) present a wide variety of information
to their users in terms of different types of content (text, video, pictures)
and different kinds of network (friends). To avail this diverse information,
users register and maintain their accounts (hereafter referred to as user
identities) across multiple OSNs. This situation leads to the problem of
User Identity Linkage (UIL) across multiple OSNs. More formally, given
a user identity on one OSN (referred to as source network), the goal is
to find user identity on another OSN (referred to as target network). In
this report, we present a systematic review of issues related to the UIL
problem from different viewpoints. Collecting ground truth user identities across multiple OSNs that belong to the same person is the first
step in the study of the UIL problem. We refer to the collected identities
belonging to the same person as linked user identities. We perform a detailed study and comparative evaluation of prior data collection methods
that collect such identities by leveraging user behaviors. Once we collect linked user identities, typically the next step is to formulate the UIL
problem as a machine learning driven classification task. Prior works compute hand-crafted features derived from user behaviors based on settings
on user profile, content posted by users and friend network maintained
by users. Subsequently, they build supervised, semi-supervised and unsupervised machine learning models on these features. Recent trend to
solve the UIL problem is to leverage graph representation techniques and
construct embedding vectors corresponding to user nodes in the social
network graph, thereby automating the feature computation task. In this
work, we perform a detailed study of both conventional machine learning
based approaches and more recent graph representation based approaches.
Linking users across OSNs have implications on many other problems in
social networks. For instance, given that the UIL problem helps in building a comprehensive user behavior expressed across OSNs, therefore, it
naturally helps in recommendation systems, targeted advertisements, link
prediction across OSNs, and many other applications. From a user’s privacy perspective, the linkage of user identities would impact particularly
those users who segregate their personal and professional activities across
different OSNs. Therefore, in the last part of this report, we present a
discussion on implications, and applications that benefit from the solution
to the UIL problem.",SocialLink,2,0,,,
Entity Matching,Results of SemTab 2020,"Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen, Ernesto Jiménez-Ruiz & Kavitha Srinivas","SemTab 2020 was the second edition of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching, successfully collocated
with the 19th International Semantic Web Conference (ISWC) and the 15th Ontology Matching (OM) Workshop. SemTab provides a common framework to
conduct a systematic evaluation of state-of-the-art systems.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,1,SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,"Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen, Ernesto Jiménez-Ruiz & Kavitha Srinivas","Data Sets from the ISWC 2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching.

For details, see: http://www.cs.ox.ac.uk/isg/challenges/sem-tab/

Note on License: This data includes data from the following sources. Refer to each source for license details:
- Wikidata https://www.wikidata.org/"
Entity Matching,JenTab: A Toolkit for Semantic Table Annotations,"Nora Abdelmageed, Sirko Schindler","Tables are a ubiquitous source of structured information.
However, their use in automated pipelines is severely affected by conflicts in naming and issues like missing entries or spelling mistakes. The
Semantic Web has proven itself a valuable tool in dealing with such issues, allowing the fusion of data from heterogeneous sources. Its usage
requires the annotation of table elements like cells and columns with
entities from existing knowledge graphs. Automating this semantic annotation, especially for noisy tabular data, remains a challenge, though.
JenTab is a modular system to map table contents onto large knowledge
graphs like Wikidata. It starts by creating an initial pool of candidates
for possible annotations. Over multiple iterations context information is
then used to eliminate candidates until, eventually, a single annotation is
identified as the best match. Based on the SemTab2020 dataset, this paper presents various experiments to evaluate the performance of JenTab.
This includes a detailed analysis of individual components and of the
impact different approaches. Further, we evaluate JenTab against other
systems and demonstrate its effectiveness in table annotation tasks.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Generating Explainable Abstractions for Wikidata Entities,Nicholas Klein. Filip Ilievski. Pedro Szekely,"The large coverage and quality of the Wikidata knowledge graph make it suitable for usage in downstream applications, such as entity summarization, entity linking, and question answering. Yet, most retrieval and similarity-based methods for Wikidata make limited use of its semantics, and lose the link between the rich structure in Wikidata and the decision-making algorithm. In this paper, we investigate how to define abstractive representations (profiles) of Wikidata entities. We propose a scalable method that can produce profiles for Wikidata entities based on salient labels associated with their types. We represent the resulting profiles as a graph, and compute profile embeddings. Our empirical analysis shows that the profiles can capture similarity competitively to baselines, but excel in terms of explainability. On the task of neural entity linking in tables, the profiles outperform all baselines in terms of accuracy, whereas their human-readable representation clearly explains the source of improvement. We make our code and data available to facilitate novel use cases based on the Wikidata profiles.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,"Creation, Enrichment and Application of Knowledge Graphs",Simon Gottschalk,"The world is in constant change, and so is the knowledge about it. Knowledge-based systems - for example, online encyclopedias, search engines and virtual assistants - are thus faced with the constant challenge of collecting this knowledge and beyond that, to understand it and make it accessible to their users. Only if a knowledge-based system is capable of this understanding - that is, it is capable of more than just reading a collection of words and numbers without grasping their semantics - it can recognise relevant information and make it understandable to its users. The dynamics of the world play a unique role in this context: Events of various kinds which are relevant to different communities are shaping the world, with examples ranging from the coronavirus pandemic to the matches of a local football team. Vital questions arise when dealing with such events: How to decide which events are relevant, and for whom? How to model these events, to make them understood by knowledge-based systems? How is the acquired knowledge returned to the users of these systems? A well-established concept for making knowledge understandable by knowledge-based systems are knowledge graphs, which contain facts about entities (persons, objects, locations, ...) in the form of graphs, represent relationships between these entities and make the facts understandable by means of ontologies. This thesis considers knowledge graphs from three different perspectives: (i) Creation of knowledge graphs: Even though the Web offers a multitude of sources that provide knowledge about the events in the world, the creation of an event-centric knowledge graph requires recognition of such knowledge, its integration across sources and its representation. (ii) Knowledge graph enrichment: Knowledge of the world seems to be infinite, and it seems impossible to grasp it entirely at any time. Therefore, methods that autonomously infer new knowledge and enrich the knowledge graphs are of particular interest. (iii) Knowledge graph interaction: Even having all knowledge of the world available does not have any value in itself; in fact, there is a need to make it accessible to humans. Based on knowledge graphs, systems can provide their knowledge with their users, even without demanding any conceptual understanding of knowledge graphs from them. For this to succeed, means for interaction with the knowledge are required, hiding the knowledge graph below the surface. In concrete terms, I present EventKG - a knowledge graph that represents the happenings in the world in 15 languages - as well as Tab2KG - a method for understanding tabular data and transforming it into a knowledge graph. For the enrichment of knowledge graphs without any background knowledge, I propose HapPenIng, which infers missing events from the descriptions of related events. I demonstrate means for interaction with knowledge graphs at the example of two web-based systems (EventKG+TL and EventKG+BT) that enable users to explore the happenings in the world as well as the most relevant events in the lives of well-known personalities.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,GitTables: A Large-Scale Corpus of Relational Tables,"Madelon Hulsebos, Çağatay Demiralp, Paul Groth","The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of 1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in GitTables with semantic types, hierarchical relations and descriptions from this http URL and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results on par with human annotations. We present three applications of GitTables, demonstrating its value for learned semantic type detection models, schema completion methods, and benchmarks for table-to-KG matching, data search, and preparation. We make the corpus and code available at this https URL.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,A Framework for Quality Assessment of Semantic Annotations of Tabular Data,"Roberto Avogadro, Marco Cremaschi, Ernesto Jiménez-Ruiz, Anisa Rula","Much information is conveyed within tables, which can be semantically annotated by humans or (semi)automatic approaches. Nevertheless, many applications cannot take full advantage of semantic annotations because of the low quality. A few methodologies exist for the quality assessment of semantic annotation of tabular data, but they do not automatically assess the quality as a multidimensional concept through different quality dimensions. The quality dimensions are implemented in STILTool 2, a web application to automate the quality assessment of the annotations. The evaluation is carried out by comparing the quality of semantic annotations with gold standards. The work presented here has been applied to at least three use cases. The results show that our approach can give us hints about the quality issues and how to address them.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Semantic Concept Annotation for Tabular Data,"Udayan Khurana, Sainyam Galhotra","Detecting semantic concept of columns in tabular data is of particular interest to many applications ranging from data integration, cleaning, search to feature engineering and model building in machine learning. Recently, several works have proposed supervised learning-based or heuristic pattern-based approaches to semantic type annotation. Both have shortcomings that prevent them from generalizing over a large number of concepts or examples. Many neural network based methods also present scalability issues. Additionally, none of the known methods works well for numerical data. We propose C2, a column to concept mapper that is based on a maximum likelihood estimation approach through ensembles. It is able to effectively utilize vast amounts of, albeit somewhat noisy, openly available table corpora in addition to two popular knowledge graphs to perform effective and efficient concept prediction for structured data. We demonstrate the effectiveness of C2 over available techniques on 9 datasets, the most comprehensive comparison on this topic so far.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,A Framework for Automatically Interpreting Tabular Data at Orange,"Yoan Chabot, Pierre Monnin, Frédéric Deuzé, Viet-Phi Huynh, Thomas Labbé, Jixiong Liu, Raphaël Troncy","Large parts of knowledge of companies are encoded in tabular data. Being able
to interpret such data is key to increase business efficiency and to propose innovative services, and Orange is no exception. With more than 140,000 employees
worldwide and a heterogeneous client portfolio, Orange produces a phenomenal
amount of tabular data every day. These tables are viscerally embedded in internal services and products (e.g., network logs, multimedia catalogs). Hence, they
are a source to discover new knowledge. Although this encourages the development of efficient tools to process them, several issues are negatively impacting
their use. First, the volume curse makes difficult to identify the right dataset for
a given use case. Then, the knowledge gap between data producers/consumers
is exacerbated by our language footprint (seven main languages), the heterogeneous tools producing various table formats, and the experience/jobs of employees leading to similar concepts being expressed by different terms across
tables.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Demonstration of MTab: Tabular Data Annotation with Knowledge Graphs,"Phuc Nguyen, Ikuya Yamada, Natthawut Kertkeidkachorn,Ryutaro Ichise, and Hideaki Takeda","This paper presents a demonstration of MTab, a tabular
data annotation toolkit with knowledge graphs: Wikidata, Wikipedia,
and DBpedia. MTab is the best performance system for all semantic
annotation tasks at the Semantic Web Challenges on tabular data to
knowledge graph matching SemTab 2019 and SemTab 2020. This paper introduces MTab’s public APIs capable of structural and semantic
annotations for tabular data. We also provide a graphical interface to visualize the annotation results. The tool supports multilingual tables and
could process many table formats such as Excel, CSV, TSV, markdown
tables, or a pasted table content. MTab’s repository is publicly available
at https://github.com/phucty/mtab_tool",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Accelerating Entity Lookups in Knowledge Graphs Through Embeddings,"Ghadeer Abuoda, Saravanan Thirumuruganathan, Ashraf Aboulnaga","Tabular data is widespread on the web and in
enterprise data lakes. Recently, there has been increasing interest
in developing algorithms for matching tabular data with knowledge graphs. This involves learning correspondences between
tabular entities such as cells, rows, and columns and entities
in the knowledge graph. Such semantic annotation of tabular
entities has numerous applications such as entity disambiguation,
knowledge graph expansion, error detection and repair in tabular
data, and more. A key first step for all these applications is the
lookup function that matches a query string to a candidate set
of knowledge graph entities. Despite the importance of entity
lookup, current implementations are not optimized, not robust
to misspellings, and ignore semantic relationships.
To address these problems, we represent each entity as an
embedding – a compact vector representation that is cognizant of
syntactic and semantic similarities and supports fast lookup. We
propose, EMBLOOKUP, a novel and efficient approach for learning
such an embedding. EMBLOOKUP is based on deep metric learning with triplet loss and supports accurate and efficient lookup of
knowledge graph entities. We conduct extensive experiments that
demonstrate that EMBLOOKUP achieves 1-2 orders of magnitude
speedup while being tolerant to many types of errors in the query
and data. We demonstrate the generality of EMBLOOKUP over
diverse application scenarios in semantic table annotation, entity
disambiguation, and data repair.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Advanced Data Processing for Better Machine Learning on Graphs,Ghadeer Abuoda,"Machine learning on graphs has recently received much attention from academia
and industry, and there are many advances in this field. We observe that there
are many ways to improve the effectiveness and efficiency of machine learning
on graphs by adopting advanced data processing techniques, and we show how
this can be applied in link prediction, integrating knowledge graphs in machine
learning applications, and entity lookup in knowledge graphs.
The first contribution of this thesis shows how recent advances in graph mining,
in particular the ability to efficiently extract motifs from large graphs, can be used
to improve the accuracy of link prediction. We show that using motifs of size up
to five as features for link prediction results in up to 10 percentage points increase
in accuracy over prior methods.
The second contribution of the thesis is RDFFrames, a programming framework for seamlessly integrating knowledge graphs into machine learning applications. RDFFrames provides the user with a programming library in Python that
enables extracting tabular data (e.g., dataframes) from a graph database. RDFFrames translates calls to this library to efficient queries on a graph database,
executes these queries, and returns the results as tabular data.
The final contribution of the thesis is EmbLookup, a framework for entity
lookup in knowledge graphs. EmbLookup provides a scalable, compact, errortolerant method to search large-scale knowledge graphs for entities matching a
lookup string. EmbLookup is based on an approach known as deep metric
representation learning with triplet loss, and we show that it increases lookup
performance and accuracy in several applications.",SemTab 2020: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching Data Sets,3,0,,,
Entity Matching,Using the Semantic Web as a Source of Training Data,"Christian Bizer, Anna Primpeli, Ralph Peeters ","Deep neural networks are increasingly used for tasks such as entity resolution, sentiment analysis, and information extraction. As the methods are rather training data hungry, it is necessary to use large training sets in order to enable the methods to play their strengths. Millions of websites have started to annotate structured data within HTML pages using the schema.org vocabulary. Popular types of entities that are annotated are products, reviews, events, people, hotels, and other local businesses [12]. These semantic annotations are used by all major search engines to display rich snippets in search results. This is also the main driver behind the wide-scale adoption of the annotation techniques.
This article explores the potential of using semantic annotations from large numbers of websites as training data for supervised entity resolution, sentiment analysis, and information extraction methods. After giving an overview of the types of structured data that are available on the Semantic Web, we focus on the task of product matching in e‑commerce and explain how semantic annotations can be used to gather a large training dataset for product matching. The dataset consists of more than 20 million pairs of offers referring to the same products. The offers were extracted from 43 thousand e‑shops, that provide schema.org annotations including some form of product identifiers, such as manufacturer part numbers (MPNs), global trade item numbers (GTINs), or stock keeping units (SKUs). The dataset, which we offer for public download, is orders of magnitude larger than the Walmart-Amazon [7], Amazon-Google [10], and Abt-Buy [10] datasets that are widely used to evaluate product matching methods. We verify the utility of the dataset as training data by using it to replicate the recent result of Mudgal et al. [15] stating that embeddings and RNNs outperform traditional symbolic matching methods on tasks involving less structured data. After the case study on product data matching, we focus on sentiment analysis and information extraction and discuss how semantic annotations from the Web can be used as training data within both tasks.",Abt-Buy Dataset,4,1,Abt-Buy Dataset,"Anna Primpeli, Christian Bizer
","The Abt-Buy dataset for entity resolution derives from the online retailers Abt.com and Buy.com. The dataset contains 1081 entities from abt.com and 1092 entities from buy.com as well as a gold standard (perfect mapping) with 1097 matching record pairs between the two data sources. The common attributes between the two data sources are: product name, product description and product price.

The dataset was initially published in the repository of the Database Group of the University of Leipzig: https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution

To enable the reproducibility of the results and the comparability of the performance of different matchers on the Abt-Buy matching task, the dataset was split into fixed train, validation and test sets. The fixed splits are provided in the CompERBench repository:

http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"
Entity Matching,Using schema.org Annotations for Training and Maintaining Product Matcher,"Ralph Peeters, Anna Primpeli, Benedikt Wichtlhuber, Christian Bizer","Product matching is a central task within e-commerce applications such as price comparison portals and online market places. State-of-the-art product matching methods achieve F1 scores above 0.90 using deep learning techniques combined with huge amounts of training data (e.g > 100K pairs of offers). Gathering and maintaining such large training corpora is costly, as it implies labeling pairs of offers as matches or non-matches. Acquiring the ability to be good at product matching thus means a major investment for an e-commerce company. This paper shows that the manual labeling of training data for product matching can be replaced by relying exclusively on schema.org annotations gathered from the public Web. We show that using only schema.org data for training, we are able to achieve F1 scores between 0.92 and 0.95 depending on the product category. As new products appear everyday, it is important that matching models can be maintained with justifiable effort. In order to give practical advice on how to maintain matching models, we compare the performance of deep learning and traditional matching models on unseen products and experiment with different fine-tuning and re-training strategies for model maintenance, again using only schema.org annotations as training data. Finally, as using the public Web as distant supervision carries inherent noise, we evaluate deep learning and traditional matching models with regards to their label-noise resistance and show that deep learning is able to deal with the amounts of identifier-noise found in schema.org annotations.",Abt-Buy Dataset,4,0,,,
Entity Matching,Using Weak Supervision to Identify Long-Tail Entities for Knowledge Base Completion,"Yaser Oulabi, Christian Bizer","Deep neural networks are increasingly used
for tasks such as entity resolution, sentiment analysis,
and information extraction. As the methods are rather
training data hungry, it is necessary to use large training sets in order to enable the methods to play their
strengths.
Millions of websites have started to annotate structured data within HTML pages using the schema.org
vocabulary. Popular types of entities that are annotated
are products, reviews, events, people, hotels, and other
local businesses [11]. These semantic annotations are
used by all major search engines to display rich snippets
in search results. This is also the main driver behind the
wide-scale adoption of the annotation techniques.
This article explores the potential of using semantic
annotations from large numbers of websites as training
data for supervised entity resolution, sentiment analysis, and information extraction methods. After giving
an overview of the types of structured data that are
available on the Semantic Web, we focus on the task
of product matching in e-commerce and explain how
semantic annotations can be used to gather a large training dataset for product matching. The dataset consists of more than 20 million pairs of offers referring
to the same products. The offers were extracted from
43 thousand e-shops, that provide schema.org annotations including some form of product identifiers, such
as manufacturer part numbers (MPNs), global trade
item numbers (GTINs), or stock keeping units (SKUs).
The dataset, which we offer for public download, is orders of magnitude larger than the Walmart-Amazon [6],
Amazon-Google [9], and Abt-Buy [9] datasets that are
widely used to evaluate product matching methods. We
verify the utility of the dataset as training data by using it to replicate the recent result of Mugdal et al. [14]
stating that embeddings and RNNs outperform traditional symbolic matching methods on tasks involving
less structured data. After the case study on product
data matching, we focus on sentiment analysis and information extraction and discuss how semantic annotations from the Web can be used as training data within
both tasks.",Abt-Buy Dataset,4,0,,,
Entity Matching,Catalog Integration of Heterogeneous and Volatile Product Data,"Oliver Schmidts, Bodo Kraft, Marvin Winkens, Albert Zündorf","The integration of frequently changing, volatile product data from different manufacturers into a single catalog is a significant challenge for small and medium-sized e-commerce companies. They rely on timely integrating product data to present them aggregated in an online shop without knowing format specifications, concept understanding of manufacturers, and data quality. Furthermore, format, concepts, and data quality may change at any time. Consequently, integrating product catalogs into a single standardized catalog is often a laborious manual task. Current strategies to streamline or automate catalog integration use techniques based on machine learning, word vectorization, or semantic similarity. However, most approaches struggle with low-quality or real-world data. We propose Attribute Label Ranking (ALR) as a recommendation engine to simplify the integration process of previously unknown, proprietary tabular format into a standardized catalog for practitioners. We evaluate ALR by focusing on the impact of different neural network architectures, language features, and semantic similarity. Additionally, we consider metrics for industrial application and present the impact of ALR in production and its limitations.",Abt-Buy Dataset,4,0,,,
Entity Matching,An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining,"Ziqi Zhang, Xingyi Song","The Linked Open Data practice has led to a significant growth of structured data on the Web in the last decade. Such structured data describe real-world entities in a machine-readable way, and have created an unprecedented opportunity for research in the field of Natural Language Processing. However, there is a lack of studies on how such data can be used, for what kind of tasks, and to what extent they can be useful for these tasks. This work focuses on the e-commerce domain to explore methods of utilising such structured data to create language resources that may be used for product classification and linking. We process billions of structured data points in the form of RDF n-quads, to create multi-million words of product-related corpora that are later used in three different ways for creating of language resources: training word embedding models, continued pre-training of BERT-like language models, and training Machine Translation models that are used as a proxy to generate product-related keywords. Our evaluation on an extensive set of benchmarks shows word embeddings to be the most reliable and consistent method to improve the accuracy on both tasks (with up to 6.9 percentage points in macro-average F1 on some datasets). The other two methods however, are not as useful. Our analysis shows that this could be due to a number of reasons, including the biased domain representation in the structured data and lack of vocabulary coverage. We share our datasets and discuss how our lessons learned could be taken forward to inform future research in this direction.",Abt-Buy Dataset,4,0,,,
Entity Matching,Catalog Integration of Low-quality Product Data by Attribute Label Ranking,"Oliver Schmidts, Bodo Kraft, Marvin Winkens, Albert Zundorf ","The integration of product data from heterogeneous sources and manufacturers into a single catalog is often
still a laborious, manual task. Especially small- and medium-sized enterprises face the challenge of timely
integrating the data their business relies on to have an up-to-date product catalog, due to format specifications,
low quality of data and the requirement of expert knowledge. Additionally, modern approaches to simplify
catalog integration demand experience in machine learning, word vectorization, or semantic similarity that
such enterprises do not have. Furthermore, most approaches struggle with low-quality data. We propose
Attribute Label Ranking (ALR), an easy to understand and simple to adapt learning approach. ALR leverages
a model trained on real-world integration data to identify the best possible schema mapping of previously
unknown, proprietary, tabular format into a standardized catalog schema. Our approach predicts multiple
labels for every attribute of an input column. The whole column is taken into consideration to rank among
these labels. We evaluate ALR regarding the correctness of predictions and compare the results on real-world
data to state-of-the-art approaches. Additionally, we report findings during experiments and limitations of our
approach.",Abt-Buy Dataset,4,0,,,
Entity Matching,Augmenting cross-domain knowledge bases using web tables,Yaser Oulabi,"Cross-domain knowledge bases are increasingly used for a large variety of applications. As the usefulness of a knowledge base for many of these applications increases with its completeness, augmenting knowledge bases with new knowledge is an important task. A source for this new knowledge could be in the form of web tables, which are relational HTML tables extracted from the Web. This thesis researches data integration methods for cross-domain knowledge base augmentation from web tables. Existing methods have focused on the task of slot filling static data. We research methods that additionally enable augmentation in the form of slot filling time-dependent data and entity expansion. When augmenting knowledge bases using time-dependent web table data, we require time-aware fusion methods. They identify from a set of conflicting web table values the one that is valid given a certain temporal scope. A primary concern of time-aware fusion is therefore the estimation of temporal scope annotations, which web table data lacks. We introduce two time-aware fusion approaches. In the first, we extract timestamps from the table and its context to exploit as temporal scopes, additionally introducing approaches to reduce the sparsity and noisiness of these timestamps. We introduce a second time-aware fusion method that exploits a temporal knowledge base to propagate temporal scopes to web table data, reducing the dependence on noisy and sparse timestamps. Entity expansion enriches a knowledge base with previously unknown long-tail entities. It is a task that to our knowledge has not been researched before. We introduce the Long-Tail Entity Extraction Pipeline, the first system that can perform entity expansion from web table data. The pipeline works by employing identity resolution twice, once to disambiguate between entity occurrences within web tables, and once between entities created from web tables and existing entities in the knowledge base. In addition to identifying new long-tail entities, the pipeline also creates their descriptions according to the knowledge base schema. By running the pipeline on a large-scale web table corpus, we profile the potential of web tables for the task of entity expansion. We find, that given certain classes, we can enrich a knowledge base with tens and even hundreds of thousands new entities and corresponding facts. Finally, we introduce a weak supervision approach for long-tail entity extraction, where supervision in the form of a large number of manually labeled matching and non-matching pairs is substituted with a small set of bold matching rules build using the knowledge base schema. Using this, we can reduce the supervision effort required to train our pipeline to enable cross-domain entity expansion at web-scale. In the context of this research, we created and published two datasets. The Time-Dependent Ground Truth contains time-dependent knowledge with more than one million temporal facts and corresponding temporal scope annotations. It could potentially be employed for a large variety of tasks that consider the temporal aspect of data. We also built the Web Tables for Long-Tail Entity Extraction gold standard, the first benchmark for the task of entity expansion from web tables.",Abt-Buy Dataset,4,0,,,
Entity Matching,"Del mobile-first al data-first: schema.org, búsquedas zero-click y la incertidumbre sobre los asistentes de voz From mobile-first to data-first: Schema.org, zero-click searches and the uncertainty about voice assistants",Tomás Saorín; Juan-Antonio Pastor-Sánchez,"A reflection is made on the semantic marking from the point of view of the edition and the digital publication. The term data-first is coined, understood as the use of structured data in digital content to improve how search engines accurately understand the information they contain. The evolution and adoption of the Schema.org vocabulary as a de facto standard on the Web is reviewed, given its impact on web positioning and improved features of search results pages. Finally, considerations on the effect of structured data on direct answers, the controversial phenomenon of zero-click searches and on interaction through voice assistants are pointed out.",Abt-Buy Dataset,4,0,,,
Entity Matching,Performance Evaluation of the Semantic Web Reasoners,"U Sinha, S Tiwari","As the performance of semantic reasoners change significantly with regard to all included characteristics, and therefore requires assessment and evaluation before selecting an
appropriate reasoner for a given application. There are
number of inference engines like Pellet, FaCT++, Hermit,
RacerPro, KaON2, F-OWL and BaseVISor. Some of them
are reviewed and tested for few prebuilt ontologies. Paper
presents a performance evaluation and comparison of semantic reasoner for ontology of Health and Anatomy domain.
Reasoners are characterized based on reasoning method, reasoning algorithm, computational complexity, classification,
scalability, query and rule support.",Abt-Buy Dataset,4,0,,,
Entity Matching,MWPD2020: Semantic Web Challenge on Mining the Web of HTML-embedded Product Data,"Ziqi Zhang, Christian Bizer, Ralph Peeters, and Anna Primpeli","This paper gives an overview of the Semantic Web Challenge
on Mining the Web of HTML-embedded Product Data (MWPD2020)
which has been conducted as part of the International Semantic Web
Conference (ISWC2020). The challenge consists of two tasks: product
matching and product classification. In the first task, participants need
to identify offers for the same product originating from different websites.
The goal of the second task is to categorize offers from different websites
into the GS1 GPC product hierarchy. Six teams from the USA, China,
Japan, and Germany participated in the challenge. The winning system
in Task 1, PMap, achieved an F1 score of 86.05 using an ensemble of
transformer-based language models. Task 2 was won by team Rhinobird
achieving a weighted average F1 score of 88.62 using a BERT-based ensemble which considers the dependencies among different category levels.","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,1,"Web Data Commons Phones Dataset, Augmented Version, Fixed Splits","Anna Primpeli, Christian Bizer
","An augmented version of the wdc phones dataset for benchmarking entity matching/record linkage methods found at:
http://webdatacommons.org/productcorpus/index.html#toc4"">http://webdatacommons.org/productcorpus/index.html#toc4

The augmented version adds fixed splits for training, validation and testing as well as their corresponding feature vectors. The feature vectors are built using data type specific similarity metrics.
The dataset contains 447 records describing products deriving from 17 e-shops which are matched against a product catalog of 50 products. The gold standards have manual annotations for 258 matching and 22,092 non-matching pairs. The total number of attributes used to decribe the product records are 26 while the attribute density is 0.25.

The augmented dataset enhances the reproducibility of matching methods and the comparability of matching results.
The dataset is part of the CompERBench repository which provides 21 complete benchmark tasks for entity matching for public download:
http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"">http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"
Entity Matching,Dual-objective fine-tuning of BERT for entity matching," Christian Bizer, Ralph Peeters ","An increasing number of data providers have adopted shared numbering schemes such as GTIN, ISBN, DUNS, or ORCID numbers for
identifying entities in the respective domain. This means for data
integration that shared identifiers are often available for a subset
of the entity descriptions to be integrated while such identifiers are
not available for others. The challenge in these settings is to learn a
matcher for entity descriptions without identifiers using the entity
descriptions containing identifiers as training data. The task can
be approached by learning a binary classifier which distinguishes
pairs of entity descriptions for the same real-world entity from
descriptions of different entities. The task can also be modeled as a
multi-class classification problem by learning classifiers for identifying descriptions of individual entities. We present a dual-objective
training method for BERT, called JointBERT, which combines binary matching and multi-class classification, forcing the model to
predict the entity identifier for each entity description in a training
pair in addition to the match/non-match decision. Our evaluation
across five entity matching benchmark datasets shows that dualobjective training can increase the matching performance for seen
products by 1% to 5% F1 compared to single-objective Transformerbased methods, given that enough training data is available for both
objectives. In order to gain a deeper understanding of the strengths
and weaknesses of the proposed method, we compare JointBERT
to several other BERT-based matching methods as well as baseline
systems along a set of specific matching challenges. This evaluation shows that JointBERT, given enough training data for both
objectives, outperforms the other methods on tasks involving seen
products, while it underperforms for unseen products. Using a combination of LIME explanations and domain-specific word classes,
we analyze the matching decisions of the different deep learning
models and conclude that BERT-based models are better at focusing
on relevant word classes compared to RNN-based models.","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,Improving Hierarchical Product Classification using Domain-specific Language Modelling,"Alexander Brinkmann, Christian Bizer","In order to deliver a coherent user experience, product aggregators
such as market places or price portals integrate product offers from
many web shops into a single product categorization hierarchy.
Recently, transformer models have shown remarkable performance
on various NLP tasks. These models are pre-trained on huge crossdomain text corpora using self-supervised learning and fine-tuned
afterwards for specific downstream tasks. Research from other
application domains indicates that additional self-supervised pretraining using domain-specific text corpora can further increase
downstream performance without requiring additional task-specific
training data. In this paper, we first show that transformers outperform a more traditional fastText-based classification technique
on the task of assigning product offers from different web shops
into a product hierarchy. Afterwards, we investigate whether it is
possible to improve the performance of the transformer models by
performing additional self-supervised pre-training using different
corpora of product offers, which were extracted from the Common
Crawl. Our experiments show that by using large numbers of related product offers for masked language modelling, it is possible
to increase the performance of the transformer models by 1.22% in
wF1 and 1.36% in hF1 reaching a performance of nearly 89% wF1.","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,GaussianProductAttributes: Density-Based Distributed Representations for Products,"Hossein Ghodrati Noushahr, Jeremy Levesley, Samad Ahmadi, Evgeny Mirkes","Multivariate Gaussian probability distributions have been used as distributed representations for text. In comparison with traditional vector representations, these density-based representations are able to model uncertainty, inclusion and entailment. We present a model to learn such representations for products based on a public e-commerce dataset. We qualitatively analyse the properties of the proposed model and how the learned representations capture semantic relatedness, similarity and entailment between products and text.","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining,"Ziqi Zhang, Xingyi Song","The Linked Open Data practice has led to a significant growth of structured data on the Web in the last decade. Such structured data describe real-world entities in a machine-readable way, and have created an unprecedented opportunity for research in the field of Natural Language Processing. However, there is a lack of studies on how such data can be used, for what kind of tasks, and to what extent they can be useful for these tasks. This work focuses on the e-commerce domain to explore methods of utilising such structured data to create language resources that may be used for product classification and linking. We process billions of structured data points in the form of RDF n-quads, to create multi-million words of product-related corpora that are later used in three different ways for creating of language resources: training word embedding models, continued pre-training of BERT-like language models, and training Machine Translation models that are used as a proxy to generate product-related keywords. Our evaluation on an extensive set of benchmarks shows word embeddings to be the most reliable and consistent method to improve the accuracy on both tasks (with up to 6.9 percentage points in macro-average F1 on some datasets). The other two methods however, are not as useful. Our analysis shows that this could be due to a number of reasons, including the biased domain representation in the structured data and lack of vocabulary coverage. We share our datasets and discuss how our lessons learned could be taken forward to inform future research in this direction.","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,WDC Product Data Corpus and Gold Standard for Large-Scale Product Matching-Version 2.0,"Christian Bizer, Anna Primpeli, Ralph Peeters ","Many e-shops have started to mark-up product data within their HTML pages using the schema.org vocabulary. The Web Data Commons project regularly extracts such data from the Common Crawl, a large public web crawl. The Web Data Commons Training and Test Sets for Large-Scale Product Matching contain product offers from different e-shops in the form of binary product pairs (with corresponding label “match” or “no match”) for four product categories, computers, cameras, watches and shoes. In order to support the evaluation of machine learning-based matching methods, the data is split into training, validation and test sets. For each product category, we provide training sets in four different sizes (2.000-70.000 pairs). Furthermore there are sets of ids for each training set for a possible validation split (stratified random draw) available. The test set for each product category consists of 1.100 product pairs. The labels of the test sets were manually checked while those of the training sets were derived using shared product identifiers from the Web weak supervision. The data stems from the WDC Product Data Corpus for Large-Scale Product Matching - Version 2.0 which consists of 26 million product offers originating from 79 thousand websites. For more information and download links for the corpus itself, please follow the links below.

","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,Text Classification for Predicting Multi-level Product Categories,"Hadi Jahanshahi, Ozan Ozyegen, Mucahit Cevik, Beste Bulut, Deniz Yigit, Fahrettin F. Gonen, Ayşe Başar","In an online shopping platform, a detailed classification of the products facilitates user navigation. It also helps online retailers keep track of the price
fluctuations in a certain industry or special discounts on a specific product
category. Moreover, an automated classification system may help to pinpoint
incorrect or subjective categories suggested by an operator. In this study,
we focus on product title classification of the grocery products. We perform
a comprehensive comparison of six different text classification models to establish a strong baseline for this task, which involves testing both traditional
and recent machine learning methods. In our experiments, we investigate
the generalizability of the trained models to the products of other online
retailers, the dynamic masking of infeasible subcategories for pretrained language models, and the benefits of incorporating product titles in multiple
languages. Our numerical results indicate that dynamic masking of subcategories is effective in improving prediction accuracy. In addition, we observe
that using bilingual product titles is generally beneficial, and neural network based models perform significantly better than SVM and XGBoost models.
Lastly, we investigate the reasons for the misclassified products and propose
future research directions to further enhance the prediction models.
Keywords: multi-level classification, machine learning, supervised learning,
product category classification","Web Data Commons Phones Dataset, Augmented Version, Fixed Splits",5,0,,,
Entity Matching,SemTab 2019: Resources to Benchmark Tabular Data to Knowledge Graph Matching Systems,"Ernesto Jiménez-Ruiz. Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen. Kavitha Srinivas","Tabular data to Knowledge Graph matching is the process of assigning semantic tags from knowledge graphs (e.g., Wikidata or DBpedia) to the elements of a table. This task is a challenging problem for various reasons, including the lack of metadata (e.g., table and column names), the noisiness, heterogeneity, incompleteness and ambiguity in the data. The results of this task provide significant insights about potentially highly valuable tabular data, as recent works have shown, enabling a new family of data analytics and data science applications. Despite significant amount of work on various flavors of this problem, there is a lack of a common framework to conduct a systematic evaluation of state-of-the-art systems. The creation of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab) aims at filling this gap. In this paper, we report about the datasets, infrastructure and lessons learned from the first edition of the SemTab challenge.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,1,SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,"Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen, Ernesto Jiménez-Ruiz & Kavitha Srinivas","Data Sets from the ISWC 2019 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching.

For details, see: http://www.cs.ox.ac.uk/isg/challenges/sem-tab/

Note on License: This data includes data from the following sources. Refer to each source for license details:
- Wikipedia https://www.wikipedia.org/
- DBpedia http://dbpedia.org/
- T2Dv2 Gold Standard for Matching Web Tables to DBpedia http://webdatacommons.org/webtables/goldstandardV2.html"
Entity Matching,TURL: Table Understanding through Representation Learning,"Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu","Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning. Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,Tough Tables: Carefully Evaluating Entity Linking for Tabular Data,"Vincenzo Cutrona, Federico Bianchi, Ernesto Jiménez-Ruiz, Matteo Palmonari","Table annotation is a key task to improve querying the Web and support the Knowledge Graph population from legacy sources (tables). Last year, the SemTab challenge was introduced to unify different efforts to evaluate table annotation algorithms by providing a common interface and several general-purpose datasets as a ground truth. The SemTab dataset is useful to have a general understanding of how these algorithms work, and the organizers of the challenge included some artificial noise to the data to make the annotation trickier. However, it is hard to analyze specific aspects in an automatic way. For example, the ambiguity of names at the entity-level can largely affect the quality of the annotation. In this paper, we propose a novel dataset to complement the datasets proposed by SemTab. The dataset consists of a set of high-quality manually-curated tables with non-obviously linkable cells, i.e., where values are ambiguous names, typos, and misspelled entity names not appearing in the current version of the SemTab dataset. These challenges are particularly relevant for the ingestion of structured legacy sources into existing knowledge graphs. Evaluations run on this dataset show that ambiguity is a key problem for entity linking algorithms and encourage a promising direction for future work in the field.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,Results of SemTab 2020,"Ernesto Jiménez-Ruiz. Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen. Kavitha Srinivas, V. Cutrona","SemTab 2020 was the second edition of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching, successfully collocated
with the 19th International Semantic Web Conference (ISWC) and the 15th Ontology Matching (OM) Workshop. SemTab provides a common framework to
conduct a systematic evaluation of state-of-the-art systems.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,MTab4Wikidata at SemTab 2020: Tabular Data Annotation with Wikidata,"Phuc Nguyen, Ikuya Yamada, Natthawut Kertkeidkachorn, Ryutaro Ichise, Hideaki Takeda","This paper introduces an automatic semantic annotation
system, namely MTab4Wikidata, for the three semantic annotation tasks,
i.e., Cell-Entity Annotation (CEA), Column-Type Annotation (CTA),
Column Relation-Property Annotation (CPA), of Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab 2020).
In particular, we introduce (1) a novel fuzzy entity search to address misspelling table cells, (2) a fuzzy statement search to deal with ambiguous
cells, (3) a statement enrichment module to address the Wikidata shifting
issue, (4) an efficient and effective post-processing for the matching tasks.
Our system achieves impressive empirical performance for the three annotation tasks and wins the first prize at SemTab 2020. MTab4Wikidata
is ranked 1st in the two tasks of CEA and CPA, and 2nd rank in the CTA
task on the round 1, 2, 3 datasets and 1st rank on the round 4 dataset
and the Tough Tables (2T) dataset.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,Generating Conceptual Subgraph from Tabular Data for Knowledge Graph Matching,"Donguk Kim, Heesung Park, Jae Kyu Lee, Wooju Kim","In this paper, we study the problem of analyzing the relationship between data given in a tabular format and a target knowledge graph, e.g., Wikidata.
It is most important to find the label that indicates the correct meaning in Wikidata where data and values are annotated with each label. It is a very difficult task
for a machine to correctly understand or infer its meaning. For this to happen,
data must be accurately tagged. Wikidata has a label for each document. In addition, it has the characteristic of being linked to another document through these
documents. These connected data can be represented as graphs. In this paper, a
method is proposed to create a graph based on related elements and infer the
relationship of other elements using advanced Wikidata SPARQL queries. Above
all, this approach helps in interpreting clear inference relationships and provides
a very suitable approach in an environment where data changes frequently such
as an open access database.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,Neural Relation Extraction on Wikipedia Tables for Augmenting Knowledge Graphs,"Erin Macdonald, Denilson Barbosa","Knowledge Graph Augmentation is the task of adding missing facts to an incomplete knowledge graph to improve its effectiveness in applications such as web search and question answering. State-of-the-art methods rely on information extraction from running text, leaving rich sources of facts such as tables behind. We help close this gap with a neural method that uses contextual information surrounding a table in a Wikipedia article to extract relations between entities appearing in the same row of a table or between the entity of said article and entities appearing in the table. We trained and tested our method on a much larger dataset compared to previous work which we have made public and observed experimentally that our method is very promising for the task.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,Stratified Data Integration,"Fausto Giunchiglia, Alessio Zamboni, Mayukh Bagchi, Simone Bocca","We propose a novel approach to the problem of semantic heterogeneity where data are organized into a set of stratified and independent representation layers, namely: conceptual(where a set of unique alinguistic identifiers are connected inside a graph codifying their meaning), language(where sets of synonyms, possibly from multiple languages, annotate concepts), knowledge(in the form of a graph where nodes are entity types and links are properties), and data(in the form of a graph of entities populating the previous knowledge graph). This allows us to state the problem of semantic heterogeneity as a problem of Representation Diversity where the different types of heterogeneity, viz. Conceptual, Language, Knowledge, and Data, are uniformly dealt within each single layer, independently from the others. In this paper we describe the proposed stratified representation of data and the process by which data are first transformed into the target representation, then suitably integrated and then, finally, presented to the user in her preferred format. The proposed framework has been evaluated in various pilot case studies and in a number of industrial data integration problems.",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,A survey of OpenRefine reconciliation services,Antonin Delpeuch,"We review the services implementing the OpenRefine reconciliation API, comparing their design to the state of the art in record linkage. Due to the design of the API, the matching scores returned by the services are of little help to guide matching decisions. This suggests possible improvements to the specifications of the API, which could improve user workflows by giving more control over the scoring mechanism to the client.
",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,bbw: Matching CSV to Wikidata via Meta-lookup,"Renat Shigapov, Philipp Zumstein, Jan Kamlah, Lars Oberl¨ander, Joerg Mechnich, and Irene Schumm","We present our publicly available semantic annotator bbw
(boosted by wiki) tested at the second Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab2020). It annotates a
raw CSV-table using the entities, types and properties in Wikidata. Our
key ideas are meta-lookup over the SearX metasearch API and contextual matching with at least two features. Avoiding the use of dump
files, we kept the storage requirements low, used only up-to-date values
in Wikidata and ranked third in the challenge",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,DAGOBAH: Enhanced Scoring Algorithms for Scalable Annotations of Tabular Data,"Viet-Phi Huynh, Jixiong Liu, Yoan Chabot, Thomas Labb´e, Pierre Monnin, and Rapha¨el Troncy","We present new approaches used in the DAGOBAH system
to perform automatic semantic table interpretation. DAGOBAH semantically annotates tables with Wikidata entities and relations to perform
three tasks: Columns-Property Annotation (CPA), Cell-Entity Annotation (CEA) and Column-Type Annotation (CTA). In our system, the initial scores from entity disambiguation influence the CPA output, which,
in turn, influences the output of the CEA. Finally, the CTA is computed
using the type hierarchy available in the knowledge graph in order to annotate columns with the most suitable fine-grained types. This approach
that leverages mutual influences between annotations allows DAGOBAH
to obtain very competitive results on all tasks of the SemTab2020 challenge",SemTab2019: Semantic Web Challenge on Tabular Data to Knowledge Graph Matching - 2019 Data Sets,6,0,,,
Entity Matching,The WDC Training Dataset and Gold Standard for Large-Scale Product Matching,"Anna Primpeli, Ralph Peeters, Christian Bizer","A current research question in the area of entity resolution (also
called link discovery or duplicate detection) is whether and in
which cases embeddings and deep neural network based matching
methods outperform traditional symbolic matching methods. The
problem with answering this question is that deep learning based
matchers need large amounts of training data. The entity resolution
benchmark datasets that are currently available to the public are
too small to properly evaluate this new family of matching methods.
The WDC Training Dataset for Large-Scale Product Matching fills
this gap. The English language subset of the training dataset consists
of 20 million pairs of offers referring to the same products. The offers
were extracted from 43 thousand e-shops which provide schema.org
annotations including some form of product ID such as a GTIN or
MPN. We also created a gold standard by manually verifying 2200
pairs of offers belonging to four product categories. Using a subset of
our training dataset together with this gold standard, we are able to
publicly replicate the recent result of Mudgal et al. that embeddings
and deep neural network based matching methods outperform
traditional symbolic matching methods on less structured data.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,1,Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,"Christian Bizer, Anna Primpeli, Ralph Peeters
","The training dataset consisting of 20 million pairs of product offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 2000 pairs of offers belonging to four different product categories.

"
Entity Matching,Deep Entity Matching with Pre-Trained Language Models,"Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, Wang-Chiew Tan","We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn ""harder"" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Using schema.org Annotations for Training and Maintaining Product Matchers,"Ralph Peeters, Anna Primpeli, Benedikt Wichtlhuber, Christian Bizer","Product matching is a central task within e-commerce applications such as price comparison portals and online market places. State-of-the-art product matching methods achieve F1 scores above 0.90 using deep learning techniques combined with huge amounts of training data (e.g > 100K pairs of offers). Gathering and maintaining such large training corpora is costly, as it implies labeling pairs of offers as matches or non-matches. Acquiring the ability to be good at product matching thus means a major investment for an e-commerce company. This paper shows that the manual labeling of training data for product matching can be replaced by relying exclusively on schema.org annotations gathered from the public Web. We show that using only schema.org data for training, we are able to achieve F1 scores between 0.92 and 0.95 depending on the product category. As new products appear everyday, it is important that matching models can be maintained with justifiable effort. In order to give practical advice on how to maintain matching models, we compare the performance of deep learning and traditional matching models on unseen products and experiment with different fine-tuning and re-training strategies for model maintenance, again using only schema.org annotations as training data. Finally, as using the public Web as distant supervision carries inherent noise, we evaluate deep learning and traditional matching models with regards to their label-noise resistance and show that deep learning is able to deal with the amounts of identifier-noise found in schema.org annotations.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,MWPD2020: Semantic Web Challenge on Mining the Web of HTML-embedded Product Data.,"Ziqi Zhang, Christian Bizer, Ralph Peeters, Anna Primpeli","This paper gives an overview of the Semantic Web Challenge
on Mining the Web of HTML-embedded Product Data (MWPD2020)
which has been conducted as part of the International Semantic Web
Conference (ISWC2020). The challenge consists of two tasks: product
matching and product classification. In the first task, participants need
to identify offers for the same product originating from different websites.
The goal of the second task is to categorize offers from different websites
into the GS1 GPC product hierarchy. Six teams from the USA, China,
Japan, and Germany participated in the challenge. The winning system
in Task 1, PMap, achieved an F1 score of 86.05 using an ensemble of
transformer-based language models. Task 2 was won by team Rhinobird
achieving a weighted average F1 score of 88.62 using a BERT-based ensemble which considers the dependencies among different category levels.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Intermediate Training of BERT for Product Matching,"Ralph Peeters, Christian Bizer, Goran Glavaš","Transformer-based models like BERT have pushed the state-of
the-art for a wide range of tasks in natural language processing.
General-purpose pre-training on large corpora allows Transformers
to yield good performance even with small amounts of training
data for task-specific fine-tuning. In this work, we apply BERT to
the task of product matching in e-commerce and show that BERT
is much more training data efficient than other state-of-the-art
methods. Moreover, we show that we can further boost its effectiveness through an intermediate training step, exploiting large
collections of product offers. Our intermediate training leads to
strong performance (>90% F1) on new, unseen products without
any product-specific fine-tuning. Further fine-tuning yields additional gains, resulting in improvements of up to 12% F1 for small
training sets. Adding the masked language modeling objective in
the intermediate training step in order to further adapt the language
model to the application domain leads to an additional increase of
up to 3% F1.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Alaska: A Flexible Benchmark for Data Integration Tasks,"Valter Crescenzi, Andrea De Angelis, Donatella Firmani, Maurizio Mazzei, Paolo Merialdo, Federico Piai, Divesh Srivastava","Data integration is a long-standing interest of the data management community and has many disparate applications, including business, science and government. We have recently witnessed impressive results in specific data integration tasks, such as Entity Resolution, thanks to the increasing availability of benchmarks. A limitation of such benchmarks is that they typically come with their own task definition and it can be difficult to leverage them for complex integration pipelines. As a result, evaluating end-to-end pipelines for the entire data integration process is still an elusive goal. In this work, we present Alaska, the first benchmark based on real-world dataset to support seamlessly multiple tasks (and their variants) of the data integration pipeline. The dataset consists of ~70k heterogeneous product specifications from 71 e-commerce websites with thousands of different product attributes. Our benchmark comes with profiling meta-data, a set of pre-defined use cases with diverse characteristics, and an extensive manually curated ground truth. We demonstrate the flexibility of our benchmark by focusing on several variants of two crucial data integration tasks, Schema Matching and Entity Resolution. Our experiments show that our benchmark enables the evaluation of a variety of methods that previously were difficult to compare, and can foster the design of more holistic data integration solutions.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Dual-objective fine-tuning of BERT for entity matching," Christian Bizer, Ralph Peeters ","An increasing number of data providers have adopted shared numbering schemes such as GTIN, ISBN, DUNS, or ORCID numbers for
identifying entities in the respective domain. This means for data
integration that shared identifiers are often available for a subset
of the entity descriptions to be integrated while such identifiers are
not available for others. The challenge in these settings is to learn a
matcher for entity descriptions without identifiers using the entity
descriptions containing identifiers as training data. The task can
be approached by learning a binary classifier which distinguishes
pairs of entity descriptions for the same real-world entity from
descriptions of different entities. The task can also be modeled as a
multi-class classification problem by learning classifiers for identifying descriptions of individual entities. We present a dual-objective
training method for BERT, called JointBERT, which combines binary matching and multi-class classification, forcing the model to
predict the entity identifier for each entity description in a training
pair in addition to the match/non-match decision. Our evaluation
across five entity matching benchmark datasets shows that dualobjective training can increase the matching performance for seen
products by 1% to 5% F1 compared to single-objective Transformerbased methods, given that enough training data is available for both
objectives. In order to gain a deeper understanding of the strengths
and weaknesses of the proposed method, we compare JointBERT
to several other BERT-based matching methods as well as baseline
systems along a set of specific matching challenges. This evaluation shows that JointBERT, given enough training data for both
objectives, outperforms the other methods on tasks involving seen
products, while it underperforms for unseen products. Using a combination of LIME explanations and domain-specific word classes,
we analyze the matching decisions of the different deep learning
models and conclude that BERT-based models are better at focusing
on relevant word classes compared to RNN-based models.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Towards Multi-Modal Entity Resolution for Product Matching,"Moritz Wilke, Erhard Rahm","Entity Resolution has been applied successfully to match
product offers from different web shops. Unfortunately, in
certain domains the (textual or numerical) attributes of a
product are not sufficient for a reliable match decision. To
overcome this problem we extend an attribute-based matching system to incorporate image data, which are available in
almost every web shop. To evaluate the system we enhance
the WDC product matching dataset with images crawled
from the web. First evaluations show that the use of images
is beneficial to increase recall and overall match quality.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Deep Entity Matching: Challenges and Opportunities,"Yuliang Li, Jinfeng Li, Yoshihiko Suhara, Jin Wang, Wataru Hirota, Wang-Chiew Tan","Entity matching refers to the task of determining whether two different representations refer to the same real-world entity. It continues to be a prevalent problem for many organizations where data resides in different sources and duplicates the need to be identified and managed. The term “entity matching” also loosely refers to the broader problem of determining whether two heterogeneous representations of different entities should be associated together. This problem has an even wider scope of applications, from determining the subsidiaries of companies to matching jobs to job seekers, which has impactful consequences.

In this article, we first report our recent system DITTO, which is an example of a modern entity matching system based on pretrained language models. Then we summarize recent solutions in applying deep learning and pre-trained language models for solving the entity matching task. Finally, we discuss research directions beyond entity matching, including the promise of synergistically integrating blocking and entity matching steps together, the need to examine methods to alleviate steep training data requirements that are typical of deep learning or pre-trained language models, and the importance of generalizing entity matching solutions to handle the broader entity matching problem, which leads to an even more pressing need to explain matching outcomes.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Improving Hierarchical Product Classification using Domain-specific Language Modelling,"Alexander Brinkmann, Christian Bizer","In order to deliver a coherent user experience, product aggregators
such as market places or price portals integrate product offers from
many web shops into a single product categorization hierarchy.
Recently, transformer models have shown remarkable performance
on various NLP tasks. These models are pre-trained on huge crossdomain text corpora using self-supervised learning and fine-tuned
afterwards for specific downstream tasks. Research from other
application domains indicates that additional self-supervised pretraining using domain-specific text corpora can further increase
downstream performance without requiring additional task-specific
training data. In this paper, we first show that transformers outperform a more traditional fastText-based classification technique
on the task of assigning product offers from different web shops
into a product hierarchy. Afterwards, we investigate whether it is
possible to improve the performance of the transformer models by
performing additional self-supervised pre-training using different
corpora of product offers, which were extracted from the Common
Crawl. Our experiments show that by using large numbers of related product offers for masked language modelling, it is possible
to increase the performance of the transformer models by 1.22% in
wF1 and 1.36% in hF1 reaching a performance of nearly 89% wF1.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,Exploiting Knowledge Graphs for Facilitating Product/Service Discovery,Sarika Jain,"Most of the existing techniques to product discovery rely on syntactic approaches, thus ignoring valuable and specific semantic information of the underlying standards during the process. The product data comes from different heterogeneous sources and formats giving rise to the problem of interoperability. Above all, due to the continuously increasing influx of data, the manual labeling is getting costlier. Integrating the descriptions of different products into a single representation requires organizing all the products across vendors in a single taxonomy. Practically relevant and quality product categorization standards are still limited in number; and that too in academic research projects where we can majorly see only prototypes as compared to industry. This work presents a cost-effective solution for e-commerce on the Data Web by employing an unsupervised approach for data classification and exploiting the knowledge graphs for matching. The proposed architecture describes available products in web ontology language OWL and stores them in a triple store. User input specifications for certain products are matched against the available product categories to generate a knowledge graph. This mullti-phased top-down approach to develop and improve existing, if any, tailored product recommendations will be able to connect users with the exact product/service of their choice.",Web Data Commons - The WDC Data Training Dataset and Gold Standard for Large-Scale Product Matching,7,0,,,
Entity Matching,3D Matching of Resource Vision Tracking Trajectories,"Eirini Konstantinou, Ioannis Brilakis","Issues related to management and workforce play a key role in the productivity gap of construction and manufacturing. Both issues are directly related to the way productivity is measured. Current measurement methods tend to be ineffective because they are labour intensive, costly and prone to human errors whereas they are mainly reactive processes initiated after the detection of a negatively influencing factor. So far, research efforts in automating the measuring process have not achieved full automation because they require prior knowledge of the type of tasks performed in specific working zones. This is associated with the lack of depth information. For this purpose, this paper proposes a computationally efficient computer vision method for matching construction workers across different frames based on epipolar geometry, template and motion matching methods. The main result of this process is to provide a method for the acquisition of the 4D features (x, y, z, t) that compose the detailed profile of a construction activity in terms of both time and space.
",3D Matching of resource vision tracking trajectories,8,1,3D Matching of resource vision tracking trajectories,"Eirini Konstantinou, Ioannis Brilakis","Three dimensional (3D) paths of resources, have been proposed in construction management, as an efficient way for measuring labor productivity. These paths, are extracted either by using sensors such as Global Positioning System (GPS), Radio Frequency Identification (RFID), and Ultra-wideband (UWB), or based on cameras placed at jobsites for surveillance purposes. However, the tag based methods are seriously limited by privacy conflicts since they are not welcome from the personnel. On the other hand, the computer vision based methods have not achieved full automation in measuring labour productivity because they require prior knowledge of the type of tasks performed in specific working zones. This is associated with the lack of depth information. For this purpose, this paper proposes a computationally efficient computer vision method for matching construction workers across different frames. Entity matching, is a process that corresponds to a compulsory step prior to the calculation of the 3D position. The proposed matching method, is based on epipolar geometry, template and motion similarity features. The main result of this process, is to provide a method for the acquisition of the 3D paths that compose the detailed profile of a construction activity in terms of both time and space."
Entity Matching,"End-to-end vision-based detection, tracking and activity analysis of earthmoving equipment filmed at ground level","Dominic Roberts, Mani Golparvar-Fard","This paper presents a new benchmark dataset for validating vision-based methods that automatically identifies visually distinctive working activities of excavators and dump trucks from individual frames of a video sequence. Our dataset consists of 10 videos of interacting pairs of construction equipment filmed at ground level with accompanying ground truth annotations. These annotations consist of per-equipment and per-frame equipment bounding boxes that also have associated identities and activity labels. Our videos depict an excavator interacting with 1 or more dump trucks. We also propose a deep learning-based method for detecting and tracking objects based on Convolutional Neural Networks (CNNs). The tracking trajectories are fed into a Hidden Markov Model (HMM) that automatically discovers and assigns activity labels for any observed object. Our HMM method leverages trajectories to train a Gaussian Mixture Model (GMM) with which we estimate the probability density function of each activity using Support Vector Machine (SVM) classifiers. The proposed HMM also models activity duration and the transition between activities. We show that our method can accurately distinguish between individual equipment working activities. Results show 97.43% detection Average Precision (AP) for excavators and 75.29% AP for dump trucks, as well as cross-category tracking accuracy of 81.94% and tracking precision of 87.45%. Separate experiment results show activity analysis results of 86.8% accuracy for excavators and 88.5% for dump trucks. Our results show that our method can accurately conduct activity analysis and can be fused with methods that detect motion trajectories to scale to the needs of practical applications.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Matching Construction Workers across Views for Automated 3D Vision Tracking On-Site,"Eirini Konstantinou, Ioannis Brilakis","Computer vision-based tracking methods are used to track construction resources for productivity and safety purposes. This type of tracking requires that targets are accurately matched across multiple camera views to obtain a 3D trajectory out of two or more 2D trajectories. This matching is straightforward when it involves easily distinguishable targets in uncluttered scenes. This can be challenging in industrial scenes such as construction sites due to congestion, occlusions and workers in greatly similar high visibility apparel. This paper proposes a novel vision-based method that addresses all these issues. It uses as input the output of a 2D vision-based tracking method and searches for potential matches in three sequential steps. It terminates only when a positive match is found. The first step returns the strongest candidate by correlating a segment of workers’ past 2D trajectories. The second employs geometric restrictions, whilst the third correlates colour intensity values. The proposed method features a promising performance of 97% precision, 98% recall and 95% accuracy.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Automatic matching of construction onsite resources under camera views,"Bingfei Zhang, Zhenhua Zhu, Amin Hammad, Walid Aly","When a video camera network is placed on a construction site to monitor onsite activities, construction resources, such as equipment and worker, might be captured by two or more cameras at the same time. Therefore, it is important to conduct the matching to identify whether the resources captured into separate camera views refer to the same one on the site. Otherwise, it leads to the repetitive counting, when analyzing the onsite resources utilization automatically. This paper proposes a novel matching method that relies on the construction site visual features and the spatial relationships of onsite construction resources as the matching cues. Specifically, the method first searches the potential matching candidates between two camera views following their epipolar constraints. Then, the triangular coordinates of these candidates are calculated based on their locations in the triangular mesh of each camera view. This way, the matching of multiple construction resources between two camera views could be converted to a combinatorial optimization problem and solved with the Hungarian algorithm. The proposed method has been tested with the images and videos captured from real construction sites. The test results showed that the average matching accuracy could reach 93%.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Vision-Based Construction Worker Activity Analysis Informed by Body Posture,"Dominic Roberts, Wilfredo Torres Calderon, Shuai Tang, Mani Golparvar-Fard","Activity analysis of construction resources is generally performed by manually observing construction operations either in person or through recorded videos. It is thus prone to observer fatigue and bias and is of limited scalability and cost-effectiveness. Automating this procedure obviates these issues and can allow project teams to focus on performance improvement. This paper introduces a novel deep learning– and vision-based activity analysis framework that estimates and tracks two-dimensional (2D) worker pose and outputs per-frame worker activity labels given input red-green-blue (RGB) video footage of a construction worker operation. We used 317 annotated videos of bricklaying and plastering operations to train and validate the proposed method. This method obtained 82.6% mean average precision (mAP) for pose estimation and 72.6% multiple-object tracking accuracy (MOTA), and 81.3% multiple-object tracking precision (MOTP) for pose tracking. Cross-validation activity analysis accuracy of 78.5% was also obtained. We show that worker pose contributes to activity analysis results. This highlights the potential for using vision-based ergonomics assessment methods that rely on pose in conjunction with the proposed method for assessing the ergonomic viability of individual activities.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Productivity monitoring in building construction projects: a systematic review,"Wesam Salah Alaloul, Khalid M. Alzubi, Ahmad B. Malkawi, Marsail Al Salaheen, Muhammad Ali Musarat ","Purpose - The unique nature of the construction sector makes it fall behind other sectors in terms of productivity. Monitoring construction productivity is crucial for the construction project's success. Current practices for construction productivity monitoring are time-consuming, manned and error prone. Although previous studies have been implemented toward reducing these limitations, a gap still exists in the automated monitoring of construction productivity. Design/methodology/approach - This study aims to investigate and assess the different techniques used for monitoring productivity in building construction projects. Therefore, a mixed review methodology (bibliometric analysis and systematic review) was adopted. All the related publications were collected from different databases, which were further screened to get the most relevant based on the Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) criteria. Findings - A detailed review was performed, and it was found that traditional methods, computer vision-based and photogrammetry are the most adopted data acquisition for productivity monitoring of building projects, respectively. Machine learning algorithms (ANN, SVM) and BIM were integrated with monitoring tools and technologies to enhance the automated monitoring performance in construction productivity. Also, it was observed that current studies did not cover all the complex construction job sites and they were applied based on a small sample of construction workers and machines separately. Originality/value - This review paper contributes to the literature on construction management by providing insight into different productivity monitoring techniques.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,An Annotation Tool for Benchmarking Methods for Automated Construction Worker Pose Estimation and Activity Analysis,"D. Roberts, M. Wang, W. Torres Calderon, M. Golparvar-Fard, MJ DeJong, JM Schooling","The increase in affordability and quality of jobsite cameras provides opportunities to facilitate monitoring of construction worker activities. Research has highlighted the potential of computer vision algorithms to be used for automated activity analysis. However, applying and adapting such techniques to the construction setting and deploying them for use on job sites entails extensive validation against a construction-specific benchmark dataset. Most state-of-the-art computer vision algorithms are strongly supervised and thus such benchmarks require large amounts of per-image ground truth annotations, both to enable the algorithm to learn how to perform its task during training time and for evaluation on a separate portion of this dataset at test-time. We address this need by introducing an annotation tool that enables users to label visual footage of construction workers by positioning both worker instances and individual key points corresponding to the body parts of each worker (e.g., left wrist, right elbow, nose) in the scene. This annotation tool is the first such publicly available tool capable of producing ground truth for pose estimation, tracking and activity analysis methods to the best of our knowledge. We demonstrate the capabilities of our tool by exhaustively annotating key points, identities and activity labels in a dataset of 393 image sequences that depict workers performing various construction-related activities. This preliminary set of annotation tasks demonstrates the ease of use and flexibility of our annotation tool.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Trajectory-based worker task productivity monitoring,"Eirini Konstantinou, Ioannis Brilakis","Over the past decades labour productivity in construction has been declining. The prevalent approach to estimating labour productivity is through an analysis of the trajectories of the construction entities. This analysis typically exploits four types of trajectory data: a) walking path trajectories, b) dense trajectories (posture), c) physiological rates such as heart rate (beats/minute) and respiratory rate (breaths/minute), and d) sound signals. The output of this analysis is the number of work cycles performed by construction workers. The total duration of these cycles is equal to the labour input of a task. However, all such methods do not meet the requirements for proactive monitoring of labour productivity in an accurate, non-obtrusive, time and cost efficient way for multiple workers. This paper proposes a method to address this shortcoming. It features a promising accuracy in terms of calculating the labour input.

",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Automated Monitoring For Construction Productivity Recognition,"Khalid Mhmoud Alzubi, Wesam Salah Alaloul, Marsail Al Salaheen, Abdul Hannan Qureshi, Muhammad Ali Musarat, Abdullah O. Baarimah","Comparing to other sectors, the construction sector suffers from low productivity, and it is not improving over time due to the unique nature of construction projects. It is believed that construction productivity cannot be improved without efficient monitoring and measuring, and this is crucial for project success. There are many limitations for the traditional construction productivity monitoring practices like time and cost consuming and error-prone. Although a lot of studies have been implemented to eliminate these limitations, a gap still exists in the automated monitoring of construction productivity. This study proposes an automated monitoring model for indoor productivity recognition in construction projects. The model will provide an instant evaluation of the project productivity which will enhance the optimum utilization of the project resources. The proposed model will be developed by first generating a baseline for the activities state which will be represented as baseline state model. Then the as-built model will be generated. Preliminary experimentation was performed on selected images where the number of tiles and bricks was obtained. The experimentation was performed using Open-Source Computer Vision Library (OpenCV). Preliminary results depict that by using the proposed model the automated monitoring of productivity is achievable. Although, there is a need of dedicated efforts for improvement and development of technique for more effective and efficient results.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Multi-View Matching for Onsite Construction Resources with Combinatorial Optimization,"Bingfei Zhang, Zhenhua Zhu, Amin Hammad, Walid Aly","When multiple video cameras are set up to monitor construction activities, onsite construction resources (e.g. equipment and workers) might be captured by two or more cameras at the same time. It becomes important to identify whether the resources captured into separate camera views refer to the same one on the site. Otherwise, the automatic reporting of onsite resources utilization will produce repetive countings. This paper proposes a novel method for matching onsite construction resources in multiple camera views. The method relies on the visual features of a construction site and the spatial relationships of the resources on the site as matching cues. It starts with searching potential matching candidates between the camera views following their epipolar constraints. Then, the candidates’ local triangular coordinates are calculated to define matching costs. This way, the matching of multiple construction resources between camera views could be solved through combinatorial optimization. The proposed method has been tested to match workers, equipment and traffic cones within the images and videos captured from construction sites. The test results showed that the method could reach an average of 93% matching accuracy.",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,KAJIAN SINGKAT TENTANG: PENGENDALIAN DAN PENJEJAKAN OBJEK BERBASIS VISUAL,"Ari Yuliati, Carmadi Machbub. Pranoto H Rusmin","Pengendalian   dan   penjejakan   objek banyak   digunakan   diberbagai   bidang seperti   pendidikan,   kesehatan   dan kesejahteraan, olahraga,  pengawasan  pada  industri  konstuksi,  pengawasan  di  supermarket,  dan  lain-lain.  Sistem pengendalian dan  penjejakan objek terdiri dari tigasub-sistem, yaitu pendeteksi dan pengenalan objek, sistem estimasi pergerakan objek, dan pengendalian perangkat kamera dan aktuatornya agar dapat menjejaki objek. Banyak kajian telah dilakukantentang  beberapa  algoritma  untuk  melakukan  deteksi  dan  pengenalan  objek  seperti camshift, viola-jones, gaussian  mixturemodel, surf,  dan  lain-lain.  Dengan  kelebihan  dan  kelemahan  sebagai  berikut  (i)  ketika  data  terbatas jumlahnya, (ii) sistem operasi berbasis fitur jauh lebih cepat daripada sistem berbasis piksel, (iii) posisi, ukuran, orientasi objek  yang  berubah-ubah,  dan  lain-lain.Algoritma-algortima  ini  telah  dikembangkan  untuk  estimasi  pergerakan  objek namun dalam hal ini masih terdapat kelemahaan antara lain pada gerak objek dan lokasi objek yang selalu berubah-ubah dan jumlah objek lebih banyak daripada jumlah kamera. Sedangkan,persoalan yang dihadapi dalam sistem pengendalian adalah  ketika  penjejakan  pada  objek  terjadi  seperti  (i) hijacking  problem,  (ii) centralization  problem,  (iii) drifting problem. Tujuan dari kajian ini adalah untuk melakukan bahasan dari penelitian mengenai pengendalian dan penjejakan objek,  yang  telah  dilakukan  sebelumnya  dan  untuk  mendapatkan  titik  temu  peluang  kontribusi dan  nilai  kebaruandari bidang ini",3D Matching of resource vision tracking trajectories,8,0,,,
Entity Matching,Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings,"Vasilis Efthymiou, Oktie Hassanzadeh, Mariano Rodriguez-Muro, Vassilis Christophides","Web tables constitute valuable sources of information for
various applications, ranging from Web search to Knowledge Base (KB)
augmentation. An underlying common requirement is to annotate the
rows of Web tables with semantically rich descriptions of entities published in Web KBs. In this paper, we evaluate three unsupervised annotation methods: (a) a lookup-based method which relies on the minimal
entity context provided in Web tables to discover correspondences to
the KB, (b) a semantic embeddings method that exploits a vectorial
representation of the rich entity context in a KB to identify the most relevant subset of entities in the Web table, and (c) an ontology matching
method, which exploits schematic and instance information of entities
available both in a KB and a Web table. Our experimental evaluation
is conducted using two existing benchmark data sets in addition to a
new large-scale benchmark created using Wikipedia tables. Our results
show that: 1) our novel lookup-based method outperforms state-of-theart lookup-based methods, 2) the semantic embeddings method outperforms lookup-based methods in one benchmark data set, and 3) the lack
of a rich schema in Web tables can limit the ability of ontology matching
tools in performing high-quality table annotation. As a result, we propose a hybrid method that significantly outperforms individual methods
on all the benchmarks.",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,1,Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,"Vasilis Efthymiou, Oktie Hassanzadeh, Mariano Rodríguez-Muro,  Vassilis Christophides
",
Entity Matching,ColNet: Embedding the Semantics of Web Tables for Column Type Prediction,"Jiaoyan Chen, Ernesto Jiménez-Ruiz, Ian Horrocks,Charles Sutton","Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,SemTab 2019: Resources to Benchmark Tabular Data to Knowledge Graph Matching Systems,"Ernesto Jiménez-Ruiz, Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen, Kavitha Srinivas","Tabular data to Knowledge Graph matching is the process of assigning semantic tags from knowledge graphs (e.g., Wikidata or DBpedia) to the elements of a table. This task is a challenging problem for various reasons, including the lack of metadata (e.g., table and column names), the noisiness, heterogeneity, incompleteness and ambiguity in the data. The results of this task provide significant insights about potentially highly valuable tabular data, as recent works have shown, enabling a new family of data analytics and data science applications. Despite significant amount of work on various flavors of this problem, there is a lack of a common framework to conduct a systematic evaluation of state-of-the-art systems. The creation of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab) aims at filling this gap. In this paper, we report about the datasets, infrastructure and lessons learned from the first edition of the SemTab challenge.

",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,"Web Table Extraction, Retrieval, and Augmentation: A Survey","Shuo Zhang, Krisztian Balog","Tables are a powerful and popular tool for organizing and manipulating data. A vast number of tables can be found on the Web, which represents a valuable knowledge resource. The objective of this survey is to synthesize and present two decades of research on web tables. In particular, we organize existing literature into six main categories of information access tasks: table extraction, table interpretation, table search, question answering, knowledge base augmentation, and table augmentation. For each of these tasks, we identify and describe seminal approaches, present relevant resources, and point out interdependencies among the different tasks.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,End-to-End Entity Resolution for Big Data: A Survey,"Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George Papadakis, Kostas Stefanidis","One of the most important tasks for improving data quality and the reliability of data analytics results is Entity Resolution (ER). ER aims to identify different descriptions that refer to the same real-world entity, and remains a challenging problem. While previous works have studied specific aspects of ER (and mostly in traditional settings), in this survey, we provide for the first time an end-to-end view of modern ER workflows, and of the novel aspects of entity indexing and matching methods in order to cope with more than one of the Big Data characteristics simultaneously. We present the basic concepts, processing steps and execution strategies that have been proposed by different communities, i.e., database, semantic Web and machine learning, in order to cope with the loose structuredness, extreme diversity, high speed and large scale of entity descriptions used by real-world applications. Finally, we provide a synthetic discussion of the existing approaches, and conclude with a detailed presentation of open research directions.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,Learning Semantic Annotations for Tabular Data,"Jiaoyan Chen, Ernesto Jimenez-Ruiz, Ian Horrocks, Charles Sutton","The usefulness of tabular data such as web tables critically depends on understanding their semantics. This study focuses on column type prediction for tables without any meta data. Unlike traditional lexical matching-based methods, we propose a deep prediction model that can fully exploit a table's contextual semantics, including table locality features learned by a Hybrid Neural Network (HNN), and inter-column semantics features learned by a knowledge base (KB) lookup and query answering this http URL exhibits good performance not only on individual table sets, but also when transferring from one table set to another.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,Novel Entity Discovery from Web Tables,"Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda","When working with any sort of knowledge base (KB) one has to make sure it is as complete and also as up-to-date as possible. Both tasks are non-trivial as they require recall-oriented efforts to determine which entities and relationships are missing from the KB. As such they require a significant amount of labor. Tables on the Web, on the other hand, are abundant and have the distinct potential to assist with these tasks. In particular, we can leverage the content in such tables to discover new entities, properties, and relationships. Because web tables typically only contain raw textual content we first need to determine which cells refer to which known entities---a task we dub table-to-KB matching. This first task aims to infer table semantics by linking table cells and heading columns to elements of a KB. Then second task builds upon these linked entities and properties to not only identify novel ones in the same table but also to bootstrap their type and additional relationships. We refer to this process as novel entity discovery and, to the best of our knowledge, it is the first endeavor on mining the unlinked cells in web tables. Our method identifies not only out-of-KB (``novel'') information but also novel aliases for in-KB (``known'') entities. When evaluated using three purpose-built test collections, we find that our proposed approaches obtain a marked improvement in terms of precision over our baselines whilst keeping recall stable.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,Auto-completion for Data Cells in Relational Tables,"Shuo Zhang, Krisztian Balog","We address the task of auto-completing data cells in relational tables. Such tables describe entities (in rows) with their attributes (in columns). We present the CellAutoComplete framework to tackle several novel aspects of this problem, including: (i) enabling a cell to have multiple, possibly conflicting values, (ii) supplementing the predicted values with supporting evidence, (iii) combining evidence from multiple sources, and (iv) handling the case where a cell should be left empty. Our framework makes use of a large table corpus and a knowledge base as data sources, and consists of preprocessing, candidate value finding, and value ranking components. Using a purpose-built test collection, we show that our approach is 40\% more effective than the best baseline.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,CSV2KG: Transforming Tabular Data into Semantic Knowledge,"Bram Steenwinckel, Gilles Vandewiele, Filip De Turck, Femke Ongenae","A large portion of structured data does not yet reap the benefits of the Semantic Web, or Web 2.0, as it is not semantically annotated.
In this paper, we propose a system to generates semantic knowledge,
available on DBPedia, from common CSV files. The “Tabular Data to
Knowledge Graph Matching” competition, consisting of three different
subchallenges, was used to evaluate the proposed methodology.",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,Extracting Novel Facts from Tables for Knowledge Graph Completion,"Benno Kruit, Peter Boncz, Jacopo Urbani","We propose a new end-to-end method for extending a Knowledge Graph (KG) from tables. Existing techniques tend to interpret tables by focusing on information that is already in the KG, and therefore tend to extract many redundant facts. Our method aims to find more novel facts. We introduce a new technique for table interpretation based on a scalable graphical model using entity similarities. Our method further disambiguates cell values using KG embeddings as additional ranking method. Other distinctive features are the lack of assumptions about the underlying KG and the enabling of a fine-grained tuning of the precision/recall trade-off of extracted facts. Our experiments show that our approach has a higher recall during the interpretation process than the state-of-the-art, and is more resistant against the bias observed in extracting mostly redundant facts since it produces more novel extractions.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,TURL: Table Understanding through Representation Learning,"Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu","Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning. Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.
",Evaluating Web Table Annotation Methods: From Entity Lookups to Entity Embeddings,9,0,,,
Entity Matching,Fine-Grained Entity Recognition,"Xiao Ling, Daniel S. Weld","Entity Recognition (ER) is a key component of relation extraction systems and many other natural-language processing
applications. Unfortunately, most ER systems are restricted
to produce labels from to a small set of entity classes, e.g.,
person, organization, location or miscellaneous. In order to
intelligently understand text and extract a wide range of information, it is useful to more precisely determine the semantic classes of entities mentioned in unstructured text. This
paper defines a fine-grained set of 112 tags, formulates the
tagging problem as multi-class, multi-label classification, describes an unsupervised method for collecting training data,
and presents the FIGER implementation. Experiments show
that the system accurately predicts the tags for entities. Moreover, it provides useful information for a relation extraction
system, increasing the F1 score by 93%. We make FIGER and
its data available as a resource for future work.",Fine-Grained Entity Recognition,10,1,Fine-Grained Entity Recognition,"Xiao Ling, Daniel S. Weld","The FIGER dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as person/doctor, art/written_work and building/hotel. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually."
Entity Matching,"Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions","Wei Shen, Jianyong Wang, Jiawei Han","The large number of potential applications from bridging web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,A Survey on Deep Learning for Named Entity Recognition,"Jing Li, Aixin Sun, Jianglei Han, Chenliang Li","Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,DeepType: Multilingual Entity Linking by Neural Type System Evolution,"Jonathan Raiman, Olivier Raiman","The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,Cross-lingual Name Tagging and Linking for 282 Languages,"Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, Heng Ji","The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating “silver-standard” annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,Design Challenges for Entity Linking,"Xiao Ling, Sameer Singh, Daniel S. Weld","Recent research on entity linking (EL) has introduced a plethora of promising techniques, ranging from deep neural networks to joint inference. But despite numerous papers there is surprisingly little understanding of the state of the art in EL. We attack this confusion by analyzing differences between several versions of the EL problem and presenting a simple yet effective, modular, unsupervised system, called Vinculum, for entity linking. We conduct an extensive evaluation on nine data sets, comparing Vinculum with two state-of-the-art systems, and elucidate key aspects of the system that include mention extraction, candidate generation, entity type prediction, entity coreference, and coherence.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,Semantic NLP-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking,"Jiansong Zhang, Nora M. El-Gohary","Automated regulatory compliance checking requires automated extraction of requirements from regulatory textual documents and their formalization in a computer-processable rule representation. Such information extraction (IE) is a challenging task that requires complex analysis and processing of text. Natural language processing (NLP) aims to enable computers to process natural language text in a human-like manner. This paper proposes a semantic, rule-based NLP approach for automated IE from construction regulatory documents. The proposed approach uses a set of pattern-matching-based IE rules and conflict resolution (CR) rules in IE. A variety of syntactic (syntax/grammar-related) and semantic (meaning/context-related) text features are used in the patterns of the IE and CR rules. Phrase structure grammar (PSG)-based phrasal tags and separation and sequencing of semantic information elements are proposed and used to reduce the number of needed patterns. An ontology is used to aid in the recognition of semantic text features (concepts and relations). The proposed IE algorithms were tested in extracting quantitative requirements from the 2009 International Building Code and achieved 0.969 and 0.944 precision and recall, respectively.",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,"Entity Linking via Joint Encoding of Types, Descriptions, and Context","Nitish Gupta, Sameer Singh, Dan Roth","For accurate entity linking, we need to capture various information aspects of an entity, such as its description in a KB, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our model can effectively “embed” entities that are new to the KB, and is able to link its mentions accurately.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages,"Benjamin Heinzerling, Michael Strube","We present BPEmb, a collection of pre-trained subword unit embeddings in 275 languages, based on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed, BPEmb performs competitively, and for some languages bet- ter than alternative subword approaches, while requiring vastly fewer resources and no tokenization. BPEmb is available at this https URL
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,"Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, Partha Talukdar","Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE's effectiveness. We have made RESIDE's source code available to encourage reproducible research.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases,"Xiang Ren,Zeqiu Wu,Wenqi He,Meng Qu,Clare R. Voss,Heng Ji,Tarek F. Abdelzaher,Jiawei Han","Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object ""translation"" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.
",Fine-Grained Entity Recognition,10,0,,,
Entity Matching,Using the Semantic Web as a Source of Training Data,"Christian Bizer, Anna Primpeli, Ralph Peeters ","Deep neural networks are increasingly used
for tasks such as entity resolution, sentiment analysis,
and information extraction. As the methods are rather
training data hungry, it is necessary to use large training sets in order to enable the methods to play their
strengths.
Millions of websites have started to annotate structured data within HTML pages using the schema.org
vocabulary. Popular types of entities that are annotated
are products, reviews, events, people, hotels, and other
local businesses [11]. These semantic annotations are
used by all major search engines to display rich snippets
in search results. This is also the main driver behind the
wide-scale adoption of the annotation techniques. training dataset for product matching. The dataset consists of more than 20 million pairs of offers referring
to the same products. The offers were extracted from
43 thousand e-shops, that provide schema.org annotations including some form of product identifiers, such
as manufacturer part numbers (MPNs), global trade
item numbers (GTINs), or stock keeping units (SKUs).
The dataset, which we offer for public download, is orders of magnitude larger than the Walmart-Amazon [6],
Amazon-Google [9], and Abt-Buy [9] datasets that are
widely used to evaluate product matching methods. We
verify the utility of the dataset as training data by using it to replicate the recent result of Mugdal et al. [14]
stating that embeddings and RNNs outperform traditional symbolic matching methods on tasks involving
less structured data. After the case study on product
data matching, we focus on sentiment analysis and information extraction and discuss how semantic annotations from the Web can be used as training data within
both tasks.",Amazon-Google Dataset,11,1,Amazon-Google Dataset,"Christian Bizer, Anna Primpeli, Ralph Peeters","The Amazon-Google dataset for entity resolution derives from the online retailers Amazon.com and the product search service of Google accessible through the Google Base Data API. The dataset contains 1363 entities from amazon.com and 3226 google products as well as a gold standard (perfect mapping) with 1300 matching record pairs between the two data sources. The common attributes between the two data sources are: product name, product description, manufacturer and price.

The dataset was initially published in the repository of the Database Group of the University of Leipzig: https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution

To enable the reproducibility of the results and the comparability of the performance of different matchers on the Amazon-Google matching task, the dataset was split into fixed train, validation and test sets. The fixed splits are provided in the CompERBench repository"
Entity Matching,Using schema.org Annotations for Training and Maintaining Product Matcher,"Ralph Peeters, Anna Primpeli, Benedikt Wichtlhuber, Christian Bizer","Product matching is a central task within e-commerce applications such as price comparison portals and online market places. State-of-the-art product matching methods achieve F1 scores above 0.90 using deep learning techniques combined with huge amounts of training data (e.g > 100K pairs of offers). Gathering and maintaining such large training corpora is costly, as it implies labeling pairs of offers as matches or non-matches. Acquiring the ability to be good at product matching thus means a major investment for an e-commerce company. This paper shows that the manual labeling of training data for product matching can be replaced by relying exclusively on schema.org annotations gathered from the public Web. We show that using only schema.org data for training, we are able to achieve F1 scores between 0.92 and 0.95 depending on the product category. As new products appear everyday, it is important that matching models can be maintained with justifiable effort. In order to give practical advice on how to maintain matching models, we compare the performance of deep learning and traditional matching models on unseen products and experiment with different fine-tuning and re-training strategies for model maintenance, again using only schema.org annotations as training data. Finally, as using the public Web as distant supervision carries inherent noise, we evaluate deep learning and traditional matching models with regards to their label-noise resistance and show that deep learning is able to deal with the amounts of identifier-noise found in schema.org annotations.",Amazon-Google Dataset,11,0,,,
Entity Matching,Using Weak Supervision to Identify Long-Tail Entities for Knowledge Base Completion,"Yaser Oulabi, Christian Bizer","Data from relational web tables can be used to augment cross-domain knowledge bases like DBpedia, Wikidata, or the Google Knowledge Graph with descriptions of entities that are not yet part of the knowledge base. Such long-tail entities can include for instance small villages, niche songs, or athletes that play in lower-level leagues. In previous work, we have presented an approach to successfully assemble descriptions of long-tail entities from relational HTML tables using supervised matching methods and manually labeled training data in the form of positive and negative entity matches. Manually labeling training data is a laborious task given knowledge bases covering many different classes. In this work, we investigate reducing the labeling effort for the task of long-tail entity extraction by using weak supervision. We present a bootstrapping approach that requires domain experts to provide a small set of simple, class-specific matching rules, instead of requiring them to label a large set of entity matches, thereby reducing the human supervision effort considerably. We evaluate this weak supervision approach and find that it performs only slightly worse compared to methods that rely on large sets of manually labeled entity matches.",Amazon-Google Dataset,11,0,,,
Entity Matching,Catalog Integration of Heterogeneous and Volatile Product Data,"Oliver Schmidts, Bodo Kraft, Marvin Winkens, Albert Zündorf","The integration of frequently changing, volatile product data from different manufacturers into a single catalog is a significant challenge for small and medium-sized e-commerce companies. They rely on timely integrating product data to present them aggregated in an online shop without knowing format specifications, concept understanding of manufacturers, and data quality. Furthermore, format, concepts, and data quality may change at any time. Consequently, integrating product catalogs into a single standardized catalog is often a laborious manual task. Current strategies to streamline or automate catalog integration use techniques based on machine learning, word vectorization, or semantic similarity. However, most approaches struggle with low-quality or real-world data. We propose Attribute Label Ranking (ALR) as a recommendation engine to simplify the integration process of previously unknown, proprietary tabular format into a standardized catalog for practitioners. We evaluate ALR by focusing on the impact of different neural network architectures, language features, and semantic similarity. Additionally, we consider metrics for industrial application and present the impact of ALR in production and its limitations.",Amazon-Google Dataset,11,0,,,
Entity Matching,An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining,"Ziqi Zhang, Xingyi Song","The Linked Open Data practice has led to a significant growth of structured data on the Web in the last decade. Such structured data describe real-world entities in a machine-readable way, and have created an unprecedented opportunity for research in the field of Natural Language Processing. However, there is a lack of studies on how such data can be used, for what kind of tasks, and to what extent they can be useful for these tasks. This work focuses on the e-commerce domain to explore methods of utilising such structured data to create language resources that may be used for product classification and linking. We process billions of structured data points in the form of RDF n-quads, to create multi-million words of product-related corpora that are later used in three different ways for creating of language resources: training word embedding models, continued pre-training of BERT-like language models, and training Machine Translation models that are used as a proxy to generate product-related keywords. Our evaluation on an extensive set of benchmarks shows word embeddings to be the most reliable and consistent method to improve the accuracy on both tasks (with up to 6.9 percentage points in macro-average F1 on some datasets). The other two methods however, are not as useful. Our analysis shows that this could be due to a number of reasons, including the biased domain representation in the structured data and lack of vocabulary coverage. We share our datasets and discuss how our lessons learned could be taken forward to inform future research in this direction.
",Amazon-Google Dataset,11,0,,,
Entity Matching,Catalog Integration of Low-quality Product Data by Attribute Label Ranking,"Oliver Schmidts, Bodo Kraft, Marvin Winkens, Albert Zundorf ","The integration of product data from heterogeneous sources and manufacturers into a single catalog is often
still a laborious, manual task. Especially small- and medium-sized enterprises face the challenge of timely
integrating the data their business relies on to have an up-to-date product catalog, due to format specifications,
low quality of data and the requirement of expert knowledge. Additionally, modern approaches to simplify
catalog integration demand experience in machine learning, word vectorization, or semantic similarity that
such enterprises do not have. Furthermore, most approaches struggle with low-quality data. We propose
Attribute Label Ranking (ALR), an easy to understand and simple to adapt learning approach. ALR leverages
a model trained on real-world integration data to identify the best possible schema mapping of previously
unknown, proprietary, tabular format into a standardized catalog schema. Our approach predicts multiple
labels for every attribute of an input column. The whole column is taken into consideration to rank among
these labels. We evaluate ALR regarding the correctness of predictions and compare the results on real-world
data to state-of-the-art approaches. Additionally, we report findings during experiments and limitations of our
approach.",Amazon-Google Dataset,11,0,,,
Entity Matching,Augmenting cross-domain knowledge bases using web tables,Yaser Oulabi,"Cross-domain knowledge bases are increasingly used for a large variety of applications. As the usefulness of a knowledge base for many of these applications increases with its completeness, augmenting knowledge bases with new knowledge is an important task. A source for this new knowledge could be in the form of web tables, which are relational HTML tables extracted from the Web. This thesis researches data integration methods for cross-domain knowledge base augmentation from web tables. Existing methods have focused on the task of slot filling static data. We research methods that additionally enable augmentation in the form of slot filling time-dependent data and entity expansion. When augmenting knowledge bases using time-dependent web table data, we require time-aware fusion methods. They identify from a set of conflicting web table values the one that is valid given a certain temporal scope. A primary concern of time-aware fusion is therefore the estimation of temporal scope annotations, which web table data lacks. We introduce two time-aware fusion approaches. In the first, we extract timestamps from the table and its context to exploit as temporal scopes, additionally introducing approaches to reduce the sparsity and noisiness of these timestamps. We introduce a second time-aware fusion method that exploits a temporal knowledge base to propagate temporal scopes to web table data, reducing the dependence on noisy and sparse timestamps. Entity expansion enriches a knowledge base with previously unknown long-tail entities. It is a task that to our knowledge has not been researched before. We introduce the Long-Tail Entity Extraction Pipeline, the first system that can perform entity expansion from web table data. The pipeline works by employing identity resolution twice, once to disambiguate between entity occurrences within web tables, and once between entities created from web tables and existing entities in the knowledge base. In addition to identifying new long-tail entities, the pipeline also creates their descriptions according to the knowledge base schema. By running the pipeline on a large-scale web table corpus, we profile the potential of web tables for the task of entity expansion. We find, that given certain classes, we can enrich a knowledge base with tens and even hundreds of thousands new entities and corresponding facts. Finally, we introduce a weak supervision approach for long-tail entity extraction, where supervision in the form of a large number of manually labeled matching and non-matching pairs is substituted with a small set of bold matching rules build using the knowledge base schema. Using this, we can reduce the supervision effort required to train our pipeline to enable cross-domain entity expansion at web-scale. In the context of this research, we created and published two datasets. The Time-Dependent Ground Truth contains time-dependent knowledge with more than one million temporal facts and corresponding temporal scope annotations. It could potentially be employed for a large variety of tasks that consider the temporal aspect of data. We also built the Web Tables for Long-Tail Entity Extraction gold standard, the first benchmark for the task of entity expansion from web tables",Amazon-Google Dataset,11,0,,,
Entity Matching,"Del mobile-first al data-first: schema.org, búsquedas zero-click y la incertidumbre sobre los asistentes de voz From mobile-first to data-first: Schema.org, zero-click searches and the uncertainty about voice assistants",Tomás Saorín; Juan-Antonio Pastor-Sánchez,"This work reflects about semantic markup from the point of view of digital publishing and editing. It’s coined the term “data-first” with the meaning of the use of structured data in digital content to improve how search engines understand precisely the information these contents embodied. Evolution and adoption os Schema.org vocabulary is outlined, as a de facto web standard, due to its impact in web engine optimization and featured snippets in SERPs. Finally, contented phenomena as direct answers in search results and zero-click searches are presented as results of the spread of interaction with conversational assistants.",Amazon-Google Dataset,11,0,,,
Entity Matching,Performance Evaluation of the Semantic Web Reasoners,"U Sinha, S Tiwari","As the performance of semantic reasoners change significantly with regard to all included characteristics, and therefore requires assessment and evaluation before selecting an appropriate reasoner for a given application. There are number of inference engines like Pellet, FaCT++, Hermit, RacerPro, KaON2, F-OWL and BaseVISor. Some of them are reviewed and tested for few prebuilt ontologies. Paper presents a performance evaluation and comparison of semantic reasoner for ontology of Health and Anatomy domain. Reasoners are characterized based on reasoning method, reasoning algorithm, computational complexity, classification, scalability, query and rule support.

",Amazon-Google Dataset,11,0,,,
Entity Matching,Laying the Groundwork for Knowledge Base Population: Nine Years of Linguistic Resources for TAC KBP,"Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi Song, Jennifer Tracey","Knowledge Base Population (KBP) is an evaluation series within the Text Analysis Conference (TAC) evaluation campaign conducted
by the National Institute of Standards and Technology (NIST). Over the past nine years TAC KBP evaluations have targeted
information extraction technologies for the population of knowledge bases comprised of entities, relations, and events. Linguistic Data
Consortium (LDC) has supported TAC KBP since 2009, developing, maintaining, and distributing linguistic resources in three
languages for seven distinct evaluation tracks. This paper describes LDC's resource creation efforts for the various KBP tracks, and
highlights changes made over the years to support evolving evaluation requirements.",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2013,12,1,TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2013,"Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi Song, Jennifer Tracey","TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2013 was developed by the Linguistic Data Consortium (LDC) and contains training and evaluation data produced in support of the TAC KBP English Entity Linking tasks in 2009, 2010, 2011, 2012, and 2013. It includes queries and gold standard entity type information, Knowledge Base links, and equivalence class clusters for NIL entities. Also included are the source documents for the queries, specifically, English newswire, discussion forum and web data. The corresponding knowledge base is available as TAC KBP Reference Knowledge Base (LDC2014T16). Also included in this package are the results of an Entity Linking IAA (Inter-Annotator Agreement) study conducted in 2010. Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST). TAC was developed to encourage research in natural language processing and related applications by providing a large test collection, common evaluation procedures, and a forum for researchers to share their results. Through its various evaluations, the Knowledge Base Population (KBP) track of TAC encourages the development of systems that can match entities mentioned in natural texts with those appearing in a knowledge base and extract novel information about entities from a document collection and add it to a new or existing knowledge base. English Entity Linking was first conducted as part of the 2009 TAC KBP evaluations. Its goal is to measure systems’ ability to determine whether an entity, specified by a query, has a matching node in a reference knowledge base (KB) and, if so, to create a link between the two. If there is no matching node for a query entity in the KB, EL systems are required to cluster the mention together with others referencing the same entity. More information about the TAC KBP Entity Linking task and other TAC KBP evaluations can be found on the NIST TAC website. Data All source documents were originally released as XML but have been converted to text files for this release. This change was made primarily because the documents were used as text files during data development but also because some fail XML parsing."
Entity Matching,KBPearl: a knowledge base population system supported by joint entity and relation linking,"Xueling Lin, Haoyang Li, Hao Xin, Zijian Li, Lei Chen","Nowadays, most openly available knowledge bases (KBs) are
incomplete, since they are not synchronized with the emerging facts happening in the real world. Therefore, knowledge
base population (KBP) from external data sources, which
extracts knowledge from unstructured text to populate KBs,
becomes a vital task. Recent research proposes two types of
solutions that partially address this problem, but the performance of these solutions is limited. The first solution,
dynamic KB construction from unstructured text, requires
specifications of which predicates are of interest to the KB,
which needs preliminary setups and is not suitable for an
in-time population scenario. The second solution, Open Information Extraction (Open IE) from unstructured text, has
limitations in producing facts that can be directly linked to
the target KB without redundancy and ambiguity. In this
paper, we present an end-to-end system, KBPearl, for KBP,
which takes an incomplete KB and a large corpus of text
as input, to (1) organize the noisy extraction from Open
IE into canonicalized facts; and (2) populate the KB by
joint entity and relation linking, utilizing the context knowledge of the facts and the side information inferred from the
source text. We demonstrate the effectiveness and efficiency
of KBPearl against the state-of-the-art techniques, through
extensive experiments on real-world datasets.",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2014,12,0,,,
Entity Matching,KnowledgeNet: A Benchmark Dataset for Knowledge Base Population,"Filipe Mesquita, Matteo Cannaviccio, Jordan Schmidek, Paramita Mirza, Denilson Barbosa","KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2015,12,0,,,
Entity Matching,Open Knowledge Enrichment for Long-tail Entities,"Ermei Cao, Difeng Wang, Jiacheng Huang, Wei Hu","Knowledge bases (KBs) have gradually become a valuable asset for many AI applications. While many current KBs are quite large, they are widely acknowledged as incomplete, especially lacking facts of long-tail entities, e.g., less famous persons. Existing approaches enrich KBs mainly on completing missing links or filling missing values. However, they only tackle a part of the enrichment problem and lack specific considerations regarding long-tail entities. In this paper, we propose a full-fledged approach to knowledge enrichment, which predicts missing properties and infers true facts of long-tail entities from the open Web. Prior knowledge from popular entities is leveraged to improve every enrichment step. Our experiments on the synthetic and real-world datasets and comparison with related work demonstrate the feasibility and superiority of the approach.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2016,12,0,,,
Entity Matching,Automated Extraction of Socio-political Events from News (AESPEN): Workshop and Shared Task Report,"Ali Hürriyetoğlu, Vanni Zavarella, Hristo Tanev, Erdem Yörük, Ali Safaya, Osman Mutlu","We describe our effort on automated extraction of socio-political events from news in the scope of a workshop and a shared task we organized at Language Resources and Evaluation Conference (LREC 2020). We believe the event extraction studies in computational linguistics and social and political sciences should further support each other in order to enable large scale socio-political event information collection across sources, countries, and languages. The event consists of regular research papers and a shared task, which is about event sentence coreference identification (ESCI), tracks. All submissions were reviewed by five members of the program committee. The workshop attracted research papers related to evaluation of machine learning methodologies, language resources, material conflict forecasting, and a shared task participation report in the scope of socio-political event information collection. It has shown us the volume and variety of both the data sources and event information collection approaches related to socio-political events and the need to fill the gap between automated text processing techniques and requirements of social and political sciences.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2017,12,0,,,
Entity Matching,"Cross-Document, Cross-Language Event Coreference Annotation Using Event Hoppers","Zhiyi Song, Ann Bies, Justin Mott, Xuansong Li, Stephanie Strassel, Christopher Caruso","We discuss the development and implementation of an approach for cross-document, cross-lingual event coreference for the DEFT Rich
Entities, Relations and Events (Rich ERE) annotation task. Rich ERE defined the notion of event hoppers to enable intuitive withindocument coreference for the DEFT event ontology, and the expansion of coreference to cross-document, cross-lingual event mentions
relies crucially on this same construct. We created new annotation guidelines, data processes and user interfaces to enable annotation of
505 documents in three languages selected from data already labeled for Rich ERE, yielding 389 cross-document event hoppers. We
discuss the data creation process and the central role of event hoppers in making cross-document, cross-lingual coreference decisions.
We present the challenges encountered during annotation along with three directions for future work.",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2018,12,0,,,
Entity Matching,Scalable Knowledge Graph Construction from Text Collections,"Ryan Clancy, Ihab F. Ilyas, Jimmy Lin","We present a scalable, open-source platform that “distills” a potentially large text collection into a knowledge graph. Our platform takes documents stored in Apache Solr and scales out the Stanford CoreNLP toolkit via Apache Spark integration to extract mentions and relations that are then ingested into the Neo4j graph database. The raw knowledge graph is then enriched with facts extracted from an external knowledge graph. The complete product can be manipulated by various applications using Neo4j’s native Cypher query language: We present a subgraph-matching approach to align extracted relations with external facts and show that fact verification, locating textual support for asserted facts, detecting inconsistent and missing facts, and extracting distantly-supervised training data can all be performed within the same framework.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2019,12,0,,,
Entity Matching,New Insights into Cross-Document Event Coreference: Systematic Comparison and a Simplified Approach,"Andres Cremisini, Mark Finlayson","Cross-Document Event Coreference (CDEC) is the task of finding coreference relationships between events in separate documents, most commonly assessed using the Event Coreference Bank+ corpus (ECB+). At least two different approaches have been proposed for CDEC on ECB+ that use only event triggers, and at least four have been proposed that use both triggers and entities. Comparing these approaches is complicated by variation in the systems’ use of gold vs. computed labels, as well as variation in the document clustering pre-processing step. We present an approach that matches or slightly beats state-of-the-art performance on CDEC over ECB+ with only event trigger annotations, but with a significantly simpler framework and much smaller feature set relative to prior work. This study allows us to directly compare with prior systems and draw conclusions about the effectiveness of various strategies. Additionally, we provide the first cross-validated evaluation on the ECB+ dataset; the first explicit evaluation of the pairwise event coreference classification step; and the first quantification of the effect of document clustering on system performance. The last in particular reveals that while document clustering is a crucial pre-processing step, improvements can at most provide for a 3 point improvement in CDEC performance, though this might be attributable to ease of document clustering on ECB+.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2020,12,0,,,
Entity Matching,Set Generation Networks for End-to-End Knowledge Base Population,"Dianbo Sui, Chenhao Wang, Yubo Chen, Kang Liu, Jun Zhao, Wei Bi","The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the networks, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2021,12,0,,,
Entity Matching,GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System,"Manling Li, Ying Lin, Ananya Subburathinam, Spencer Whitehead, Xiaoman Pan, Di Lu, Qingyun Wang, Tongtao Zhang, Lifu Huang, Heng Ji, Alireza Zareian, Hassan Akbari, Brian Chen, Bo Wu, Emily Allaway, Shih-Fu Chang, Kathleen R. McKeown, Yixiang Yao, Jennifer Chen, Eric Berquist, Kexuan Sun, Xujun Peng, Ryan Gabbard, Marjorie Freedman, Pedro A. Szekely, T. K. Satish Kumar, Arka Sadhu, Ram Nevatia, Miguel E. Rodríguez, Yifan Wang, Yang Bai, Ali Sadeghian, Daisy Zhe Wang","In the past year the GAIA team has improved our end-to-end knowledge extraction, grounding, inference, clustering and hypothesis generation system that covers all languages (English, Russian and Ukrainian), data modalities and knowledge element types defined in new AIDA ontologies. We participated in the evaluations of all tasks within TA1, TA2 and TA3 and achieved highly competitive performance. Our TA1 system achieves top performance at both intrinsic evaluation and extrinsic evaluation through TA2 and TA3. The system incorporates a number of impactful and fresh research innovations

",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2022,12,0,,,
Entity Matching,BioRelEx 1.0: Biological Relation Extraction Benchmark,"Hrant Khachatrian, Lilit Nersisyan, Karen Hambardzumyan, Tigran Galstyan, Anna Hakobyan, Arsen Arakelyan, Andrey Rzhetsky, Aram Galstyan","Automatic extraction of relations and interactions between biological entities from scientific literature remains an extremely challenging problem in biomedical information extraction and natural language processing in general. One of the reasons for slow progress is the relative scarcity of standardized and publicly available benchmarks. In this paper we introduce BioRelEx, a new dataset of fully annotated sentences from biomedical literature that capture binding interactions between proteins and/or biomolecules. To foster reproducible research on the interaction extraction task, we define a precise and transparent evaluation process, tools for error analysis and significance tests. Finally, we conduct extensive experiments to evaluate several baselines, including SciIE, a recently introduced neural multi-task architecture that has demonstrated state-of-the-art performance on several tasks.
",TAC KBP English Entity Linking - Comprehensive Training and Evaluation Data 2009-2023,12,0,,,
Entity Matching,Extracting medical entities from social media,"Sanja Scepanovic, Enrique Martin-Lopez, Daniele Quercia, Khan Baykaner","Accurately extracting medical entities from social media is challenging because people use informal language with different expressions for the same concept, and they also make spelling mistakes. Previous work either focused on specific diseases (e.g., depression) or drugs (e.g., opioids) or, if working with a wide-set of medical entities, only tackled individual and small-scale benchmark datasets (e.g., AskaPatient). In this work, we first demonstrated how to accurately extract a wide variety of medical entities such as symptoms, diseases, and drug names on three benchmark datasets from varied social media sources, and then also validated this approach on a large-scale Reddit dataset.

We first implemented a deep-learning method using contextual embeddings that upon two existing benchmark datasets, one containing annotated AskaPatient posts (CADEC) and the other containing annotated tweets (Micromed), outperformed existing state-of-the-art methods. Second, we created an additional benchmark dataset by annotating medical entities in 2K Reddit posts (made publicly available under the name of MedRed) and showed that our method also performs well on this new dataset.

Finally, to demonstrate that our method accurately extracts a wide variety of medical entities on a large scale, we applied the model pre-trained on MedRed to half a million Reddit posts. The posts came from disease-specific subreddits so we could categorise them into 18 diseases based on the subreddit. We then trained a machine-learning classifier to predict the post's category solely from the extracted medical entities. The average F1 score across categories was .87. These results open up new cost-effective opportunities for modeling, tracking and even predicting health behavior at scale.",Models for Extracting Medical Entities from Text,13,1,Models for Extracting Medical Entities from Text,"Sanja Scepanovic, Enrique Martin-Lopez, Daniele Quercia, Khan Baykaner","Included are 3 pre-trained models on the 3 respective social media datasets: CADEC, Micromed, and MedRed, for extracting medical entities from text. These models can be used in the GitHub repo:https://github.com/sanja7s/MedRedMore details in the publication: Extracting Medical Entities from Social Media.Note: The files are large, around 1.7GB each."
Entity Matching,COMETA: A Corpus for Medical Entity Linking in the Social Media,"Marco Basaldella, Fangyu Liu, Ehsan Shareghi, Nigel Collier","Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Machine learning and algorithmic fairness in public and population health,"Vishwali Mhasawade, Yuan Zhao & Rumi Chunara","Until now, much of the work on machine learning and health has focused on processes inside the hospital or clinic. However, this represents only a narrow set of tasks and challenges related to health; there is greater potential for impact by leveraging machine learning in health tasks more broadly. In this Perspective we aim to highlight potential opportunities and challenges for machine learning within a holistic view of health and its influences. To do so, we build on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities. We present a brief introduction to research in these fields, data sources and types of tasks, and use these to identify settings where machine learning is relevant and can contribute to new knowledge. Given the key foci of health equity and disparities within public and population health, we juxtapose these topics with the machine learning subfield of algorithmic fairness to highlight specific opportunities where machine learning, public and population health may synergize to achieve health equity.

",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,How epidemic psychology works on Twitter: evolution of responses to the COVID-19 pandemic in the U.S.,"Luca Maria Aiello, Daniele Quercia, Ke Zhou, Marios Constantinides, Sanja Šćepanović & Sagar Joglekar","Disruptions resulting from an epidemic might often appear to amount to chaos but, in reality, can be understood in a systematic way through the lens of “epidemic psychology”. According to Philip Strong, the founder of the sociological study of epidemic infectious diseases, not only is an epidemic biological; there is also the potential for three psycho-social epidemics: of fear, moralization, and action. This work empirically tests Strong’s model at scale by studying the use of language of 122M tweets related to the COVID-19 pandemic posted in the U.S. during the whole year of 2020. On Twitter, we identified three distinct phases. Each of them is characterized by different regimes of the three psycho-social epidemics. In the refusal phase, users refused to accept reality despite the increasing number of deaths in other countries. In the anger phase (started after the announcement of the first death in the country), users’ fear translated into anger about the looming feeling that things were about to change. Finally, in the acceptance phase, which began after the authorities imposed physical-distancing measures, users settled into a “new normal” for their daily activities. Overall, refusal of accepting reality gradually died off as the year went on, while acceptance increasingly took hold. During 2020, as cases surged in waves, so did anger, re-emerging cyclically at each wave. Our real-time operationalization of Strong’s model is designed in a way that makes it possible to embed epidemic psychology into real-time models (e.g., epidemiological and mobility models).

",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Humane Visual AI: Telling the Stories Behind a Medical Condition,Wonyoung So; Edyta P. Bogucka; Sanja Šćepanović; Sagar Joglekar; Ke Zhou; Daniele Quercia,"A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these two aspects are hard to quantify and inherently difficult to communicate. To quantify psychological aspects, this work mined around half a million Reddit posts in the sub-communities specialised in 14 medical conditions, and it did so with a new deep-learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data. To finally visually communicate each medical condition's biological, psychological, and social aspects through storytelling, we designed a narrative-style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10% gave more importance to the psychological aspects of medical conditions, and 27% were more favourable to the use of social media data in healthcare, suggesting the importance of persuasive elements in interactive visualizations.
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Pressure Test: Good Stress for Company Success,"Sanja Šćepanović, Marios Constantinides, Daniele Quercia, Seunghyun Kim","Workplace stress is often considered to be negative, yet lab studies on individuals suggest that not all stress is bad. There are two types of stress: distress refers to harmful stimuli, while eustress refers to healthy, euphoric stimuli that create a sense of fulfillment and achievement. Telling the two types of stress apart is challenging, let alone quantifying their impact across corporations. We just did that for the S&P 500 companies in the U.S., and did so by leveraging a dataset of 440K company reviews published during twelve successive years, and developing a state-of-the-art deep-learning framework to accurately extract stress mentions from these reviews. We proposed a new methodology that places each company on a stress-by-rating quadrant (based on its overall stress score and overall rating on the site), and accordingly scores the company to be, on average, either a low stress, passive, negative stress, or positive stress company. We found that (former) employees of positive stress companies tended to describe high-growth and collaborative workplaces in their reviews, and that such companies' stock evaluations grew, on average, 5.1 times in 10 years (2009-2019) as opposed to the companies of the other three stress types that grew, on average, 3.7 times in the same time period. We also found that the four stress scores aggregated every year - from 2008 to 2020 - closely followed the unemployment rate in the U.S.: a year of positive stress (2008) was rapidly followed by several years of negative stress (2009-2015), which peaked during the Great Recession (2009-2011).
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Suicide Risk and Protective Factors in Online Support Forum Posts: Annotation Scheme Development and Validation Study,"Stevie Chancellor;  Steven A Sumner;  Corinne David-Ferdon, Tahirah Ahmad;  Munmun De Choudhury ","Background Online communities provide support for individuals looking for help with suicidal ideation and crisis. As community data are increasingly used to devise machine learning models to infer who might be at risk, there have been limited efforts to identify both risk and protective factors in web-based posts. These annotations can enrich and augment computational assessment approaches to identify appropriate intervention points, which are useful to public health professionals and suicide prevention researchers. Objective This qualitative study aims to develop a valid and reliable annotation scheme for evaluating risk and protective factors for suicidal ideation in posts in suicide crisis forums. Methods We designed a valid, reliable, and clinically grounded process for identifying risk and protective markers in social media data. This scheme draws on prior work on construct validity and the social sciences of measurement. We then applied the scheme to annotate 200 posts from r/SuicideWatch—a Reddit community focused on suicide crisis. Results We documented our results on producing an annotation scheme that is consistent with leading public health information coding schemes for suicide and advances attention to protective factors. Our study showed high internal validity, and we have presented results that indicate that our approach is consistent with findings from prior work. Conclusions Our work formalizes a framework that incorporates construct validity into the development of annotation schemes for suicide risk on social media. This study furthers the understanding of risk and protective factors expressed in social media data. This may help public health programming to prevent suicide and computational social science research and investigations that rely on the quality of labels for downstream machine learning tasks.
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Epidemic dreams: dreaming about health during the COVID-19 pandemic,"Sanja Šćepanović, Luca Maria Aiello, Deirdre Barrett and Daniele Quercia","The continuity hypothesis of dreams suggests that the content of dreams is continuous with the dreamer's waking experiences. Given the unprecedented nature of the experiences during COVID-19, we studied the continuity hypothesis in the context of the pandemic. We implemented a deep-learning algorithm that can extract mentions of medical conditions from text and applied it to two datasets collected during the pandemic: 2888 dream reports (dreaming life experiences), and 57 milion tweets (waking life experiences) mentioning the pandemic. The health expressions common to both sets were typical COVID-19 symptoms (e.g. cough, fever and anxiety), suggesting that dreams reflected people's real-world experiences. The health expressions that distinguished the two sets reflected differences in thought processes: expressions in waking life reflected a linear and logical thought process and, as such, described realistic symptoms or related disorders (e.g. nasal pain, SARS, H1N1); those in dreaming life reflected a thought process closer to the visual and emotional spheres and, as such, described either conditions unrelated to the virus (e.g. maggots, deformities, snake bites), or conditions of surreal nature (e.g. teeth falling out, body crumbling into sand). Our results confirm that dream reports represent an understudied yet valuable source of people's health experiences in the real world.
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,CREATING A HEALTH TAXONOMY WITH SOCIAL MEDIA,"Sanja Scepanovic, Luca Maria Aiello, Ke Zhou, Daniele Quercia","Since the uptake of social media, researchers have mined online discussions to track the outbreak and evolution of specific diseases or chronic conditions such as influenza or depression. To broaden the set of diseases under study, we developed a Deep Learning tool for Natural Language Processing that extracts mentions of virtually any medical condition or disease from unstructured social media text. With that tool at hand, we processed Reddit and Twitter posts, analyzed the clusters of the two resulting co-occurrence networks of conditions, and discovered that they correspond to well-defined categories of medical conditions. This resulted in the creation of the first comprehensive taxonomy of medical conditions automatically derived from online discussions. We validated the structure of our taxonomy against the official International Statistical Classification of Diseases and Related Health Problems (ICD-11), finding matches of our clusters with 20 official categories, out of 22. Based on the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced in the U.S., we were then able to compute disease-specific health scores. As opposed to counts of disease mentions or counts with no knowledge of our taxonomy's structure, we found that our disease-specific health scores are causally linked with the officially reported prevalence of 18 conditions.
",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Detecting adherence to the recommended childhood vaccination schedule from user-generated content in a US parenting forum,"Lorenzo Betti ,Gianmarco De Francisci Morales,Laetitia Gauvin,Kyriaki Kalimeri,Yelena Mejova,Daniela Paolotti,Michele Starnini","Vaccine hesitancy is considered as one of the leading causes for the resurgence of vaccine preventable diseases. A non-negligible minority of parents does not fully adhere to the recommended vaccination schedule, leading their children to be partially immunized and at higher risk of contracting vaccine preventable diseases. Here, we leverage more than one million comments of 201,986 users posted from March 2008 to April 2019 on the public online forum BabyCenter US to learn more about such parents. For 32% with geographic location, we find the number of mapped users for each US state resembling the census population distribution with good agreement. We employ Natural Language Processing to identify 6884 and 10,131 users expressing their intention of following the recommended and alternative vaccination schedule, respectively RSUs and ASUs. From the analysis of their activity on the forum we find that ASUs have distinctly different interests and previous experiences with vaccination than RSUs. In particular, ASUs are more likely to follow groups focused on alternative medicine, are two times more likely to have experienced adverse events following immunization, and to mention more serious adverse reactions such as seizure or developmental regression. Content analysis of comments shows that the resources most frequently shared by both groups point to governmental domains (.gov). Finally, network analysis shows that RSUs and ASUs communicate between each other (indicating the absence of echo chambers), however with the latter group being more endogamic and favoring interactions with other ASUs. While our findings are limited to the specific platform analyzed, our approach may provide additional insights for the development of campaigns targeting parents on digital platforms.

",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,Self-disclosure on Twitter During the COVID-19 Pandemic: A Network Perspective,"Prasanna Umar, Chandan Akiti, Anna Squicciarini, Sarah Rajtmajer","Amidst social distancing, quarantines, and everyday disruptions caused by the COVID-19 pandemic, users’ heightened activity on online social media has provided enhanced opportunities for self-disclosure. We study the incidence and the evolution of self-disclosure temporally as important events unfold throughout the pandemic’s timeline. Using a BERT-based supervised learning approach, we label a dataset of over 31 million COVID-19 related tweets for self-disclosure. We map users’ self-disclosure patterns, characterize personal revelations, and examine users’ disclosures within evolving reply networks. We employ natural language processing models and social network analyses to investigate self-disclosure patterns in users’ interaction networks as they seek social connectedness and focused conversations during COVID-19 pandemic. Our analyses show heightened self-disclosure levels in tweets following the World Health Organization’s declaration of pandemic worldwide on March 11, 2020. We disentangle network-level patterns of self-disclosure and show how self-disclosure characterizes temporally persistent social connections. We argue that in pursuit of social rewards users intentionally self-disclose and associate with similarly disclosing users. Finally, our work illustrates that in this pursuit users may disclose intimate personal health information such as personal ailments and underlying conditions which pose privacy risks.",Models for Extracting Medical Entities from Text,13,0,,,
Entity Matching,MMKG: Multi-modal Knowledge Graphs,"Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, David S. Rosenblum","We present MMKG, a collection of three knowledge graphs that contain both numerical features and (links to) images for all entities as well as entity alignments between pairs of KGs. Therefore, multi-relational link prediction and entity matching communities can benefit from this resource. We believe this data set has the potential to facilitate the development of novel multi-modal learning approaches for knowledge graphs.We validate the utility ofMMKG in the sameAs link prediction task with an extensive set of experiments. These experiments show that the task at hand benefits from learning of multiple feature types.
",MMKG Dataset,14,1,MMKG Dataset,"Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, David S. Rosenblum","MMKG is a collection of three knowledge graphs for link prediction and entity matching research. Contrary to other knowledge graph datasets, these knowledge graphs contain both numerical features and images for all entities as well as entity alignments between pairs of KGs. While MMKG is intended to perform relational reasoning across different entities and images, previous resources are intended to perform visual reasoning within the same image.

The three knowledge graphs augmented with numerical features and images are called FB15k, YAGO15k, and DBPEDIA15k."
Entity Matching,Kadaster Knowledge Graph: Beyond the Fifth Star of Open Data,"Stanislav Ronzhin,Erwin Folmer,Pano Maria, Marco Brattinga, Wouter Beek, Rob Lemmens, Rein van’t Veer","After more than a decade, the supply-driven approach to publishing public (open) data has resulted in an ever-growing number of data silos. Hundreds of thousands of datasets have been catalogued and can be accessed at data portals at different administrative levels. However, usually, users do not think in terms of datasets when they search for information. Instead, they are interested in information that is most likely scattered across several datasets. In the world of proprietary in-company data, organizations invest heavily in connecting data in knowledge graphs and/or store data in data lakes with the intention of having an integrated view of the data for analysis. With the rise of machine learning, it is a common belief that governments can improve their services, for example, by allowing citizens to get answers related to government information from virtual assistants like Alexa or Siri. To provide high-quality answers, these systems need to be fed with knowledge graphs. In this paper, we share our experience of constructing and using the first open government knowledge graph in the Netherlands. Based on the developed demonstrators, we elaborate on the value of having such a graph and demonstrate its use in the context of improved data browsing, multicriteria analysis for urban planning, and the development of location-aware chat bots. View Full-Text
",MMKG Dataset,14,0,,,
Entity Matching,Multi-Task Learning for Recommendation Over Heterogeneous Information Network,Hui Li; Yanlin Wang; Ziyu Lyu; Jieming Shi,"Traditional recommender systems (RS) only consider homogeneous data and cannot fully model heterogeneous information of complex objects and relations. Recent advances in the study of Heterogeneous Information Network (HIN) have shed some light on how to leverage heterogeneous information in RS. However, existing HIN-based recommendation models assume HIN is invariable and merely use HIN as a data source for assisting recommendation, which limits their performance. In this paper, we propose a multi-task learning framework, called MTRec, for recommendation over HIN. MTRec relies on self-attention mechanism to learn the semantics of meta-paths in HIN and jointly optimizes the tasks of both recommendation and link prediction. Using a Bayesian task weight learner, MTRec is able to achieve the balance of two tasks during optimization automatically. Moreover, MTRec provides good interpretabilities of recommendation through a “translation” mechanism which is used to model the three-way interactions among users, items and the meta-paths connecting them. Experimental results demonstrate the superiority of MTRec over state-of-the-art HIN-based recommendation models, and the case studies we provide illustrate that MTRec enhances the explainability of RS.
",MMKG Dataset,14,0,,,
Entity Matching,LogicENN: A Neural Based Knowledge Graphs Embedding Model with Logical Rules,Mojtaba Nayyeri; Chengjin Xu; Mirza Mohtashim Alam; Jens Lehmann; Hamed Shariat Yazdi,"Knowledge graph embedding models have gained significant attention in AI research. Recent works have shown that the inclusion of background knowledge, such as logical rules, can improve the performance of embeddings in downstream machine learning tasks. However, so far, most existing models do not allow the inclusion of rules. We address the challenge of including rules and present a new neural based embedding model (LogicENN). We prove that LogicENN can learn every ground truth of encoded rules in a knowledge graph. To the best of our knowledge, this has not been proved so far for the neural based family of embedding models. Moreover, we derive formulae for the inclusion of various rules, including (anti-)symmetric, inverse, irreflexive and transitive, implication, composition, equivalence and negation. Our formulation allows to avoid grounding for implication and equivalence relations. Our experiments show that LogicENN outperforms the state-of-the-art models in link prediction.
",MMKG Dataset,14,0,,,
Entity Matching,Visual Pivoting for (Unsupervised) Entity Alignment,"Fangyu Liu, Muhao Chen, Dan Roth, Nigel Collier","This work studies the use of visual semantic representations to align entities in heterogeneous knowledge graphs (KGs). Images are natural components of many existing KGs. By combining visual knowledge with other auxiliary information, we show that the proposed new approach, EVA, creates a holistic entity representation that provides strong signals for cross-graph entity alignment. Besides, previous entity alignment methods require human labelled seed alignment, restricting availability. EVA provides a completely unsupervised solution by leveraging the visual similarity of entities to create an initial seed dictionary (visual pivots). Experiments on benchmark data sets DBP15k and DWY15k show that EVA offers state-of-the-art performance on both monolingual and cross-lingual entity alignment tasks. Furthermore, we discover that images are particularly useful to align long-tail KG entities, which inherently lack the structural contexts necessary for capturing the correspondences.
",MMKG Dataset,14,0,,,
Entity Matching,Richpedia: A Comprehensive Multi-modal Knowledge Graph,"Meng Wang, Guilin Qi, HaoFen Wang, Qiushuo Zheng","Large-scale knowledge graphs such as Wikidata and DBpedia have become a powerful asset for semantic search and question answering. However, most of the knowledge graph construction works focus on organizing and discovering textual knowledge in a structured representation while paying little attention to the proliferation of visual resources on the Web. To improve the situation, in this paper, we present Richpedia, aim to provide a comprehensive multi-modal knowledge graph by distributing sufficient and diverse images to textual entities in Wikidata. We also set RDF links (visual semantic relations) between image entities based on the hyperlinks and descriptions in Wikipedia. The Richpedia resource is accessible on the Web via a faceted query endpoint and provides a pathway for knowledge graph and computer vision tasks, such as link prediction and visual relation detection.",MMKG Dataset,14,0,,,
Entity Matching,Injecting Background Knowledge into Embedding Models for Predictive Tasks on Knowledge Graphs,"Claudia d’Amato, Nicola Flavio Quatraro, Nicola Fanizzi","Embedding models have been successfully exploited for Knowledge Graph refinement. In these models, the data graph is projected into a lowdimensional space, in which graph structural information are preserved as much as possible, enabling an efficient computation of solutions. We propose a solution for injecting available background knowledge (schema axioms) to further improve the quality of the embeddings. The method has been applied to enhance existing models to produce embeddings that can encode knowledge that is not merely observed but rather derived by reasoning on the available axioms. An experimental evaluation on link prediction and triple classification tasks proves the improvement yielded implementing the proposed method over the original ones.
",MMKG Dataset,14,0,,,
Entity Matching,End-to-End Entity Classification on Multimodal Knowledge Graphs,"W.X. Wilcke, P. Bloem, V. de Boer, R.H. van t Veer, F.A.H. van Harmelen ","End-to-end multimodal learning on knowledge graphs has been left largely unaddressed. Instead, most end-to-end models such as message passing networks learn solely from the relational information encoded in graphs' structure: raw values, or literals, are either omitted completely or are stripped from their values and treated as regular nodes. In either case we lose potentially relevant information which could have otherwise been exploited by our learning methods. To avoid this, we must treat literals and non-literals as separate cases. We must also address each modality separately and accordingly: numbers, texts, images, geometries, et cetera. We propose a multimodal message passing network which not only learns end-to-end from the structure of graphs, but also from their possibly divers set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including images and geometries, which are projected into a joint representation space together with their relational information. We demonstrate our model on a node classification task, and evaluate the effect that each modality has on the overall performance. Our result supports our hypothesis that including information from multiple modalities can help our models obtain a better overall performance.
",MMKG Dataset,14,0,,,
Entity Matching,Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction,"Paolo Rosso, Dingqi Yang, Philippe Cudré-Mauroux","Knowledge Graph (KG) embeddings are a powerful tool for predicting missing links in KGs. Existing techniques typically represent a KG as a set of triplets, where each triplet (h, r, t) links two entities h and t through a relation r, and learn entity/relation embeddings from such triplets while preserving such a structure. However, this triplet representation oversimplifies the complex nature of the data stored in the KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h, r, t), but also the associated key-value pairs (k, v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. To address this issue, we propose HINGE, a hyper-relational KG embedding model, which directly learns from hyper-relational facts in a KG. HINGE captures not only the primary structural information of the KG encoded in the triplets, but also the correlation between each triplet and its associated key-value pairs. Our extensive evaluation shows the superiority of HINGE on various link prediction tasks over KGs. In particular, HINGE consistently outperforms not only the KG embedding methods learning from triplets only (by 0.81-41.45% depending on the link prediction tasks and settings), but also the methods learning from hyper-relational facts using the n-ary representation (by 13.2-84.1%).

",MMKG Dataset,14,0,,,
Entity Matching,Knowledge Graph Embeddings and Explainable AI,"Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini","Knowledge graph embeddings are now a widely adopted approach to knowledge representation in which entities and relationships are embedded in vector spaces. In this chapter, we introduce the reader to the concept of knowledge graph embeddings by explaining what they are, how they can be generated and how they can be evaluated. We summarize the state-of-the-art in this field by describing the approaches that have been introduced to represent knowledge in the vector space. In relation to knowledge representation, we consider the problem of explainability, and discuss models and methods for explaining predictions obtained via knowledge graph embeddings.
",MMKG Dataset,14,0,,,
Entity Matching,Multiple Knowledge GraphDB (MKGDB),"Stefano Faralli, Paola Velardi, Farid Yusifli","We present MKGDB, a large-scale graph database created as a combination of multiple taxonomy backbones extracted from 5 existing knowledge graphs, namely: ConceptNet, DBpedia, WebIsAGraph, WordNet and the Wikipedia category hierarchy. MKGDB, thanks the versatility of the Neo4j graph database manager technology, is intended to favour and help the development of open-domain natural language processing applications relying on knowledge bases, such as information extraction, hypernymy discovery, topic clustering, and others. Our resource consists of a large hypernymy graph which counts more than 37 million nodes and more than 81 million hypernymy relations.
",MMKG Dataset,14,0,,,
Entity Matching,Aligning geographic entities from historical maps for building knowledge graphs,"Kai Sun,Yingjie Hu,Jia Song, Yunqiang Zhu","Historical maps contain rich geographic information about the past of a region. They are sometimes the only source of information before the availability of digital maps. Despite their valuable content, it is often challenging to access and use the information in historical maps, due to their forms of paper-based maps or scanned images. It is even more time-consuming and labor-intensive to conduct an analysis that requires a synthesis of the information from multiple historical maps. To facilitate the use of the geographic information contained in historical maps, one way is to build a geographic knowledge graph (GKG) from them. This paper proposes a general workflow for completing one important step of building such a GKG, namely aligning the same geographic entities from different maps. We present this workflow and the related methods for implementation, and systematically evaluate their performances using two different datasets of historical maps. The evaluation results show that machine learning and deep learning models for matching place names are sensitive to the thresholds learned from the training data, and a combination of measures based on string similarity, spatial distance, and approximate topological relation achieves the best performance with an average F-score of 0.89.",Aligning geographic entities from historical maps for building knowledge graphs,15,1,Aligning geographic entities from historical maps for building knowledge graphs,"Kai Sun,Yingjie Hu,Jia Song, Yunqiang Zhu","This is the data and codes that support the findings of the IJGIS paper ""Aligning geographic entities from historical maps for building knowledge graphs"".
"
Entity Matching,Semantic conflation in GIScience: a systematic review,"Luis M. Vilches-Blázquez, José Ángel Ramos ","Manifold providers from a wide range of initiatives (private organizations, volunteered efforts, social media, etc.) offer enormous data amounts with geospatial characteristics. These efforts of many data providers entail multiple data scenarios and imply many viewpoints about the same feature, involving different representations, accuracy, models, vocabularies, etc. Various techniques or processes are employed to deal with these heterogeneity problems related to diverse data sources within the conflation research area. However, semantic conflation has not been addressed widely in the literature, unlike geometrical conflation. Hence, it is unclear what issues semantic conflation tries to solve and what activities, methods, metrics, and techniques have been used in existing GIScience investigations. In this article, we carry out a systematic review of approaches that focus on semantic aspects for geospatial data conflation. Besides, we analyze a wide selection of contributions following different criteria to depict a detailed semantic conflation status in GIScience. Our contributions are: (i) an overview of semantic conflation application domains, (ii) a characterization of semantic issues within these domains, (iii) the recognition of gaps and weaknesses of collected researches, and (iv) several open challenges and opportunities for next steps in this GIScience research area.

",Aligning geographic entities from historical maps for building knowledge graphs,15,0,,,
Entity Matching,Combining Remote-Sensing-Derived Data and Historical Maps for Long-Term Back-Casting of Urban Extents,"Johannes H. Uhl, Stefan Leyk, Zekun Li, Weiwei Duan, Basel Shbita, Yao-Yi Chiang , Craig A. Knoblock","Spatially explicit, fine-grained datasets describing historical urban extents are rarely available prior to the era of operational remote sensing. However, such data are necessary to better understand long-term urbanization and land development processes and for the assessment of coupled nature–human systems (e.g., the dynamics of the wildland–urban interface). Herein, we propose a framework that jointly uses remote-sensing-derived human settlement data (i.e., the Global Human Settlement Layer, GHSL) and scanned, georeferenced historical maps to automatically generate historical urban extents for the early 20th century. By applying unsupervised color space segmentation to the historical maps, spatially constrained to the urban extents derived from the GHSL, our approach generates historical settlement extents for seamless integration with the multi-temporal GHSL. We apply our method to study areas in countries across four continents, and evaluate our approach against historical building density estimates from the Historical Settlement Data Compilation for the US (HISDAC-US), and against urban area estimates from the History Database of the Global Environment (HYDE). Our results achieve Area-under-the-Curve values >0.9 when comparing to HISDAC-US and are largely in agreement with model-based urban areas from the HYDE database, demonstrating that the integration of remote-sensing-derived observations and historical cartographic data sources opens up new, promising avenues for assessing urbanization and long-term land cover change in countries where historical maps are available.
",Aligning geographic entities from historical maps for building knowledge graphs,15,0,,,
Entity Matching,Narrative Cartography with Knowledge Graphs,"Gengchen Mai, Weiming Huang, Ling Cai, Rui Zhu, Ni Lao","Narrative cartography is a discipline which studies the interwoven nature of stories and maps. However, conventional geovisualization techniques of narratives often encounter several prominent challenges, including the data acquisition & integration challenge and the semantic challenge. To tackle these challenges, in this paper, we propose the idea of narrative cartography with knowledge graphs (KGs). Firstly, to tackle the data acquisition & integration challenge, we develop a set of KG-based GeoEnrichment toolboxes to allow users to search and retrieve relevant data from integrated cross-domain knowledge graphs for narrative mapping from within a GISystem. With the help of this tool, the retrieved data from KGs are directly materialized in a GIS format which is ready for spatial analysis and mapping. Two use cases — Magellan’s expedition and World War II — are presented to show the effectiveness of this approach. In the meantime, several limitations are identified from this approach, such as data incompleteness, semantic incompatibility, and the semantic challenge in geovisualization. For the later two limitations, we propose a modular ontology for narrative cartography, which formalizes both the map content (Map Content Module) and the geovisualization process (Cartography Module). We demonstrate that, by representing both the map content and the geovisualization process in KGs (an ontology), we can realize both data reusability and map reproducibility for narrative cartography.

",Aligning geographic entities from historical maps for building knowledge graphs,15,0,,,
Entity Matching,AIDA-light: High-Throughput Named-Entity Disambiguation,"Dat Ba Nguyen, Johannes Hoffart, Martin Theobald, Gerhard Weikum","To advance the Web of Linked Data, mapping ambiguous
names in structured and unstructured contents onto knowledge bases would be a vital asset. State-of-the-art methods
for Named Entity Disambiguation (NED) face major tradeoffs regarding efficiency/scalability vs. accuracy. Fast methods use relatively simple context features and avoid computationally expensive algorithms for joint inference. While
doing very well on prominent entities in clear input texts,
these methods achieve only moderate accuracy when fed
with difficult inputs. On the other hand, methods that
rely on rich context features and joint inference for mapping names onto entities pay the price of being much slower.
This paper presents AIDA-light which achieves high accuracy on difficult inputs while also being fast and scalable.
AIDA-light uses a novel kind of two-stage mapping algorithm. It first identifies a set of “easy” mentions with low
ambiguity and links them to entities in a very efficient manner. This stage also determines the thematic domain of the
input text as an important and novel kind of feature. The
second stage harnesses the high-confidence linkage for the
“easy” mentions to establish more reliable contexts for the
disambiguation of the remaining mentions. Our experiments
with four different datasets demonstrates that the accuracy
of AIDA-light is competitive to the very best NED systems,
while its run-time is comparable to or better than the performance of the fastest systems.",AIDA CoNLL-YAGO Dataset,16,1,AIDA CoNLL-YAGO Dataset,"Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, Gerhard Weikum
","AIDA CoNLL-YAGO contains assignments of entities to the mentions of named entities annotated for the original CoNLL 2003 entity recognition task. The entities are identified by YAGO2 entity name, by Wikipedia URL, or by Freebase mid
"
Entity Matching,J-NERD: Joint Named Entity Recognition and Disambiguation with Rich Linguistic Features,"Dat Ba Nguyen, Martin Theobald, Gerhard Weikum","Methods for Named Entity Recognition and Disambiguation (NERD) perform NER and NED in two separate stages. Therefore, NED may be penalized with respect to precision by NER false positives, and suffers in recall from NER false negatives. Conversely, NED does not fully exploit information computed by NER such as types of mentions. This paper presents J-NERD, a new approach to perform NER and NED jointly, by means of a probabilistic graphical model that captures mention spans, mention types, and the mapping of mentions to entities in a knowledge base. We present experiments with different kinds of texts from the CoNLL’03, ACE’05, and ClueWeb’09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1.
",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Plato: A Selective Context Model for Entity Resolution,"Nevena Lazic, Amarnag Subramanya, Michael Ringgaard, Fernando Pereira","We present Plato, a probabilistic model for entity resolution that includes a novel approach
for handling noisy or uninformative features,
and supplements labeled training data derived
from Wikipedia with a very large unlabeled
text corpus. Training and inference in the proposed model can easily be distributed across
many servers, allowing it to scale to over 107
entities. We evaluate Plato on three standard
datasets for entity resolution. Our approach
achieves the best results to-date on TAC KBP
2011 and is highly competitive on both the
CoNLL 2003 and TAC KBP 2012 datasets.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,REDEN: Named Entity Linking in Digital Literary Editions Using Linked Data Sets,"Carmen Brando, Francesca Frontini, Jean-Gabriel Ganascia","This paper proposes a graph-based Named Entity Linking (NEL) algorithm named REDEN for the disambiguation of authors' names in French literary criticism texts and scientific essays from the 19th and early 20th centuries. The algorithm is described and evaluated according to the two phases of NEL as reported in current state of the art, namely, candidate retrieval and candidate selection. REDEN leverages knowledge from different Linked Data sources in order to select candidates for each author mention, subsequently crawls data from other Linked Data sets using equivalence links (e.g., owl:sameAs), and, finally, fuses graphs of homologous individuals into a non-redundant graph well-suited for graph centrality calculation; the resulting graph is used for choosing the best referent. The REDEN algorithm is distributed in open-source and follows current standards in digital editions (TEI) and semantic Web (RDF). Its integration into an editorial workflow of digital editions in Digital humanities and cultural heritage projects is entirely plausible. Experiments are conducted along with the corresponding error analysis in order to test our approach and to help us to study the weaknesses and strengths of our algorithm, thereby to further improvements of REDEN.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Joint Learning of Local and Global Features for Entity Linking via Neural Networks,"Thien Huu Nguyen, Nicolas Fauceglia, Mariano Rodriguez Muro, Oktie Hassanzadeh, , Alfio Massimiliano Gliozzo, Mohammad Sadoghi","Previous studies have highlighted the necessity for entity linking systems to capture the local entity-mention similarities and the global topical coherence. We introduce a novel framework based on convolutional neural networks and recurrent neural networks to simultaneously model the local and global features for entity linking. The proposed model benefits from the capacity of convolutional neural networks to induce the underlying representations for local contexts and the advantage of recurrent neural networks to adaptively compress variable length sequences of predictions for global constraints. Our evaluation on multiple datasets demonstrates the effectiveness of the model and yields the state-of-the-art performance on such datasets. In addition, we examine the entity linking systems on the domain adaptation setting that further demonstrates the cross-domain robustness of the proposed model.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Learning Dynamic Context Augmentation for Global Entity Linking,"Xiyuan Yang, Xiaotao Gu, Sheng Lin, Siliang Tang, Yueting Zhuang, Fei Wu, Zhigang Chen, Guoping Hu, Xiang Ren","Despite of the recent success of collective entity linking (EL) methods, these ""global"" inference methods may yield sub-optimal results when the ""all-mention coherence"" assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our model with different learning settings, base models, decision orders and attention mechanisms.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Named Entity Extraction for Knowledge Graphs: A Literature Overview,Tareq Al-Moslmi; Marc Gallofré Ocaña; Andreas L. Opdahl; Csaba Veres,"An enormous amount of digital information is expressed as natural-language (NL) text that is not easily processable by computers. Knowledge Graphs (KG) offer a widely used format for representing information in computer-processable form. Natural Language Processing (NLP) is therefore needed for mining (or lifting) knowledge graphs from NL texts. A central part of the problem is to extract the named entities in the text. The paper presents an overview of recent advances in this area, covering: Named Entity Recognition (NER), Named Entity Disambiguation (NED), and Named Entity Linking (NEL). We comment that many approaches to NED and NEL are based on older approaches to NER and need to leverage the outputs of state-of-the-art NER systems. There is also a need for standard methods to evaluate and compare named-entity extraction approaches. We observe that NEL has recently moved from being stepwise and isolated into an integrated process along two dimensions: the first is that previously sequential steps are now being integrated into end-to-end processes, and the second is that entities that were previously analysed in isolation are now being lifted in each other's context. The current culmination of these trends are the deep-learning approaches that have recently reported promising results.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases,"Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek","Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks,"Matthew Francis-Landau, Greg Durrett, Dan Klein","A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Information extraction meets the Semantic Web: A survey,"Martinez-Rodriguez, Jose L. , Hogan, Aidan,  Lopez-Arevalo, Ivan","We provide a comprehensive survey of the research literature that applies Information Extraction techniques in a Semantic Web setting. Works in the intersection of these two areas can be seen from two overlapping perspectives: using Semantic
Web resources (languages/ontologies/knowledge-bases/tools) to improve Information Extraction, and/or using Information Extraction to populate the Semantic Web. In more detail, we focus on the extraction and linking of three elements: entities, concepts and relations. Extraction involves identifying (textual) mentions referring to such elements in a given unstructured or semistructured input source. Linking involves associating each such mention with an appropriate disambiguated identifier referring
to the same element in a Semantic Web knowledge-base (or ontology), in some cases creating a new identifier where necessary.
With respect to entities, works involving (Named) Entity Recognition, Entity Disambiguation, Entity Linking, etc. in the context
of the Semantic Web are considered. With respect to concepts, works involving Term Extraction, Keyword Extraction, Topic
Modeling, Topic Labeling, etc., in the context of the Semantic Web are considered. Finally, with respect to relations, works
involving Relation Extraction in the context of the Semantic Web are considered. The focus of the majority of the survey is on
works applied to unstructured sources (text in natural language); however, we also provide an overview of works that develop
custom techniques adapted for semi-structured inputs, namely markup documents and web tables.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,Query-Driven On-The-Fly Knowledge Base Construction,"Dat Ba Nguyen, Abdalghani Abujabal, Nam Khanh Tran, Martin Theobald, Gerhard Weikum","Today’s openly available knowledge bases, such as DBpedia, Yago, Wikidata or Freebase, capture billions of facts about
the world’s entities. However, even the largest among these
(i) are still limited in up-to-date coverage of what happens
in the real world, and (ii) miss out on many relevant predicates that precisely capture the wide variety of relationships
among entities. To overcome both of these limitations, we
propose a novel approach to build on-the-fly knowledge bases
in a query-driven manner. Our system, called QKBfly, supports analysts and journalists as well as question answering on emerging topics, by dynamically acquiring relevant
facts as timely and comprehensively as possible. QKBfly is
based on a semantic-graph representation of sentences, by
which we perform three key IE tasks, namely named-entity
disambiguation, co-reference resolution and relation extraction, in a light-weight and integrated manner. In contrast
to Open IE, our output is canonicalized. In contrast to traditional IE, we capture more predicates, including ternary
and higher-arity ones. Our experiments demonstrate that
QKBfly can build high-quality, on-the-fly knowledge bases
that can readily be deployed, e.g., for the task of ad-hoc
question answering.",AIDA CoNLL-YAGO Dataset,16,0,,,
Entity Matching,XLore: A Large-scale English-Chinese Bilingual Knowledge Graph,"Zhigang Wang, Juan-Zi Li, Zhichun Wang, Shuangjie Li, Mingyang Li, Dongsheng Zhang, Yao Shi, Yongbin Liu, P. Zhang, Jie Tang","Current Wikipedia-based multilingual knowledge bases still suffer the following problems: (i) the scarcity of non-English knowledge,
(ii) the noise in the semantic relations and (iii) the limited coverage of
equivalent cross-lingual entities. In this demo, we present a large-scale
bilingual knowledge graph named XLore, which has adequately solved
the above problems.",XLORE,17,1,XLORE,"Hailong Jin, Chengjiang Li, Jing Zhang, Lei Hou, Juanzi Li, Peng Zhang
","XLORE is a large-scale Chinese-English cross-lingual knowledge graph. It integrates knowledge from various sources (i.e., Baidu Baike, Chinese and English Wikipedia) via a standard knowledge engineering lifecycle including knowledge modeling, acquisition and integration. The data set consists of a selected high-quality subset of XLORE including representative concepts, properties and instances. It uses separate files in TTL (Terse RDF Triple Language) format to store different types of knowledge, i.e., taxonomy, infoboxes, cross-lingual links, etc. This paper also uncovers the general framework for XLORE construction, as well as the technical details. Note that the following files are only data samples and please refer to the XLORE website (https://xlore.org/download.html) for more information.
"
Entity Matching,A Survey of Techniques for Constructing Chinese Knowledge Graphs and Their Applications,"Tianxing Wu, Guilin Qi, Cheng Li, Meng Wang ","With the continuous development of intelligent technologies, knowledge graph, the backbone of artificial intelligence, has attracted much attention from both academic and industrial communities due to its powerful capability of knowledge representation and reasoning. In recent years, knowledge graph has been widely applied in different kinds of applications, such as semantic search, question answering, knowledge management and so on. Techniques for building Chinese knowledge graphs are also developing rapidly and different Chinese knowledge graphs have been constructed to support various applications. Under the background of the “One Belt One Road (OBOR)” initiative, cooperating with the countries along OBOR on studying knowledge graph techniques and applications will greatly promote the development of artificial intelligence. At the same time, the accumulated experience of China in developing knowledge graphs is also a good reference to develop non-English knowledge graphs. In this paper, we aim to introduce the techniques of constructing Chinese knowledge graphs and their applications, as well as analyse the impact of knowledge graph on OBOR. We first describe the background of OBOR, and then introduce the concept and development history of knowledge graph and typical Chinese knowledge graphs. Afterwards, we present the details of techniques for constructing Chinese knowledge graphs, and demonstrate several applications of Chinese knowledge graphs. Finally, we list some examples to explain the potential impacts of knowledge graph on OBOR. View Full-Text",XLORE,17,0,,,
Entity Matching,Interlinking English and Chinese RDF data sets using machine translation,"Tatiana Lesnikova, Jérôme David, Jérôme Euzenat","Data interlinking is a difficult task particularly in a multilingual environment like the Web. In this paper, we evaluate the suitability of a Machine Translation approach to interlink RDF resources
described in English and Chinese languages. We represent resources as
text documents, and a similarity between documents is taken for similarity between resources. Documents are represented as vectors using two
weighting schemes, then cosine similarity is computed. The experiment
demonstrates that TF*IDF with a minimum amount of preprocessing
steps can bring high results.",XLORE,17,0,,,
Entity Matching,A Study on Big Knowledge and Its Engineering Issues,Ruqian Lu; Xiaolong Jin; Songmao Zhang; Meikang Qiu; Xindong Wu,"After entering the big data era, a new term of `big knowledge' has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed.",XLORE,17,0,,,
Entity Matching,XLORE2: Large-scale Cross-lingual Knowledge Graph Construction and Application,"Hailong Jin, Chengjiang Li, Jing Zhang, Lei Hou  , Juanzi Li, Peng Zhang","Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. Although XLORE is an English-Chinese bilingual knowledge graph, there are only 423,974 cross-lingual links between English instances and Chinese instances. We present XLORE2, an extension of the XLORE that is built automatically from Wikipedia, Baidu Baike and Hudong Baike. We add more facts by making cross-lingual knowledge linking, cross-lingual property matching and fine-grained type inference. We also design an entity linking system to demonstrate the effectiveness and broad coverage of XLORE2.",XLORE,17,0,,,
Entity Matching,Ten Years of Knowledge Harvesting: Lessons and Challenges,"Gerhard Weikum, Johannes Hoffart, Fabian Suchanek","This article is a retrospective on the theme of knowledge harvesting: automatically constructing large highquality knowledge bases from Internet sources. We draw on our experience in the Yago-Naga project over the
last decade, but consider other projects as well. The article discusses lessons learned on the architecture of a
knowledge harvesting system, and points out open challenges and research opportunities",XLORE,17,0,,,
Entity Matching,KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation,"Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, Xiaoyan Zhu","The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consist of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth to further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",XLORE,17,0,,,
Entity Matching,Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases,"Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek","Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.",XLORE,17,0,,,
Entity Matching,Automating the expansion of a knowledge graph,"SoYeop Yoo, OkRan Jeong","In order to make computers understand human languages and to reason, human knowledge needs to be represented and stored in a form that can be processed by computers. Knowledge graphs have been developed for use as a form of the knowledge base for words and general relationships among words. However, they have two limitations. One is that the knowledge graph is limited in size and scope for most of the human languages. Another is that they are not able to deal with neologisms that form a part of the human common sense. Addressing these problems, we have developed and validated PolarisX which can automatically expand a knowledge graph, by crawling and analyzing the news sites and social media in real-time. We utilize and fine-tune the pre-trained multilingual BERT model for the construction of knowledge graphs without language dependencies. We extract new relationships using the BERT-based relation extraction model and integrate them into the knowledge graph. We verify the novelty and accuracy of PolarisX. It deals with neologisms and does not have language dependencies.",XLORE,17,0,,,
Entity Matching,Knowledge Graph Construction Techniques,"LiuQiao, LiYang, DuanHong, LiuYao, QinZhiguang  ","Abstract: Google’s knowledge graph technology has drawn a lot of research attentions in recent years. However, due to the limited public disclosure of technical details, people find it difficult to understand the connotation and value of this technology. In this paper, we introduce the key techniques involved in the construction of knowledge graph in a bottom-up way, starting from a clearly defined concept and a technical architecture of the knowledge graph. Firstly, we describe in detail the definition and connotation of the knowledge graph, and then we propose the technical framework for knowledge graph construction, in which the construction process is divided into three levels according to the abstract level of the input knowledge materials, including the information extraction layer, the knowledge integration layer, and the knowledge processing layer, respectively. Secondly, the research status of the key technologies for each level are surveyed comprehensively and also investigated critically for the purposes of gradually revealing the mysteries of the knowledge graph technology, the state-of-the-art progress, and its relationship with related disciplines. Finally, five major research challenges in this area are summarized, and the corresponding key research issues are highlighted.",XLORE,17,0,,,
Entity Matching,User Generated Content Oriented Chinese Taxonomy Construction,"Jinyang Li, Chengyu Wang, Xiaofeng He, Rong Zhang, Ming Gao","The taxonomy is one of the basic components in knowledge graphs as it establishes types of classes and semantic relations among the classes. Taxonomies are normally constructed either manually, or by language-dependent rules or patterns for type and relation extraction or inference. Existing work on building taxonomies for knowledge graphs is mostly in English language environment. In this paper, we propose a novel approach for large-scale Chinese taxonomy construction based on user generated content. We take Chinese Wikipedia as the data source, develop methods to extract classes and their relations mined from user tagged categories, and build up the taxonomy using a bottom-up strategy. The algorithms can be easily applied to other Wiki-style data sources. The experiments show that the constructed Chinese taxonomy achieves better results in both quality and quantity.",XLORE,17,0,,,
Entity Matching,"Overview of Linguistic Resource for the TAC KBP 2014 Evaluations: Planning, Execution, and Results","Joe Ellis, Jeremy Getman, Stephanie Strassel","Knowledge Base Population (KBP) is an evaluation track of the Text Analysis Conference (TAC), a workshop series organized by the National Institute of Standards and Technology (NIST). In 2014, TAC KBP’s sixth year of operation,  the evaluations focused on six tracks targeting information extraction and question answering technologies: Entity
Linking, Slot Filling, Sentiment Slot
Filling, Cold Start, Event Argument
Extraction, and Entity Discovery &
Linking. Linguistic Data Consortium
(LDC) at the University of Pennsylvania
has supported the TAC KBP evaluations
since 2009 and continued in 2014,
maintaining and distributing existing
linguistic resources and producing new
data, including queries, human-generated
responses, assessments, and tools and
specifications. This paper describes
LDC's resource creation efforts and their
results in support of TAC KBP 2014.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2014,18,1,TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2014,"Joe Ellis, Jeremy Getman, Stephanie Strassel","Introduction TAC KBP Spanish Cross-Lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2014 was developed by the Linguistic Data Consortium (LDC) and contains training and evaluation data produced in support of the TAC KBP Spanish Cross-lingual Entity Linking tasks in 2012, 2013 and 2014. It includes queries and gold standard entity type information, Knowledge Base links, and equivalence class clusters for NIL entities along with the source documents for the queries, specifically, English and Spanish newswire, discussion forum and web data. The corresponding knowledge base is available as TAC KBP Reference Knowledge Base (LDC2014T16). Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST). TAC was developed to encourage research in natural language processing and related applications by providing a large test collection, common evaluation procedures, and a forum for researchers to share their results. Through its various evaluations, the Knowledge Base Population (KBP) track of TAC encourages the development of systems that can match entities mentioned in natural texts with those appearing in a knowledge base and extract novel information about entities from a document collection and add it to a new or existing knowledge base. Spanish cross-lingual entity linking was first conducted as part of the 2012 TAC KBP evaluations. The track was an extension of the monolingual English Entity Linking track (EL) whose goal was to measure systems’ ability to determine whether an entity, specified by a query, had a matching node in a reference knowledge base (KB) and, if so, to create a link between the two. If there was no matching node for a query entity in the KB, EL systems were required to cluster the mention together with others referencing the same entity. More information about the TAC KBP Entity Linking task and other TAC KBP evaluations can be found on the NIST TAC website. Data All source documents were originally released as XML but have been converted to text files for this release. This change was made primarily because the documents were used as text files during data development but also because some fail XML parsing.
"
Entity Matching,Overview of TAC-KBP2014 Entity Discovery and Linking Tasks,"Heng Ji, Joel Nothman, Ben Hachey","In this paper we give an overview of the Entity Discovery and Linking tasks at the Knowledge Base Population track at TAC 2014. In this year we introduced a new end-to-end English entity discovery and linking task which
requires a system to take raw texts
as input, automatically extract entity
mentions, link them to a knowledge base,
and cluster NIL mentions. In this paper we
provide an overview of the task definition,
annotation issues, successful methods and
research challenges associated with this
new task. This new task has attracted
a lot of participants and has intrigued
many interesting research problems and
potential approaches. We believe it’s
a promising task to be extended to a
tri-lingual setting in KBP2015.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2015,18,0,,,
Entity Matching,"From Light to Rich ERE:Annotation of Entities, Relations, and Events","Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant and Xiaoyi Ma","We describe the evolution of the Entities, Relations and Events (ERE) annotation task, created to support research and technology development within the DARPA DEFT program. We begin by describing the specification for Light ERE annotation, including the
motivation for the task within the context of
DEFT. We discuss the transition from Light
ERE to a more complex Rich ERE specification, enabling more comprehensive treatment
of phenomena of interest to DEFT.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2016,18,0,,,
Entity Matching,Event Nugget Annotation: Processes and Issues,"Teruko Mitamura, Yukari Yamakawa, Susan Holm","This paper describes the processes and issues of annotating event nuggets based on DEFT ERE Annotation Guidelines v1.3 and TACKBP Event Detection Annotation Guidelines
1.7. Using Brat Rapid Annotation Tool (brat),
newswire and discussion forum documents
were annotated. One of the challenges arising
from human annotation of documents is annotators’ disagreement about the way of tagging
events. We propose using Event Nuggets to
help meet the definitions of the specific
type/subtypes which are part of this project.
We present case studies of several examples
of event annotation issues, including discontinuous multi-word events representing single
events. Annotation statistics and consistency
analysis is provided to characterize the interannotator agreement, considering single term
events and multi-word events which are both
continuous and discontinuous. Consistency
analysis is conducted using a scorer to compare first pass annotated files against adjudicated files.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2017,18,0,,,
Entity Matching,ELDEN: Improved Entity Linking Using Densified Knowledge Graphs,"Priya Radhakrishnan, Partha Talukdar, Vasudeva Varma","Entity Linking (EL) systems aim to automatically map mentions of an entity in text to the corresponding entity in a Knowledge Graph (KG). Degree of connectivity of an entity in the KG directly affects an EL system’s ability to correctly link mentions in text to the entity in KG. This causes many EL systems to perform well for entities well connected to other entities in KG, bringing into focus the role of KG density in EL. In this paper, we propose Entity Linking using Densified Knowledge Graphs (ELDEN). ELDEN is an EL system which first densifies the KG with co-occurrence statistics from a large text corpus, and then uses the densified KG to train entity embeddings. Entity similarity measured using these trained entity embeddings result in improved EL. ELDEN outperforms state-of-the-art EL system on benchmark datasets. Due to such densification, ELDEN performs well for sparsely connected entities in the KG too. ELDEN’s approach is simple, yet effective. We have made ELDEN’s code and data publicly available.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2018,18,0,,,
Entity Matching,NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching,"Dingqi Yang, Paolo Rosso, Bin Li, Philippe Cudre-Mauroux","Embeddings have become a key paradigm to learn graph representations and facilitate downstream graph analysis tasks. Existing graph embedding techniques either sample a large number of node pairs from a graph to learn node embeddings via stochastic optimization, or factorize a high-order proximity/adjacency matrix of the graph via expensive matrix factorization. However, these techniques usually require significant computational resources for the learning process, which hinders their applications on large-scale graphs. Moreover, the cosine similarity preserved by these techniques shows suboptimal efficiency in downstream graph analysis tasks, compared to Hamming similarity, for example. To address these issues, we propose NodeSketch, a highly-efficient graph embedding technique preserving high-order node proximity via recursive sketching. Specifically, built on top of an efficient data-independent hashing/sketching technique, NodeSketch generates node embeddings in Hamming space. For an input graph, it starts by sketching the self-loop-augmented adjacency matrix of the graph to output low-order node embeddings, and then recursively generates k-order node embeddings based on the self-loop-augmented adjacency matrix and (k-1)-order node embeddings. Our extensive evaluation compares NodeSketch against a sizable collection of state-of-the-art techniques using five real-world graphs on two graph analysis tasks. The results show that NodeSketch achieves state-of-the-art performance compared to these techniques, while showing significant speedup of 9x-372x in the embedding learning process and 1.19x-1.68x speedup when performing downstream graph analysis tasks.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2019,18,0,,,
Entity Matching,"Document-level Sentiment Inference with Social, Faction, and Discourse Context","Eunsol Choi, Hannah Rashkin, Luke Zettlemoyer, Yejin Choi","We present a new approach for documentlevel sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for
all entities mentioned in a text. To encourage more complete and consistent predictions, we introduce an ILP that jointly
models (1) sentence- and discourse-level
sentiment cues, (2) factual evidence about
entity factions, and (3) global constraints
based on social science theories such
as homophily, social balance, and reciprocity. Together, these cues allow for rich
inference across groups of entities, including for example that CEOs and the companies they lead are likely to have similar sentiment towards others. We evaluate performance on new, densely labeled
data that provides supervision for all pairs,
complementing previous work that only
labeled pairs mentioned in the same sentence. Experiments demonstrate that the
global model outperforms sentence-level
baselines, by providing more coherent predictions across sets of related entities.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2020,18,0,,,
Entity Matching,Laying the Groundwork for Knowledge Base Population: Nine Years of Linguistic Resources for TAC KBP,"Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi Song, Jennifer Tracey","Knowledge Base Population (KBP) is an evaluation series within the Text Analysis Conference (TAC) evaluation campaign conducted by the National Institute of Standards and Technology (NIST). Over the past nine years TAC KBP evaluations have targeted
information extraction technologies for the population of knowledge bases comprised of entities, relations, and events. Linguistic Data
Consortium (LDC) has supported TAC KBP since 2009, developing, maintaining, and distributing linguistic resources in three
languages for seven distinct evaluation tracks. This paper describes LDC's resource creation efforts for the various KBP tracks, and
highlights changes made over the years to support evolving evaluation requirements.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2021,18,0,,,
Entity Matching,A Comparison of Event Representations in DEFT,"Ann Bies, Zhiyi Song, Jeremy Getman, Joe Ellis, Justin Mott, Stephanie Strassel, Martha Palmer, Teruko Mitamura, Marjorie Freedman, Heng Ji, Tim O’Gorman","This paper will discuss and compare event representations across a variety of types of event annotation: Rich Entities, Relations, and Events (Rich ERE), Light Entities, Relations,
and Events (Light ERE), Event Nugget (EN),
Event Argument Extraction (EAE), Richer
Event Descriptions (RED), and Event-Event
Relations (EER). Comparisons of event representations are presented, along with a comparison of data annotated according to each
event representation. An event annotation experiment is also discussed, including annotation for all of these representations on the
same set of sample data, with the purpose of
being able to compare actual annotation
across all of these approaches as directly as
possible. We walk through a brief example to
illustrate the various annotation approaches,
and to show the intersections among the various annotated data sets.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2022,18,0,,,
Entity Matching,Extracting Sentiment Attitudes From Analytical Texts,"Natalia Loukachevitch, Nicolay Rusnachenko","In this paper we present the RuSentRel corpus including analytical texts in the sphere of international relations. For each document we annotated sentiments from the author to mentioned named entities, and sentiments of relations between mentioned entities. In the current experiments, we considered the problem of extracting sentiment relations between entities for the whole documents as a three-class machine learning task. We experimented with conventional machine-learning methods (Naive Bayes, SVM, Random Forest).",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2023,18,0,,,
Entity Matching,MAVEN: A Massive General Domain Event Detection Dataset,"Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou","Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.",TAC KBP Spanish Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2012-2024,18,0,,,
Entity Matching,XLORE2: Large-scale Cross-lingual Knowledge Graph Construction and Application,"Hailong Jin, Chengjiang Li, Jing Zhang, Lei Hou , Juanzi Li, Peng Zhang","Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. Although XLORE is an English-Chinese bilingual knowledge graph, there are only 423,974 cross-lingual links between English instances and Chinese instances. We present XLORE2, an extension of the XLORE that is built automatically from Wikipedia, Baidu Baike and Hudong Baike. We add more facts by making cross-lingual knowledge linking, cross-lingual property matching and fine-grained type inference. We also design an entity linking system to demonstrate the effectiveness and broad coverage of XLORE2.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,1,XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,"Hailong Jin, Chengjiang Li, Jing Zhang, Lei Hou, Juanzi Li, Peng Zhang","One table and 11 figures. Table 1 shows XLORE2 statistics. Figure 1 shows the framework of XLORE2. Figure 2 is an example of cross-lingual knowledge linking. Figure 3 presents the framework of cross-lingual knowledge linking. Figure 4 is an example of cross-lingual property matching (attribute matching). Figure 5 shows the framework of cross-lingual property matching. Figure 6 presents an example of mistakenly derived facts. Figure 7 is the framework of cross-lingual knowledge validation. Figure 8 shows an example of fine-grained type inference. Figure 9 depicts the framework of fine-grained type inference. Figure 10 is an illustration of XLink. Figure 11 shows the interface of XLORE2 and XLink
"
Entity Matching,Knowledge graph construction from multiple online encyclopedias,"Tianxing Wu, Haofen Wang, Cheng Li, Guilin Qi, Xing Niu, Meng Wang, Lin Li, Chaomin Shi ","In recent years, lots of knowledge graphs built from Wikipedia, the largest multilingual online encyclopedia, have been published on the Web to support various applications. However, since non-English data in Wikipedia are sparse, some projects work on knowledge graph construction from multiple non-English online encyclopedias, but many technical details are missing, so it is hard to reuse their frameworks or techniques. In this paper, we propose a new framework to solve knowledge graph construction from multiple online encyclopedias. The core modules are knowledge extraction and knowledge linking. Knowledge extraction consists of regular extraction, i.e., extracting targeted article contents in the whole online encyclopedias periodically, and live extraction, which only extracts the article contents of new and updated entities. Knowledge linking utilizes heuristic lightweight entity matching strategies and a semi-supervised learning method to find duplicated entities and properties from different online encyclopedias. Experimental results show that our approaches for knowledge extraction and linking outperform state-of-the-art baselines in different evaluation metrics, and our framework can generate a large-scale knowledge graph after inputting multiple online encyclopedias.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Monotonic alignments for summarization,"Tonglee Chung, Yongbin Liu, BinXu","Summarization is the task that creates a summary with the major points of the original document. Deep learning plays an important role in both abstractive and extractive summary generations. While a number of models show that combining the two gives good results, this paper focuses on a pure abstractive method to generate good summaries. Our model is a stacked RNN network with a monotonic alignment mechanism. Monotonic alignment has an advantage because it produces the context that is in the same sequence as the original document, at the same time eliminating repeating sequences. To obtain monotonic alignment, this paper proposes two energies that are calculated using only the previous alignment state. We use sub-word method to reduce the rate of producing OOVs(Out of Vocabulary). The dropout is used for generalization and the residual connection to overcome gradient vanishing. We experiment on CNN/daily new and Reddits dataset. Our method out-performs the previous models with monotonic alignment by 4 ROUGE-1 points and achieves the results comparable to state of the art.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Course Concept Expansion in MOOCs with External Knowledge and Interactive Game,"Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li, Jie Tang, Zhiyuan Liu","As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,"Yuncheng Hua, Yuan-Fang Li, Gholamreza Haffari, Guilin Qi, Tongtong Wu","Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and metatraining on tasks constructed from only 1% of the training set. We have released our code at this https URL.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,A comprehensive survey of entity alignment for knowledge graphs,"Kaisheng Zeng, Cheng jiang Li, Lei Hou, Juan Li, Ling Feng","Knowledge Graphs (KGs), as a structured human knowledge, manage data in an ease-of-store, recognizable, and understandable way for machines and provide a rich knowledge base for different artificial intelligence applications. However, current multi-source KGs have heterogeneity and complementarity, and it is necessary to fuse heterogeneous knowledge from different data sources or different languages into a unified and consistent KG. Entity alignment aims to find equivalence relations between entities in different knowledge graphs but semantically represent the same real-world object, which is the most fundamental and essential technology in knowledge fusion. This paper investigated almost all the latest knowledge graph representations learning and entity alignment methods and summarized their core technologies and features from different aspects. Our full investigation gives a comprehensive outlook on several promising research directions for future work. We also provide an efficient and efficiency entity alignment toolkit to help researchers quickly start their own entity alignment models.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Enhanced prototypical network for few-shot relation extraction,"Wen Wen, Yongbin Liu, Chunping Ouyang, Qiang Lin, Tonglee Chung","Most existing methods for relation extraction tasks depend heavily on large-scale annotated data; they cannot learn from existing knowledge and have low generalization ability. It is urgent for us to solve the above problems by further developing few-shot learning methods. Because of the limitations of the most commonly used CNN model which is not good at sequence labeling and capturing long-range dependencies, we proposed a novel model that integrates the transformer model into a prototypical network for more powerful relation-level feature extraction. The transformer connects tokens directly to adapt to long sequence learning without catastrophic forgetting and is able to gain more enhanced semantic information by learning from several representation subspaces in parallel for each word. We evaluate our method on three tasks, including in-domain, cross-domain and cross-sentence tasks. Our method achieves a trade-off between performance and computation and has an approximately 8% improvement in different settings over the state-of-the-art prototypical network. In addition, our experiments also show that our approach is competitive when considering cross-domain transfer and cross-sentence relation extraction in few-shot learning methods.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Dual Gated Graph Attention Networks with Dynamic Iterative Training for Cross-Lingual Entity Alignment,"Zhiwen Xie, Runjie Zhu, Kunsong Zhao, Jin Liu, Guangyou Zhou, Jimmy Xiangji Huang","Cross-lingual entity alignment has attracted considerable attention in recent years. Past studies using conventional approaches to match entities share the common problem of missing important structural information beyond entities in the modeling process. This allows graph neural network models to step in. Most existing graph neural network approaches model individual knowledge graphs (KGs) separately with a small amount of pre-aligned entities served as anchors to connect different KG embedding spaces. However, this characteristic can cause several major problems, including performance restraint due to the insufficiency of available seed alignments and ignorance of pre-aligned links that are useful in contextual information in-between nodes. In this article, we propose DuGa-DIT, a dual gated graph attention network with dynamic iterative training, to address these problems in a unified model. The DuGa-DIT model captures neighborhood and cross-KG alignment features by using intra-KG attention and cross-KG attention layers. With the dynamic iterative process, we can dynamically update the cross-KG attention score matrices, which enables our model to capture more cross-KG information. We conduct extensive experiments on two benchmark datasets and a case study in cross-lingual personalized search. Our experimental results demonstrate that DuGa-DIT outperforms state-of-the-art methods.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,Knowledge Graph Entity Similarity Calculation under Active Learning,"Lianhuan Li , Zheng Zhang , Shaoda Zhang","To address the objectives of the adaptive learning platform, the requirements of the system in terms of business, functionality, and performance are mainly analysed, and the design of functions and database is completed; then, an updatable learner model is constructed based on the cognitive diagnosis model and resource preference attributes; then, the construction of the knowledge map is completed based on embedding to achieve knowledge point alignment, and based on this, the target knowledge points of learners are located with the help of deep learning; at the same time, the target knowledge points are taken as the starting point to generate the best learning path by traversing the knowledge map, and the corresponding learning resources and test questions are recommended for them with the help of the architecture; finally, the adaptive learning platform is developed in the environment using the architecture. Also, the target knowledge point is used as the starting point to traverse the knowledge map to generate the best learning path, and the corresponding learning resources and test questions are recommended for the learner in combination with the learner model; finally, this study adopts an architecture for the development of an adaptive learning platform in the environment to realize online tests, score analysis, resource recommendation, and other functions. A knowledge graph fusion system supporting interactive facilitation between entity alignment and attribute alignment is implemented. Under a unified conceptual layer, this system can combine entity alignment and attribute alignment to promote each other and truly achieve the final fusion of the two graphs. Our experimental results on real datasets show that the entity alignment algorithm proposed in this paper has a great improvement in accuracy compared with the previous mainstream alignment algorithms. Also, the attribute alignment algorithm proposed in this paper, which calculates the similarity based on associated entities, outperforms the traditional methods in terms of accuracy and recall.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,"ECCParaCorp: a cross-lingual parallel corpus towards cancer education, dissemination and application","Hetong Ma, Feihong Yang, Jiansong Ren, Ni Li, Min Dai, Xuwen Wang, An Fang, Jiao Li, Qing Qian, Jie He ","The increasing global cancer incidence corresponds to serious health impact in countries worldwide. Knowledge-powered health system in different languages would enhance clinicians’ healthcare practice, patients’ health management and public health literacy. High-quality corpus containing cancer information is the necessary foundation of cancer education. Massive non-structural information resources exist in clinical narratives, electronic health records (EHR) etc. They can only be used for training AI models after being transformed into structured corpus. However, the scarcity of multilingual cancer corpus limits the intelligent processing, such as machine translation in medical scenarios. Thus, we created the cancer specific cross-lingual corpus and open it to the public for academic use.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,ExpanRL: Hierarchical Reinforcement Learning for Course Concept Expansion in MOOCs,"Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li, Jie Tang, Minlie Huang, Zhiyuan Liu","Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses’ diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in MOOCs. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students’ feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.",XLORE2: Large-Scale Cross-Lingual Knowledge Graph Construction and Application,19,0,,,
Entity Matching,DWIE: an entity-centric dataset for multi-task document-level information extraction,"Klim Zaporojets, Johannes Deleu, Chris Develder, Thomas Demeester","This paper presents DWIE, the ‘Deutsche Welle corpus for Information Extraction’, a newly created multi-task dataset that combines four main Information Extraction (IE) annotation subtasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document. This contrasts with currently dominant mention-driven approaches that start from the detection and classification of named entity mentions in individual sentences. Further, DWIE presented two main challenges when building and evaluating IE models for it. First, the use of traditional mention-level evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can result in measurements dominated by predictions on more frequently mentioned entities. We tackle this issue by proposing a new entity-driven metric that takes into account the number of mentions that compose each of the predicted and ground truth entities. Second, the document-level multi-task annotations require the models to transfer information between entity mentions located in different parts of the document, as well as between different tasks, in a joint learning setting. To realize this, we propose to use graph-based neural message passing techniques between document-level mention spans. Our experiments show an improvement of up to 5.5  percentage points when incorporating neural graph propagation into our joint model. This demonstrates DWIE’s potential to stimulate further research in graph neural networks for representation learning in multi-task IE. We make DWIE publicly available at https://github.com/klimzaporojets/DWIE.",DWIE Dataset,20,1,DWIE Dataset,"Klim Zaporojets, Johannes Deleu, Chris Develder, Thomas Demeester","The 'Deutsche Welle corpus for Information Extraction' (DWIE) is a multi-task dataset that combines four main Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document."
Entity Matching,Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution,"Severine Verlinden, Klim Zaporojets, Johannes Deleu, Thomas Demeester, Chris Develder","We consider a joint information extraction (IE) model, solving named entity recognition, coreference resolution and relation extraction jointly over the whole document. In particular, we study how to inject information from a knowledge base (KB) in such IE model, based on unsupervised entity linking. The used KB entity representations are learned from either (i) hyperlinked text documents (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear complementary in raising IE performance. Representations of corresponding entity linking (EL) candidates are added to text span representations of the input document, and we experiment with (i) taking a weighted average of the EL candidate representations based on their prior (in Wikipedia), and (ii) using an attention scheme over the EL candidate list. Results demonstrate an increase of up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a strong performance of the prior-based model, our quantitative and qualitative analysis reveals the advantage of using the attention-based approach.",DWIE Dataset,20,0,,,
Entity Matching,HacRED: A Large-Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications,"Qiao Cheng, Juntao Liu, Xiaoye Qu, Jin Zhao, Jiaqing Liang, Zhefeng Wang, Baoxing Huai, N. Yuan, Yanghua Xiao","Relation extraction (RE) is an essential topic
in natural language processing and has attracted extensive attention. Current RE approaches achieve fantastic results on common
datasets, while they still struggle on practical
applications. In this paper, we analyze the
above performance gap, the underlying reason of which is that practical applications intrinsically have more hard cases. To make
RE models more robust on such practical hard
cases, we propose a case-oriented construction framework to build a Hard Case Relation
Extraction Dataset (HacRED). The proposed
HacRED consists of 65,225 relational facts annotated from 9,231 documents with sufficient
and diverse hard cases. Notably, HacRED is
one of the largest Chinese document-level RE
datasets and achieves a high 96% F1 score on
data quality. Furthermore, we apply the stateof-the-art RE models on this dataset and conduct a thorough evaluation. The results show
that the performance of these models is far
lower than humans, and RE applying on practical hard cases still requires further efforts. HacRED is publicly available at https://github.
com/qiaojiim/HacRED",DWIE Dataset,20,0,,,
Entity Matching,Syntax–Aware graph convolutional network for the recognition of chinese implicit inter-sentence relations,"Kaili Sun, Yuan Li, Huyin Zhang, Chi Guo, Linfei Yuan & Quan Hu ","In the literature, most previous studies on English implicit inter-sentence relation recognition only focused on semantic interactions, which could not exploit the syntactic interactive information in Chinese due to its complicated syntactic structure characteristics. In this paper, we propose a novel and effective model DSGCN-RoBERTa to learn the interaction features implied in sentences from both syntactic and semantic perspectives. To generate a rich contextual sentence embedding, we exploit RoBERTa, a large-scale pre-trained language model based on the transformer unit. DSGCN-RoBERTa consists of two key modules, the syntactic interaction and the semantic interaction modules. Specifically, the syntactic interaction module helps capture the depth-level structure information, including non-consecutive words and their relations, while the semantic interaction module enables the model to understand the context from the whole sentence to the local words. Furthermore, on top of such multi-perspective feature representations, we design a strength-dependent matching strategy that is able to adaptively capture the strong relevant interactive information in a fine-grained level. Extensive experiments demonstrate that the proposed method achieved state-of-the-art results on benchmarks Chinese compound sentence corpus CCCS and Chinese discourse corpus CDTB datasets. We also achieve comparable performance on the English corpus PDTB that demonstrates the superiority of our method.",DWIE Dataset,20,0,,,
Entity Matching,What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,"Elisa Bassignana, Barbara Plank","Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taillé et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.",DWIE Dataset,20,0,,,
Entity Matching,Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction,"Liyan Xu, Jinho D. Choi","We target on the document-level relation extraction in an end-to-end setting, where the model needs to jointly perform mention extraction, coreference resolution (COREF) and relation extraction (RE) at once, and gets evaluated in an entity-centric way. Especially, we address the two-way interaction between COREF and RE that has not been the focus by previous work, and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task characteristics, bridging decisions of two tasks for direct task interference. Our experiments are conducted on DocRED and DWIE; in addition to GC, we implement and compare different multi-task settings commonly adopted in previous work, including pipeline, shared encoders, graph propagation, to examine the effectiveness of different interactions. The result shows that GC achieves the best performance by up to 2.3/5.1 F1 improvement over the baseline.
",DWIE Dataset,20,0,,,
Entity Matching,Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues,"Liyan Xu, Jinho D. Choi","We present an effective system adapted from the end-to-end neural coreference resolution model, targeting on the task of anaphora resolution in dialogues. Three aspects are specifically addressed in our approach, including the support of singletons, encoding speakers and turns throughout dialogue interactions, and knowledge transfer utilizing existing resources. Despite the simplicity of our adaptation strategies, they are shown to bring significant impact to the final performance, with up to 27 F1 improvement over the baseline. Our final system ranks the 1st place on the leaderboard of the anaphora resolution track in the CRAC 2021 shared task, and achieves the best evaluation results on all four datasets.",DWIE Dataset,20,0,,,
Entity Matching,Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction,"Jiaxin Yu, Deqing Yang, Shuyu Tian","Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct semantics between the different mentions of an entity are overlooked. To address this problem, we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner, the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance, especially when an entity have multiple mentions in the document.",DWIE Dataset,20,0,,,
Entity Matching,MTGCN: A multi-task approach for node classification and link prediction in graph data,"Zongqian Wu, Mengmeng Zhan, Haiqi Zhang, Qimin Luo, KunTang","Both node classification and link prediction are popular topics of supervised learning on the graph data, but previous works seldom integrate them together to capture their complementary information. In this paper, we propose a Multi-Task and Multi-Graph Convolutional Network (MTGCN) to jointly conduct node classification and link prediction in a unified framework. Specifically, MTGCN consists of multiple multi-task learning so that each multi-task learning learns the complementary information between node classification and link prediction. In particular, each multi-task learning uses different inputs to output representations of the graph data. Moreover, the parameters of one multi-task learning initialize the parameters of the other multi-task learning, so that the useful information in the former multi-task learning can be propagated to the other multi-task learning. As a result, the information is augmented to guarantee the quality of representations by exploring the complex constructure inherent in the graph data. Experimental results on six datasets show that our MTGCN outperforms the comparison methods in terms of both node classification and link prediction.",DWIE Dataset,20,0,,,
Entity Matching,End-to-end event factuality prediction using directional labeled graph recurrent network,"Xiao Liu, Heyan Huang, Yue Zhang","Event factuality prediction is the task of assessing the degree to which an event mentioned in a sentence has happened. However, existing methods usually stack encoders to make factuality predictions given the gold positions of anchor words. In addition, the frequently used encoders, such as bidirectional LSTMS and graph convolution networks, ignore the directional labeled syntactic information while modeling the context. To fill the gap when facing plain text without identifying event anchor words in advance, we investigate the task of end-to-end EFP in this paper. We present the Directional Labeled Graph Recurrent Network, denoted as DLGRN, to solve Event Anchor Detection and Factuality Induction in a multi-task framework. Specifically, we represent sentences as syntactic information graphs. Then, to incorporate directional labeled information, we design edge-tied weights and edge-aware attention mechanism on top of a graph-based recurrently message passing encoder. We further propose to utilize multi-task learning to jointly model Event Anchor Detection and Factuality Induction by optimizing a mixed-objective learning function. We use four widely used factuality prediction benchmarks (i.e., FactBank, Meantime, UW, and UDS-IH2) to evaluate our framework. Our framework achieves state-of-the-art results in the two subtasks, averagely decreasing 17.12% MAE and raising 5.40% Pearson correlation  against the best baseline. In addition, experimental results show that our framework can capture the overall factuality score distributions, and incorporating directional and labeled syntactic information in EFP achieves better performances than the baselines.

",DWIE Dataset,20,0,,,
Entity Matching,Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution,"Klim Zaporojets, Johannes Deleu, Thomas Demeester, Chris Develder","We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit ""connections"" among mentions within the document itself: we propose to join the EL task with that of coreference resolution (coref). This is complementary to related works that exploit either (i) implicit document information (e.g., latent relations among entity mentions, or general language models) or (ii) connections between the candidate links (e.g, as inferred from the external knowledge base). Specifically, we cluster mentions that are linked via coreference, and enforce a single EL for all of the clustered mentions together. The latter constraint has the added benefit of increased coverage by joining EL candidate lists for the thus clustered mentions. We formulate the coref+EL problem as a structured prediction task over directed trees and use a globally normalized model to solve it. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy.",DWIE Dataset,20,0,,,
Entity Matching,Novel Entity Discovery from Web Tables,"Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda ","When working with any sort of knowledge base (KB) one has to make sure it is as complete and also as up-to-date as possible. Both tasks are non-trivial as they require recall-oriented efforts to determine which entities and relationships are missing from the KB. As such they require a significant amount of labor. Tables on the Web, on the other hand, are abundant and have the distinct potential to assist with these tasks. In particular, we can leverage the content in such tables to discover new entities, properties, and relationships. Because web tables typically only contain raw textual content we first need to determine which cells refer to which known entities---a task we dub table-to-KB matching. This first task aims to infer table semantics by linking table cells and heading columns to elements of a KB. Then second task builds upon these linked entities and properties to not only identify novel ones in the same table but also to bootstrap their type and additional relationships. We refer to this process as novel entity discovery and, to the best of our knowledge, it is the first endeavor on mining the unlinked cells in web tables. Our method identifies not only out-of-KB (``novel'') information but also novel aliases for in-KB (``known'') entities. When evaluated using three purpose-built test collections, we find that our proposed approaches obtain a marked improvement in terms of precision over our baselines whilst keeping recall stable.
","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,1,"Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables","Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda ","This repository contains resources developed for the paper: ""S. Zhang, E. Meij, K. Balog, and R. Reinanda. Novel Entity Discovery from Web Tables. In: Proceeding of the The Web Conference 2020 (WWW ’20), April 2020"".

It includes the three test collections for novel entity discovery for Web tables, entity type and mention resolution, as well as the mention-entity and heading-property correspondences for 3M tables. The cited datasets were used in this work.

Files to recreate the entity linking experiments:

training_el.csv
training_el_type.csv
training_el_type_wiki.csv
training_el_wiki.csv
training_schema.csv
Files to recreate the table matching experiments:

me_corres.csv - textual cells algorithmically linked to Wikipedia entities
hp_corres.csv - same but only table headings
Files to recreate the entity resolution experiments:

ec_golden.csv - 20K unlinked mentions textual cells, manually linked to Wikipedia
er_sf_golden.csv - 1K cell values, manually clustered
er_type_golden.csv - 1K cell values, manually linked to DBpedia types"
Entity Matching,Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases,"Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek","Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Tough Tables: Carefully Evaluating Entity Linking for Tabular Data,"Vincenzo Cutrona, Federico Bianchi, Ernesto Jiménez-Ruiz, Matteo Palmonari ","Table annotation is a key task to improve querying the Web and support the Knowledge Graph population from legacy sources (tables). Last year, the SemTab challenge was introduced to unify different efforts to evaluate table annotation algorithms by providing a common interface and several general-purpose datasets as a ground truth. The SemTab dataset is useful to have a general understanding of how these algorithms work, and the organizers of the challenge included some artificial noise to the data to make the annotation trickier. However, it is hard to analyze specific aspects in an automatic way. For example, the ambiguity of names at the entity-level can largely affect the quality of the annotation. In this paper, we propose a novel dataset to complement the datasets proposed by SemTab. The dataset consists of a set of high-quality manually-curated tables with non-obviously linkable cells, i.e., where values are ambiguous names, typos, and misspelled entity names not appearing in the current version of the SemTab dataset. These challenges are particularly relevant for the ingestion of structured legacy sources into existing knowledge graphs. Evaluations run on this dataset show that ambiguity is a key problem for entity linking algorithms and encourage a promising direction for future work in the field.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Towards Cost Minimization for Wireless Caching Networks With Recommendation and Uncharted Users’ Feature Information,"Yaru Fu, Zhong Yang; Tony Q. S. Quek, Howard H. Yang","Caching popular contents at the network edge has been considered as a promising enabler to relieve the pressure on networks due to the fact that a substantial portion of global data traffic is repeatedly requested by many subscribers and thus redundantly generated. Recommendation, on the other hand, has attracted spiraling attention for its capability of reshaping users’ contents demand patterns. In this paper, we examine the practicability of recommendation in boosting the gains of edge caching with uncharted users’ feature information. To this end, we first characterize the average system cost for a generic network model, disclosing its dependence on the recommendation and caching strategies. Then, we formulate the joint caching and recommendation decision oriented cost minimization problem, taking the constraints on each content provider’s cache capacity budget, each individual user’s recommendation size and recommendation quality into account. However, the implicit information regarding users’ preference makes the problem inextricable. To address this issue, a versatile long short term memory (LSTM) network assisted prediction paradigm is proposed to attain the preference schema of users with the assistance of their historical behavior data. Based on that, we rigorously prove the NP-hardness of obtaining the optimal recommendation and caching policies that jointly minimize the system cost. Therewith, an iterative suboptimal algorithm is developed, which has provable polynomial time complexity and convergence guarantee. Extensive simulation results validate the effectiveness of our proposed LSTM enabled feature information prediction approach and the convergence performance of the devised joint decision making methodology. In addition, it is shown that the proposed scheme outperforms numerous benchmarks significantly.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,TABBIE: Pretrained Representations of Tabular Data,"Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer","Existing work on tabular representation learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,OCT-GAN: Neural ODE-based Conditional Tabular GANs,"Jayoung Kim, Jinsung Jeon, Jaehoon Lee, Jihyeon Hyeong, Noseong Park","Synthesizing tabular data is attracting much attention these days for various purposes. With sophisticate synthetic data, for instance, one can augment its training data. For the past couple of years, tabular data synthesis techniques have been greatly improved. Recent work made progress to address many problems in synthesizing tabular data, such as the imbalanced distribution and multimodality problems. However, the data utility of state-of-the-art methods is not satisfactory yet. In this work, we significantly improve the utility by designing our generator and discriminator based on neural ordinary differential equations (NODEs). After showing that NODEs have theoretically preferred characteristics for generating tabular data, we introduce our designs. The NODE-based discriminator performs a hidden vector evolution trajectory-based classification rather than classifying with a hidden vector at the last layer only. Our generator also adopts an ODE layer at the very beginning of its architecture to transform its initial input vector (i.e., the concatenation of a noisy vector and a condition vector in our case) onto another latent vector space suitable for the generation process. We conduct experiments with 13 datasets, including but not limited to insurance fraud detection, online news article prediction, and so on, and our presented method outperforms other state-of-the-art tabular data synthesis methods in many cases of our classification, regression, and clustering experiments.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Generating Categories for Sets of Entities," Shuo Zhang, Krisztian Balog, Jamie Callan","Category systems are central components of knowledge bases, as
they provide a hierarchical grouping of semantically related concepts and entities. They are a unique and valuable resource that is
utilized in a broad range of information access tasks. To aid knowledge editors in the manual process of expanding a category system,
this paper presents a method of generating categories for sets of
entities. First, we employ neural abstractive summarization models
to generate candidate categories. Next, the location within the hierarchy is identified for each candidate. Finally, structure-, content-,
and hierarchy-based features are used to rank candidates to identify by the most promising ones (measured in terms of specificity,
hierarchy, and importance). We develop a test collection based
on Wikipedia categories and demonstrate the effectiveness of the
proposed approach.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,TabEAno: Table to Knowledge Graph Entity Annotation,"Phuc Nguyen, Natthawut Kertkeidkachorn, Ryutaro Ichise, Hideaki Takeda","In the Open Data era, a large number of table resources have been made available on the Web and data portals. However, it is difficult to directly utilize such data due to the ambiguity of entities, name variations, heterogeneous schema, missing, or incomplete metadata. To address these issues, we propose a novel approach, namely TabEAno, to semantically annotate table rows toward knowledge graph entities. Specifically, we introduce a ""two-cells"" lookup strategy bases on the assumption that there is an existing logical relation occurring in the knowledge graph between the two closed cells in the same row of the table. Despite the simplicity of the approach, TabEAno outperforms the state of the art approaches in the two standard datasets e.g, T2D, Limaye with, and in the large-scale Wikipedia tables dataset.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Information Extraction From Co-Occurring Similar Entities,"Nicolas Heist, Heiko Paulheim","Knowledge about entities and their interrelations is a crucial factor of success for tasks like question answering or text summarization. Publicly available knowledge graphs like Wikidata or DBpedia are, however, far from being complete. In this paper, we explore how information extracted from similar entities that co-occur in structures like tables or lists can help to increase the coverage of such knowledge graphs. In contrast to existing approaches, we do not focus on relationships within a listing (e.g., between two entities in a table row) but on the relationship between a listing's subject entities and the context of the listing. To that end, we propose a descriptive rule mining approach that uses distant supervision to derive rules for these relationships based on a listing's context. Extracted from a suitable data corpus, the rules can be used to extend a knowledge graph with novel entities and assertions. In our experiments we demonstrate that the approach is able to extract up to 3M novel entities and 30M additional assertions from listings in Wikipedia. We find that the extracted information is of high quality and thus suitable to extend Wikipedia-based knowledge graphs like DBpedia, YAGO, and CaLiGraph. For the case of DBpedia, this would result in an increase of covered entities by roughly 50%.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,WTR: A Test Collection for Web Table Retrieval," Zhiyu Chen, Shuo Zhang, Brian D. Davison","We describe the development, characteristics and availability of a test collection for the task of Web table retrieval, which uses a large-scale Web Table Corpora extracted from the Common Crawl. Since a Web table usually has rich context information such as the page title and surrounding paragraphs, we not only provide relevance judgments of query-table pairs, but also the relevance judgments of query-table context pairs with respect to a query, which are ignored by previous test collections. To facilitate future research with this benchmark, we provide details about how the dataset is pre-processed and also baseline results from both traditional and recently proposed table retrieval methods. Our experimental results show that proper usage of context labels can benefit previous table retrieval methods.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Semantic Table Retrieval Using Keyword and Table Queries," Shuo Zhang, Krisztian Balog ","Tables on the Web contain a vast amount of knowledge in a structured form. To tap into this valuable resource, we address the problem of table retrieval: answering an information need with a ranked list of tables. We investigate this problem in two different variants, based on how the information need is expressed: as a keyword query or as an existing table (""query-by-table""). The main novel contribution of this work is a semantic table retrieval framework for matching information needs (keyword or table queries) against tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using two purpose-built test collections based on Wikipedia tables, we demonstrate significant and substantial improvements over state-of-the-art baselines.","Resources for reproducing experiments in ""Novel Entity Discovery from Web Tables""",21,0,,,
Entity Matching,Earnings-21: A Practical Benchmark for ASR in the Wild,"Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr Zelasko, Miguel Jette","Commonly used speech corpora inadequately challenge academic and commercial ASR systems. In particular, speech corpora lack metadata needed for detailed analysis and WER measurement. In response, we present Earnings-21, a 39-hour corpus of earnings calls containing entity-dense speech from nine different financial sectors. This corpus is intended to benchmark ASR systems in the wild with special attention towards named entity recognition. We benchmark four commercial ASR models, two internal models built with open-source tools, and an open-source LibriSpeech model and discuss their differences in performance on Earnings-21. Using our recently released fstalign tool, we provide a candid analysis of each model's recognition capabilities under different partitions. Our analysis finds that ASR accuracy for certain NER categories is poor, presenting a significant impediment to transcript comprehension and usage. Earnings-21 bridges academic and commercial ASR system evaluation and enables further research on entity modeling and WER on real world audio.",Earnings-21 Dataset,22,1,Earnings-21 Dataset,"Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr Zelasko, Miguel Jette","Earnings-21, a 39-hour corpus of earnings calls containing entity-dense speech from nine different financial sectors. This corpus is intended to benchmark ASR (Automatic Speech Recognition) systems in the wild with special attention towards named entity recognition.
"
Entity Matching,ESPnet-SLU: Advancing Spoken Language Understanding Through ESPnet,"Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, Ngoc Thang Vu, Alan W Black, Shinji Watanabe","As Automatic Speech Processing (ASR) systems are getting better, there is an increasing interest of using the ASR output to do downstream Natural Language Processing (NLP) tasks. However, there are few open source toolkits that can be used to generate reproducible results on different Spoken Language Understanding (SLU) benchmarks. Hence, there is a need to build an open source standard that can be used to have a faster start into SLU research. We present ESPnet-SLU, which is designed for quick development of spoken language understanding in a single framework. ESPnet-SLU is a project inside end-to-end speech processing toolkit, ESPnet, which is a widely used open-source standard for various speech processing tasks like ASR, Text to Speech (TTS) and Speech Translation (ST). We enhance the toolkit to provide implementations for various SLU benchmarks that enable researchers to seamlessly mix-and-match different ASR and NLU models. We also provide pretrained models with intensively tuned hyper-parameters that can match or even outperform the current state-of-the-art performances. The toolkit is publicly available at this https URL.",Earnings-21 Dataset,22,0,,,
Entity Matching,VarArray: Array-Geometry-Agnostic Continuous Speech Separation,Takuya Yoshioka; Xiaofei Wang; Dongmei Wang; Min Tang; Zirun Zhu; Zhuo Chen; Naoyuki Kanda,"Continuous speech separation using a microphone array was shown to be promising in dealing with the speech overlap problem in natural conversation transcription. This paper proposes VarArray, an array-geometry-agnostic speech separation neural network model. The proposed model is applicable to any number of microphones without retraining while leveraging the nonlinear correlation between the input channels. The proposed method adapts different elements that were proposed before separately, including transform-average-concatenate, conformer speech separation, and inter-channel phase differences, and combines them in an efficient and cohesive way. Large-scale evaluation was performed with two real meeting transcription tasks by using a fully developed transcription system requiring no prior knowledge such as reference segmentations, which allowed us to measure the impact that the continuous speech separation system could have in realistic settings. The proposed model outperformed a previous approach to array-geometry-agnostic modeling for all of the geometry configurations considered, achieving asclite-based speaker-agnostic word error rates of 17.5% and 20.4% for the AMI development and evaluation sets, respectively, in the end-to-end setting using no ground-truth segmentations.",Earnings-21 Dataset,22,0,,,
Entity Matching,Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability,Somnath Roy,"Recent advances in supervised, semi-supervised and self-supervised deep learning algorithms have shown significant improvement in the performance of automatic speech recognition(ASR) systems. The state-of-the-art systems have achieved a word error rate (WER) less than 5%. However, in the past, researchers have argued the non-suitability of the WER metric for the evaluation of ASR systems for downstream tasks such as spoken language understanding (SLU) and information retrieval. The reason is that the WER works at the surface level and does not include any syntactic and semantic knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate the ASR transcripts for downstream applications in general. The SWER can be easily customized for any down-stream task.",Earnings-21 Dataset,22,0,,,
Entity Matching,Benchmarking ASR Systems Based on Post-Editing Effort and Error Analysis,"Martha Maria Papadopoulou, Anna Zaretskaya, Ruslan Mitkov","This paper offers a comparative evaluation of four commercial ASR systems which are evaluated according to the post-editing effort required to reach “publishable” quality and according to the number of errors they produce. For the error annotation task, an original error typology for transcription errors is proposed. This study also seeks to examine whether there is a difference in the performance of these systems between native and non-native English speakers. The experimental results suggest that among the four systems, Trint obtains the best scores. It is also observed that most systems perform noticeably better with native speakers and that all systems are most prone to fluency errors.",Earnings-21 Dataset,22,0,,,
Entity Matching,Large Scale Speech Recognition with Deep Learning,"Umashankar, Anand","Automatic Speech Recognition (ASR) is the task of converting speech signal into text. To enable usage of large datasets to train the end to end speech recognition neural networks, we build a pipeline for increasing efficiency in data storing, data loading and training. We train an attention based sequence-to-sequence model and use word error rates for evaluating the experiments. The time to reach benchmark accuracy is another important metric used to compare the training efficiency of different systems. This work uses a dataset with around 26,000 hours that is new for speech to text experiments. The dataset consists of conference calls with a diverse set of speakers. Around half of the data has a presentation style audio, while the other half contains conversational language.
 First, the work focuses on the steps taken to make this dataset efficient for speech recognition. Then, two types of distributed training algorithms, synchronous and asynchronous training, are applied which enables the usage of multiple GPUs for stochastic gradient descent. The comparison of the different methods show that, for the experiment setup employed in this work, synchronous training provides the best word error rate of 10.87%. This run converged in 32 hours using 4 GPUs in parallel, which is a speed-up of 2x compared to the single GPU training job to reach the benchmark word error rate. The effective batch size plays an important role in these results. The experiment results also show that increasing the scale of the data, reduces the overall training time, and hence using larger datasets is beneficial even when obtaining quicker training results is an important criterion.",Earnings-21 Dataset,22,0,,,
Entity Matching,Earnings-22: A Practical Benchmark for Accents in the Wild,"Miguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, Shipra Chandra","Modern automatic speech recognition (ASR) systems have achieved superhuman Word Error Rate (WER) on many common corpora despite lacking adequate performance on speech in the wild. Beyond that, there is a lack of real-world, accented corpora to properly benchmark academic and commercial models. To ensure this type of speech is represented in ASR benchmarking, we present Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls gathered from global companies. We run a comparison across 4 commercial models showing the variation in performance when taking country of origin into consideration. Looking at hypothesis transcriptions, we explore errors common to all ASR systems tested. By examining Individual Word Error Rate (IWER), we find that key speech features impact model performance more for certain accents than others. Earnings-22 provides a free-to-use benchmark of real-world, accented audio to bridge academic and industrial research.",Earnings-21 Dataset,22,0,,,
Entity Matching,ASR4REAL: An extended benchmark for speech models,"Morgane Riviere, Jade Copet, Gabriel Synnaeve","Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models",Earnings-21 Dataset,22,0,,,
Entity Matching,COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,"Nhung T.H. Nguyen, Roselyn S. Gabud, Sophia Ananiadou","Species occurrence records are very important in the biodiversity domain. While several available corpora contain only annotations of species names or habitats and geographical locations, there is no consolidated corpus that covers all types of entities necessary for extracting species occurrence from biodiversity literature. In order to alleviate this issue, we have constructed the COPIOUS corpus—a gold standard corpus that covers a wide range of biodiversity entities.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,1,COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,"Nhung T.H. Nguyen, Roselyn S. Gabud, Sophia Ananiadou","Background Species occurrence records are very important in the biodiversity domain. While several available corpora contain only annotations of species names or habitats and geographical locations, there is no consolidated corpus that covers all types of entities necessary for extracting species occurrence from biodiversity literature. In order to alleviate this issue, we have constructed the COPIOUS corpus—a gold standard corpus that covers a wide range of biodiversity entities. Results Two annotators manually annotated the corpus with five categories of entities, i.e. taxon names, geographical locations, habitats, temporal expressions and person names. The overall inter-annotator agreement on 200 doubly-annotated documents is approximately 81.86% F-score. Amongst the five categories, the agreement on habitat entities was the lowest, indicating that this type of entity is complex. The COPIOUS corpus consists of 668 documents downloaded from the Biodiversity Heritage Library with over 26K sentences and more than 28K entities. Named entity recognisers trained on the corpus could achieve an F-score of 74.58%. Moreover, in recognising taxon names, our model performed better than two available tools in the biodiversity domain, namely the SPECIES tagger and the Global Name Recognition and Discovery. More than 1,600 binary relations of Taxon-Habitat, Taxon-Person, Taxon-Geographical locations and Taxon-Temporal expressions were identified by applying a pattern-based relation extraction system to the gold standard. Based on the extracted relations, we can produce a knowledge repository of species occurrences. Conclusion The paper describes in detail the construction of a gold standard named entity corpus for the biodiversity domain. An investigation of the performance of named entity recognition (NER) tools trained on the gold standard revealed that the corpus is sufficiently reliable and sizeable for both training and evaluation purposes. The corpus can be further used for relation extraction to locate species occurrences in literature—a useful task for monitoring species distribution and preserving the biodiversity."
Entity Matching,Open Agile text mining for bioinformatics: the PubAnnotation ecosystem,"Jin-Dong Kim, Yue Wang, Toyofumi Fujiwara, Shujiro Okuda, Tiffany J Callahan, K Bretonnel Cohen","Most currently available text mining tools share two characteristics that make them less than optimal for use by biomedical researchers: they require extensive specialist skills in natural language processing and they were built on the assumption that they should optimize global performance metrics on representative datasets. This is a problem because most end-users are not natural language processing specialists and because biomedical researchers often care less about global metrics like F-measure or representative datasets than they do about more granular metrics such as precision and recall on their own specialized datasets. Thus, there are fundamental mismatches between the assumptions of much text mining work and the preferences of potential end-users.

",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,BIOfid Dataset: Publishing a German Gold Standard for Named Entity Recognition in Historical Biodiversity Literature,"Sajawel Ahmed, Manuel Stoeckel, Christine Driller, Adrian Pachzelt, Alexander Mehler","The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants, birds, moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23% F-score. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Tag Me If You Can! Semantic Annotation of Biodiversity Metadata with the QEMP Corpus and the BiodivTagger,"Felicitas Löffler, Nora Abdelmageed, Samira Babalou, Pawandeep Kaur, Birgitta König-Ries","Dataset Retrieval is gaining importance due to a large amount of research data and the great demand for reusing scientific data. Dataset Retrieval is mostly based on metadata, structured information about the primary data. Enriching these metadata with semantic annotations based on Linked Open Data (LOD) enables datasets, publications and authors to be connected and expands the search on semantically related terms. In this work, we introduce the BiodivTagger, an ontology-based Information Extraction pipeline, developed for metadata from biodiversity research. The system recognizes biological, physical and chemical processes, environmental terms, data parameters and phenotypes as well as materials and chemical compounds and links them to concepts in dedicated ontologies. To evaluate our pipeline, we created a gold standard of 50 metadata files (QEMP corpus) selected from five different data repositories in biodiversity research. To the best of our knowledge, this is the first annotated metadata corpus for biodiversity research data. The results reveal a mixed picture. While materials and data parameters are properly matched to ontological concepts in most cases, some ontological issues occurred for processes and environmental terms.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,TaxoNERD: Deep neural models for the recognition of taxonomic entities in the ecological and evolutionary literature,"Nicolas Le Guillarme,Wilfried Thuiller","Easy access to multi-taxa information (e.g. distribution, traits, diet) in the scientific literature is essential to understand, map and predict all-inclusive biodiversity. Tools are needed to automatically extract useful information from the ever-growing corpus of ecological texts and feed this information to open data repositories. A prerequisite is the ability to recognise mentions of taxa in text, a special case of named entity recognition (NER). In recent years, deep learning-based NER systems have become ubiquitous, yielding state-of-the-art results in the general and biomedical domains. However, no such tool is available to ecologists wishing to extract information from the biodiversity literature.
We propose a new tool called TaxoNERD that provides deep neural network (DNN) models to recognise taxon mentions in ecological documents. To achieve high performance, these models usually need to be trained on a large corpus of manually annotated text. Creating such a corpus is a laborious and costly process, with the result that manually annotated corpora in the ecological domain tend to be too small to learn an accurate DNN model from scratch. To address this issue, we leverage existing models pretrained on large biomedical corpora using transfer learning. The performance of our models is evaluated on four corpora and compared to the most popular taxonomic entity recognition tools.
Our experiments suggest that existing taxonomic NER tools are not suited to the extraction of ecological information from text as they performed poorly on ecologically oriented corpora, either because they do not take account of the variability of taxon naming practices or because they do not generalise well to the ecological domain. Conversely, a domain-specific DNN-based tool like TaxoNERD outperformed the other approaches on an ecological information extraction task.
Efforts are needed to raise ecological information extraction to the same level of performance as its biomedical counterpart. One promising direction is to leverage the huge corpus of unlabelled ecological texts to learn a language representation model that could benefit downstream tasks. These efforts could be highly beneficial to ecologists on the long term.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Recognition of Latin scientific names using artificial neural networks,Damon P. Little,"Premise: The automated recognition of Latin scientific names within vernacular text has many applications, including text mining, search indexing, and automated specimen-label processing. Most published solutions are computationally inefficient, incapable of running within a web browser, and focus on texts in English, thus omitting a substantial portion of biodiversity literature.

Methods and results: An open-source browser-executable solution, Quaesitor, is presented here. It uses pattern matching (regular expressions) in combination with an ensembled classifier composed of an inclusion dictionary search (Bloom filter), a trio of complementary neural networks that differ in their approach to encoding text, and word length to automatically identify Latin scientific names in the 16 most common languages for biodiversity articles.

Conclusions: In combination, the classifiers can recognize Latin scientific names in isolation or embedded within the languages used for >96% of biodiversity literature titles. For three different data sets, they resulted in a 0.80-0.97 recall and a 0.69-0.84 precision at a rate of 8.6 ms/word.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Towards Creating a New Triple Store for Literature-Based Discovery,"Anna Koroleva, Maria Anisimova & Manuel Gil ","Literature-based discovery (LBD) is a field of research aiming at discovering new knowledge by mining scientific literature. Knowledge bases are commonly used by LBD systems. SemMedDB, created with the use of SemRep information extraction system, is the most frequently used database in LBD. However, new applications of LBD are emerging that go beyond the scope of SemMedDB. In this work, we propose some new discovery patterns that lie in the domain of Natural Products and that are not covered by the existing databases and tools. Our goal thus is to create a new, extended knowledge base, addressing limitations of SemMedDB. Our proposed contribution is three-fold: 1) we add types of entities and relations that are of interest for LBD but are not covered by SemMedDB; 2) we plan to leverage full texts of scientific publications, instead of titles and abstracts only; 3) we envisage using the RDF model for our database, in accordance with Semantic Web standards. To create a new database, we plan to build a distantly supervised entity and relation extraction system, employing a neural networks/deep learning architecture. We describe the methods and tools we plan to employ.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,"Multiple annotation for biodiversity: developing an annotation framework among biology, linguistics and text technology","Andy Lücking, Christine Driller, Manuel Stoeckel, Giuseppe Abrami, Adrian Pachzelt & Alexander Mehler ","Biodiversity information is contained in countless digitized and unprocessed scholarly texts. Although automated extraction of these data has been gaining momentum for years, there are still innumerable text sources that are poorly accessible and require a more advanced range of methods to extract relevant information. To improve the access to semantic biodiversity information, we have launched the BIOfid project (www.biofid.de) and have developed a portal to access the semantics of German language biodiversity texts, mainly from the 19th and 20th century. However, to make such a portal work, a couple of methods had to be developed or adapted first. In particular, text-technological information extraction methods were needed, which extract the required information from the texts. Such methods draw on machine learning techniques, which in turn are trained by learning data. To this end, among others, we gathered the BIOfid text corpus, which is a cooperatively built resource, developed by biologists, text technologists, and linguists. A special feature of BIOfid is its multiple annotation approach, which takes into account both general and biology-specific classifications, and by this means goes beyond previous, typically taxon- or ontology-driven proper name detection. We describe the design decisions and the genuine Annotation Hub Framework underlying the BIOfid annotations and present agreement results. The tools used to create the annotations are introduced, and the use of the data in the semantic portal is described. Finally, some general lessons, in particular with multiple annotation projects, are drawn.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Bootstrapping semi-supervised annotation method for potential suicidal messages,"Roberto Wellington Acuna Caicedo, Jose Manuel Gomez ´ Soriano,Hector Andres Melgar Sasieta","The suicide of a person is a tragedy that deeply affects families, communities, and countries. According to the standardized rate of suicides per number of inhabitants worldwide, in 2022 there will be approximately about 903,450 suicides and 18,069,000 unconsummated suicides, affecting people of all ages, countries, races, beliefs, social status, economic status, sex, etc. The publication of suicidal intentions by users of social networks has led to the initiation of research processes in this field, to detect them and encourage them not to commit suicide. This study focused on determining a semi-supervised method to populate the Life Corpus, using a bootstrapping technique, to automatically detect and classify texts extracted from social networks and forums related to suicide and depression based on initial supervised samples. To carry out the experiments we used two different classifiers: Support Vector Machine (SVM) (with Bag of Words (BoW) features with and without Term-Frequency/Inverse Document Frequency (Tf/Idf), as a weighted term, and with or without stopwords) and Rasa (with the default feature extraction system). In addition, we performed the experiments using five data collections: Life, Reddit, Life+Reddit, Life_en, and Life_en + Reddit. Using the semi-supervised method, we managed to increase the size of the Life Corpus from 102 to 273 samples with texts from the social network Reddit, in a combination Life+Reddit+BoW_Embeddings, with the SVM classifier, with which a macro f1 value of 0.80 was achieved. These texts were in turn evaluated by annotators manually with a Cohen's Kappa level of agreement of 0.86.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Vers un corpus optimal pour la fouille de textes : stratégie de constitution de corpus spécialisés à partir d'ISTEX,"Camille de Salabert, Sabine Barreaux ","An essential prerequisite for many NLP and text mining activities, the development of a corpus may require several processing phases to improve its quality and thus obtain the best automatic analysis results. The post-processing applied to such a corpus, in particular to guarantee the relevance of its content and the homogeneity of its format, may prove to be all the more costly and tedious as the construction of the work corpus will have been imprecise. This demonstration will propose to take advantage of the ISTEX platform and its associated services to constitute, through an iterative cycle, a homogeneous corpus of scientifically relevant publications for simplified use by excavation tools.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,NEURAL NAMED ENTITY RECOGNITION AND TEMPORAL RELATION EXTRACTION,Meizhi Ju,"Automatically identifying information of interest from texts is one of the most
difficult challenges. One crucial step towards information extraction is named entity recognition, where many entities are embedded in other entities (i.e., nested
entities). Nested entities contain rich fine-grained information, which is essential
in understanding texts. However, most work ignored nested entity recognition
though they are common in many domains. In addition to the semantic information expressed in named entities, temporal information conveyed by named
entities is another important dimension in understanding texts. Temporally classifying the relations (e.g., before) between entities is known as temporal relation
extraction, which is required in many tasks such as text summarisation.
The thesis is the first comprehensive research focusing on nested entity recognition for information extraction using neural network methods. In this research,
we describe our work on (1) neural nested entity recognition; (2) evaluation on
different domains of corpora; (3) task-specific evaluation including (a) neuroscience entity extraction; (b) screening reference documents; (c) extraction of
medication and adverse drug information; (d) and extraction of chronic obstructive pulmonary disease phenotypes. In addition to nested entity recognition,e further investigate neural temporal relation extraction, which focuses on the
extraction of both intra-sentence and inter-sentence temporal relations.",COPIOUS: A gold standard corpus of named entities towards extracting species occurrence from biodiversity literature,23,0,,,
Entity Matching,Covid-on-the-Web: Knowledge Graph and Services to Advance COVID-19 Research,"Franck Michel, Fabien Gandon, Valentin Ah-Kane, Anna Bobasheva, Elena Cabrio, Olivier Corby, Raphaël Gazzotti, Alain Giboin, Santiago Marro, Tobias Mayer, Mathieu Simon, Serena Villata & Marco Winckler","Scientists are harnessing their multidisciplinary expertise and resources to fight the COVID-19 pandemic. Aligned with this mind-set, the Covid-on-the-Web project aims to allow biomedical researchers to access, query and make sense of COVID-19 related literature. To do so, it adapts, combines and extends tools to process, analyze and enrich the ""COVID-19 Open Research Dataset"" (CORD-19) that gathers 50,000+ full-text scientific articles related to the coronaviruses. We report on the RDF dataset and software resources produced in this project by leveraging skills in knowledge representation, text, data and argument mining, as well as data visualization and exploration. The dataset comprises two main knowledge graphs describing (1) named entities mentioned in the CORD-19 corpus and linked to DBpedia, Wikidata and other BioPortal vocabularies, and (2) arguments extracted using ACTA, a tool automating the extraction and visualization of argumentative graphs, meant to help clinicians analyze clinical trials and make decisions. On top of this dataset, we provide several visualization and exploration tools based on the Corese Semantic Web platform, MGExplorer visualization library, as well as the Jupyter Notebook technology. All along this initiative, we have been engaged in discussions with healthcare and medical research institutes to align our approach with the actual needs of the biomedical community, and we have paid particular attention to comply with the open and reproducible science goals, and the FAIR principles.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,1,CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),"Franck Michel, Fabien Gandon, Raphaël Gazzotti,","CORD-19 Named Entities Knowledge Graph (CORD19-NEKG) is an RDF dataset describing named entities identified in the scholarly articles of the COVID-19 Open Research Dataset (CORD-19), a resource of over 47,000 articles about COVID-19 and the coronavirus family of viruses."
Entity Matching,Argument Mining on Clinical Trials,"Tobias Mayer, Elena Cabrio, Marco Lippi, Paolo Torroni, S. Villata ","Argument-based decision making has been employed to support a variety of reasoning tasks over medical knowledge. These include evidence-based justifications of the effects of treatments, the detection of conflicts in the knowledge
base, and the enabling of uncertain and defeasible reasoning in the health-care sector. However, a common limitation of these approaches is that they rely on structured input information. Recent advances in argument mining have shown increasingly accurate results in detecting argument components and predicting their relations from unstructured, natural language texts. In this study, we discuss evidence
and claim detection from Randomized Clinical Trials. To this end, we create a new
annotated dataset about four different diseases (glaucoma, diabetes, hepatitis B, and
hypertension), containing 976 argument components (697 containing evidence, 279
claims). Empirical results are promising, and show the portability of the proposed
approach over different branches of medicine.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,EABlock: a declarative entity alignment block for knowledge graph creation pipelines,"Samaneh Jozashoori, Ahmad Sakor, Enrique Iglesias, Maria-Esther Vidal","Despite encoding enormous amount of rich and valuable data, existing data sources are mostly created independently, being a significant challenge to their integration. Mapping languages, e.g., RML and R2RML, facilitate declarative specification of the process of applying meta-data and integrating data into a knowledge graph. Mapping rules can also include knowledge extraction functions in addition to expressing correspondences among data sources and a unified schema. Combining mapping rules and functions represents a powerful formalism to specify pipelines for integrating data into a knowledge graph transparently. Surprisingly, these formalisms are not fully adapted, and many knowledge graphs are created by executing ad-hoc programs to pre-process and integrate data. In this paper, we present EABlock, an approach integrating Entity Alignment (EA) as part of RML mapping rules. EABlock includes a block of functions performing entity recognition from textual attributes and link the recognized entities to the corresponding resources in Wikidata, DBpedia, and domain specific thesaurus, e.g., UMLS. EABlock provides agnostic and efficient techniques to evaluate the functions and transfer the mappings to facilitate its application in any RML-compliant engine. We have empirically evaluated EABlock performance, and results indicate that EABlock speeds up knowledge graph creation pipelines that require entity recognition and linking in state-of-the-art RML-compliant engines. EABlock is also publicly available as a tool through a GitHub repository and a DOI.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Knowledge Graphs for COVID-19: An Exploratory Review of the Current Landscape,"Avishek Chatterjee, Cosimo Nardi, Cary Oberije, Philippe Lambin ","Background: Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts. The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation. When the answer to a search query requires linking together multiple pieces of information across documents, simple keyword searches are insufficient. To answer such complex information needs, an innovative artificial intelligence (AI) technology named a knowledge graph (KG) could prove to be effective. Methods: We conducted an exploratory literature review of KG applications in the context of COVID-19. The search term used was “covid-19 knowledge graph”. In addition to PubMed, the first five pages of search results for Google Scholar and Google were considered for inclusion. Google Scholar was used to include non-peer-reviewed or non-indexed articles such as pre-prints and conference proceedings. Google was used to identify companies or consortiums active in this domain that have not published any literature, peer-reviewed or otherwise. Results: Our search yielded 34 results on PubMed and 50 results each on Google and Google Scholar. We found KGs being used for facilitating literature search, drug repurposing, clinical trial mapping, and risk factor analysis. Conclusions: Our synopses of these works make a compelling case for the utility of this nascent field of research.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Grafos de conocimiento para gestionar información epidemiológica sobre COVID-19,"Tatiana Delgado Fernández, Mavis Lis Stuart Cárdenas, Mercedes Delgado Fernández","The control of the spread of infectious diseases requires exhaustive epidemiological investigations, which has been validated by the performance of the Ministry of Public Health over several decades in combating numerous diseases such as dengue, cholera and various types of influenza , among other. However, the COVID-19 pandemic is testing the most rigorous epidemiological protocols in Cuba and the world due to its high capacity for contagion and spread. Given this context, this article proposed to use knowledge graphs to support epidemiological studies of COVID-19, placing greater emphasis on exposure factors and contact tracing. To achieve this objective, a study related to the state of the art on knowledge graphs and their use in the health sector, particularly in the fight against the new SARS-CoV-2 coronavirus, was carried out. The research was supported by a methodological approach to the creation and use of knowledge graphs adapted to the field of study. The results are simulated in the scenario of the outbreak that occurred in mid-July 2020 in the municipality of Bauta in the province of Artemisa, using reality data, extracted from the Web, combined with other simulated data.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Mining RDF Data of COVID-19 Scientific Literature for Interesting Association Rules,Lucie Cadorel; Andrea G. B. Tettamanzi,"In the context of the global effort to study, understand, and fight the new Coronavirus, prompted by the
publication of a rich, reusable linked data containing named
entities mentioned in the COVID-19 Open Research Dataset,
a large corpus of scientific articles related to coronaviruses,
we propose a method to discover interesting association rules
from an RDF knowledge graph, by combining clustering,
community detection, and dimensionality reduction, as well as
criteria for filtering the discovered association rules in order to
keep only the most interesting rules. Our results demonstrate
the effectiveness and scalability of the proposed method and
suggest several possible uses of the discovered rules, including
(i) curating the knowledge graph by detecting errors, (ii)
finding relevant and coherent collections of scientific articles,
and (iii) suggesting novel hypotheses to biomedical researchers
for further investigation.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,ARViz: Interactive Visualization of Association Rules for RDF Data Exploration,Aline Menin; Lucie Cadorel; Andrea Tettamanzi; Alain Giboin; Fabien Gandon; Marco Winckler,"Association rule mining often leads the analyst into a rough rummaging process to identify rules that are relevant to understand specific problems. We propose a visualization interface to assist the rule selection process and evaluate it on an RDF knowledge graph derived from the COVID-19 Open Research Dataset. The user interface supports data exploration with focus on the overview of rules through a scatter plot, subsets of rules through a chord diagram chart, and itemsets through an association graph which is dynamically created by entering an item of interest (i.e. a named entity). Further, the analyst can interactively recover a list of publications containing the named entities involved in a particular rule. Among the original aspects of our approach, we highlight the representation of attributes describing measures of interest (i.e. confidence and interestingness), a visual indication of existence (or not) of symmetry in association rules, the exploration of subsets of rules according to clusters of publications and named entities, and an interactive prompting that aims at expanding the discovery of named entities within selected association rules. We assess our approach through a semi-structured interview involving experts in the domains of data mining and biomedicine, whose feedback could assist the refinement of the visual and interaction tools.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Covid-on-the-Web: Exploring the COVID-19 scientific literature through visualization of linked data from entity and argument mining,"Aline Menin, Franck Michel, Fabien Gandon, Raphaël Gazzotti, Elena Cabrio, Olivier Corby, Alain Giboin, Santiago Marro, Tobias Mayer, Serena Villata, Marco Winckler ","The unprecedented mobilization of scientists caused by the COVID-19 pandemic has generated an enormous number of scholarly articles that are impossible for a human being to keep track of and explore without appropriate tool support. In this context, we created the Covid-on-the-Web project, which aims to assist the accessing, querying, and sense-making of COVID-19-related literature by combining efforts from the semantic web, natural language processing, and visualization fields. In particular, in this paper we present an RDF data set (a linked version of the “COVID-19 Open Research Dataset” (CORD-19), enriched via entity linking and argument mining) and the “Linked Data Visualizer” (LDViz), which assists the querying and visual exploration of the referred data set. The LDViz tool assists in the exploration of different views of the data by combining a querying management interface, which enables the definition of meaningful subsets of data through SPARQL queries, and a visualization interface based on a set of six visualization techniques integrated in a chained visualization concept, which also supports the tracking of provenance information. We demonstrate the potential of our approach to assist biomedical researchers in solving domain-related tasks, as well as to perform exploratory analyses through use case scenarios.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Covid-on-the-Web : graphe de connaissances et services pour faire progresser la recherche sur la COVID-19,"Franck Michel, Fabien Gandon, Valentin Ah-Kane, Anna Bobasheva, Elena Cabrio, Olivier Corby, Raphaël Gazzotti, Alain Giboin, Santiago Marro,Tobias Mayer,Mathieu Simon, Serena Villata, Marco Winckler","The Covid-on-the-Web project allows researchers to access, interrogate and extract knowledge from the literature related to the coronavirus family. It aligns with concrete needs formulated by health and research institutes. Thus, it adapts, combines and extends tools intended to process, analyze and enrich the CORD-19 corpus which brings together more than 100,000 scientific articles relating to coronaviruses. This dataset includes two main knowledge graphs describing (1) 113 million named entity mentions linked to the web of data, and (2) the arguments extracted using ACTA, an extraction and visualization of argumentative graphs. We also provide several visualization and exploration tools based on the Corese platform",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,CKGSE: A Prototype Search Engine for Chinese Knowledge Graphs,"Xiaxia Wang, Tengteng Lin, Weiqing Luo, Gong Cheng, Yuzhong Qu","Nowadays, with increasing open knowledge graphs (KGs) being published on the Web, users depend on open data portals and search engines to find KGs. However, existing systems provide search services and present results with only metadata while ignoring the contents of KGs, i.e., triples. It brings difficulty for users' comprehension and relevance judgement. To overcome the limitation of metadata, in this paper we propose a content-based search engine for open KGs named CKGSE. Our system provides keyword search, KG snippet generation, KG profiling and browsing, all based on KGs' detailed, informative contents rather than their brief, limited metadata. To evaluate its usability, we implement a prototype with Chinese KGs crawled from OpenKG.CN and report some preliminary results and findings.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,An Analysis of COVID-19 Knowledge Graph Construction and Applications,Dominic Flocco; Bryce Palmer-Toy; Ruixiao Wang; Hongyu Zhu; Rishi Sonthalia; Junyuan Lin; Andrea L. Bertozzi; P. Jeffrey Brantingham,"The construction and application of knowledge graphs have seen a rapid increase across many disciplines in recent years. Additionally, the problem of uncovering relationships between developments in the COVID-19 pandemic and social media behavior is of great interest to researchers hoping to curb the spread of the disease. In this paper we present a knowledge graph constructed from COVID-19 related tweets in the Los Angeles area, supplemented with federal and state policy announcements and disease spread statistics. By incorporating dates, topics, and events as entities, we construct a knowledge graph that describes the connections between these useful information. We use natural language processing and change point analysis to extract tweet-topic, tweet-date, and event-date relations. Further analysis on the constructed knowledge graph provides insight into how tweets reflect public sentiments towards COVID-19 related topics and how changes in these sentiments correlate with real-world events.",CORD-19 Named Entities Knowledge Graph (CORD19-NEKG),24,0,,,
Entity Matching,Ace 2004 multilingual training corpus,"A Mitchell, S Strassel, S Huang, R Zakhary ","ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program. The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.",ACE 2004 Dataset,25,1,ACE 2004 Dataset,A Mitchell,"ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program. The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic."
Entity Matching,Design Challenges for Entity Linking,"Xiao Ling, Sameer Singh, Daniel S. Weld","Recent research on entity linking (EL) has introduced a plethora of promising techniques, ranging from deep neural networks to joint inference. But despite numerous papers there is surprisingly little understanding of the state of the art in EL. We attack this confusion by analyzing differences between several versions of the EL problem and presenting a simple yet effective, modular, unsupervised system, called Vinculum, for entity linking. We conduct an extensive evaluation on nine data sets, comparing Vinculum with two state-of-the-art systems, and elucidate key aspects of the system that include mention extraction, candidate generation, entity type prediction, entity coreference, and coherence.",ACE 2004 Dataset,25,0,,,
Entity Matching,Improved Relation Extraction with Feature-Rich Compositional Embedding Models,"Matthew R. Gormley, Mo Yu, Mark Dredze","Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.",ACE 2004 Dataset,25,0,,,
Entity Matching,Evaluation Metrics For End-to-End Coreference Resolution Systems,"Jie Cai, Michael Strube","Commonly used coreference resolution
evaluation metrics can only be applied to
key mentions, i.e. already annotated mentions. We here propose two variants of the
B 3 and CEAF coreference resolution evaluation algorithms which can be applied
to coreference resolution systems dealing
with system mentions, i.e. automatically
determined mentions. Our experiments
show that our variants lead to intuitive and
reliable results.",ACE 2004 Dataset,25,0,,,
Entity Matching,End-to-End Coreference Resolution via Hypergraph Partitioning,"Jie Cai, Michael Strube","We describe a novel approach to coreference resolution which implements a
global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps,
but perform coreference resolution globally in one step. Our hypergraph-based
global model implemented within an endto-end coreference resolution system outperforms two strong baselines (Soon et al.,
2001; Bengtson & Roth, 2008) using system mentions only",ACE 2004 Dataset,25,0,,,
Entity Matching,A Pipeline Arabic Named Entity Recognition Using a Hybrid Approach,"Mai Mohamed Oudah, Khaled Shaalan","Most Arabic Named Entity Recognition (NER) systems have been developed using either of two
approaches: a rule-based or Machine Learning (ML) based approach, with their strengths and
weaknesses. In this paper, the problem of Arabic NER is tackled through integrating the two
approaches together in a pipelined process to create a hybrid system with the aim of enhancing
the overall performance of NER tasks. The proposed system is capable of recognizing 11
different types of named entities (NEs): Person, Location, Organization, Date, Time, Price,
Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments are
conducted using three different ML classifiers to evaluate the overall performance of the hybrid
system. The empirical results indicate that the hybrid approach outperforms both the rule-based
and the ML-based approaches. Moreover, our system outperforms the state-of-the-art of Arabic
NER in terms of accuracy when applied to ANERcorp dataset, with f-measures 94.4% for
Person, 90.1% for Location, and 88.2% for Organization. ",ACE 2004 Dataset,25,0,,,
Entity Matching,A hybrid approach to Arabic named entity recognition,"Khaled Shaalan, Mai Oudah","In this paper, we propose a hybrid named entity recognition (NER) approach that takes the advantages of rule-based and machine learning-based approaches in order to improve the overall system performance and overcome the knowledge elicitation bottleneck and the lack of resources for underdeveloped languages that require deep language processing, such as Arabic. The complexity of Arabic poses special challenges to researchers of Arabic NER, which is essential for both monolingual and multilingual applications. We used the hybrid approach to develop an Arabic NER system that is capable of recognizing 11 types of Arabic named entities: Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments were conducted using decision trees, Support Vector Machines and logistic regression classifiers to evaluate the system performance. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches when they are processed independently. More importantly, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp standard dataset, with F-measures 0.94 for Person, 0.90 for Location and 0.88 for Organization.",ACE 2004 Dataset,25,0,,,
Entity Matching,CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing,"Ossama Obeid, Nasser Zalmout, Salam Khalifa, Dima Taji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl Eryani, Alexander Erdmann, Nizar Habash","Over the last two decades, there have been many efforts to
develop resources to support Arabic natural language processing (NLP). Some of these resources target specific NLP
tasks such as tokenization, diacritization or sentiment analysis, while others attempt to target multiple tasks jointly.
Most resources focus on a specific variety of Arabic such
as Modern Standard Arabic (MSA), or Egyptian Arabic.
These resource also vary in terms of the programming languages they use, the types of interfaces they provide, the
data representations and standards they utilize, and the degree of public availability (e.g., open-source, commercial,
unavailable). These variations make it difficult to write new
applications that combine multiple resources.
To address many of these issues, we present CAMeL Tools,
an open-source Python toolkit that supports Arabic and
Arabic dialect pre-processing, morphological modeling, dialect identification, named entity recognition and sentiment
analysis. CAMeL Tools provides command-line interfaces
(CLIs) and application programming interfaces (APIs) covering these utilities.
The rest of the paper is organized as follows. We present
some background on the difficulty of processing Arabic text
(Section 2), and then discuss previous work on a variety of
Arabic NLP tasks (Section 3). We describe the design and
implementation of CAMeL Tools (Section 4) and provide
more details about each of its components (Sections 6 to 9).
Finally, we discuss some future additions to CAMeL Tools
(Section 10).",ACE 2004 Dataset,25,0,,,
Entity Matching,Uncovering and managing the impact of methodological choices for the computational construction of socio-technical networks from texts,Jana Diesner,"This thesis is motivated by the need for scalable and reliable methods and technologies that support the construction of network data based on information from text data. Ultimately, the resulting data can be used for answering substantive and graph-theoretical questions about sociotechnical networks. One main limitation with constructing network data from text data is that the validation of the resulting network data can be hard to infeasible, e.g. in the cases of covert, historical and large scale networks. This thesis addresses this problem by identifying the impact of coding choices that must be made when extracting network data from text data on the structure of networks and network analysis results. My findings suggest that conducting reference resolution on text data can alter the identity and weight of 76 of the nodes and 23 of the links, and can cause major changes in the value of commonly used network metrics. Also, performing reference resolution prior to relation extraction leads to the retrieval of completely different sets of key entities in comparison to not applying this pre-processing technique. Based on the outcome of the presented experiments, I recommend strategies for avoiding or mitigating the identified issues in practical applications. When extracting socio-technical networks from texts, the set of relevant node classes might go beyond the classes that are typically supported by tools for named entity extraction. I address this lack of technology by developing an entity extractor that combines an ontology for sociotechnical networks that originates from the social sciences, is theoretically grounded and has been empirically validated in prior work, with a supervised machine learning technique that is based on probabilistic graphical models.
",ACE 2004 Dataset,25,0,,,
Entity Matching,The Colorado Richly Annotated Full Text (CRAFT) Corpus: Multi-Model Annotation in the Biomedical Domain,"K. Bretonnel Cohen, Karin Verspoor, Karën Fort, Christopher Funk, Michael Bada, Martha Palmer & Lawrence E. Hunter ","The Colorado Richly Annotated Full Text (CRAFT) corpus consists of full-text journal articles. The primary motivation for the annotation project was the accumulating body of evidence indicating that the bodies of journal articles contain much information that is not present in the abstracts, and that the textual and structural characteristics of article bodies are different from those of abstracts. The development of CRAFT was characterized by a “multi-model” annotation task. The sample population was all journal articles that had been used by the Mouse Genome Informatics group as evidence for at least one Gene Ontology or Mouse Phenotype Ontology “annotation.” The linguistic annotation is represented in the widely known Penn Treebank format (Marcus et al., Comput. Linguist. 19(2), 313–330, 1993) [50], with the addition of a small number of tags and phrasal categories to accommodate the idiosyncrasies of the domain.",ACE 2004 Dataset,25,0, ,,
Entity Matching,2.0: Improving coverage and performance of rule-based named entity recognition for Arabic,"MAI OUDAH, KHALED SHAALAN NERA ","Named Entity Recognition (NER) is an essential task for many natural language processing systems, which makes use of various linguistic resources. NER becomes more complicated when the language in use is morphologically rich and structurally complex, such as Arabic. This language has a set of characteristics that makes it particularly challenging to handle. In a previous work, we have proposed an Arabic NER system that follows the hybrid approach, i.e. integrates both rule-based and machine learning-based NER approaches. Our hybrid NER system is the state-of-the-art in Arabic NER according to its performance on standard evaluation datasets. In this article, we discuss a novel methodology for overcoming the coverage drawback of rule-based NER systems in order to improve their performance and allow for automated rule update. The presented mechanism utilizes the recognition decisions made by the hybrid NER system in order to identify the weaknesses of the rule-based component and derive new linguistic rules aiming at enhancing the rule base, which will help in achieving more reliable and accurate results. We used ACE 2004 Newswire standard dataset as a resource for extracting and analyzing new linguistic rules for person, location and organization names recognition. We formulate each new rule based on two distinctive feature groups, i.e. Gazetteers of each type of named entities and Part-of-Speech tags, in particular noun and proper noun. Fourteen new patterns are derived, formulated as grammar rules, and evaluated in terms of coverage. The conducted experiments exploit a POS tagged version of the ACE 2004 NW dataset. The empirical results show that the performance of the enhanced rule-based system, i.e. NERA 2.0, improves the coverage of the previously misclassified person, location and organization named entities types by 69.93 per cent, 57.09 per cent and 54.28 per cent, respectively.",ACE 2004 Dataset,25,0, , ,
Entity Matching,Laying the Groundwork for Knowledge Base Population: Nine Years of Linguistic Resources for TAC KBP,"Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi Song, Jennifer Tracey","Knowledge Base Population (KBP) is an evaluation series within the Text Analysis Conference (TAC) evaluation campaign conducted
by the National Institute of Standards and Technology (NIST). Over the past nine years TAC KBP evaluations have targeted
information extraction technologies for the population of knowledge bases comprised of entities, relations, and events. Linguistic Data
Consortium (LDC) has supported TAC KBP since 2009, developing, maintaining, and distributing linguistic resources in three
languages for seven distinct evaluation tracks. This paper describes LDC's resource creation efforts for the various KBP tracks, and
highlights changes made over the years to support evolving evaluation requirements.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2014,26,1,TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2014,"Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi Song, Jennifer Tracey","TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2014 was developed by the Linguistic Data Consortium and contains training and evaluation data produced in support of the TAC KBP Chinese Cross-lingual Entity Linking tasks in 2011, 2012, 2013 and 2014. It includes queries and gold standard entity type information, Knowledge Base links, and equivalence class clusters for NIL entities along with the source documents for the queries, specifically, English and Chinese newswire, discussion forum and web data. The corresponding knowledge base is available as TAC KBP Reference Knowledge Base (LDC2014T16). Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST). TAC was developed to encourage research in natural language processing and related applications by providing a large test collection, common evaluation procedures, and a forum for researchers to share their results. Through its various evaluations, the Knowledge Base Population (KBP) track of TAC encourages the development of systems that can match entities mentioned in natural texts with those appearing in a knowledge base and extract novel information about entities from a document collection and add it to a new or existing knowledge base. Chinese Cross-lingual Entity Linking was first conducted as part of the 2011 TAC KBP evaluations. The track was an extension of the monolingual English Entity Linking track (EL) whose goal is to measure systems’ ability to determine whether an entity, specified by a query, has a matching node in a reference knowledge base (KB) and, if so, to create a link between the two. If there is no matching node for a query entity in the KB, EL systems are required to cluster the mention together with others referencing the same entity. More information about the TAC KBP Entity Linking task and other TAC KBP evaluations can be found on the NIST TAC website. Data All source documents were originally released as XML but have been converted to text files for this release. This change was made primarily because the documents were used as text files during data development but also because some fail XML parsing. Acknowledgement This material is based on research sponsored by Air Force Research Laboratory and Defense Advance Research Projects Agency under agreement number FA8750-13-2-0045. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Air Force Research Laboratory and Defense Advanced Research Projects Agency or the U.S. Government.
"
Entity Matching,KnowledgeNet: A Benchmark Dataset for Knowledge Base Population,"Filipe Mesquita, Matteo Cannaviccio, Jordan Schmidek, Paramita Mirza, Denilson Barbosa","KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2015,26,0, ,,
Entity Matching,KBPearl: a knowledge base population system supported by joint entity and relation linking,"Xueling Lin, Haoyang Li, Hao Xin, Zijian Li, Lei Chen","Nowadays, most openly available knowledge bases (KBs) are incomplete, since they are not synchronized with the emerging facts happening in the real world. Therefore, knowledge base population (KBP) from external data sources, which extracts knowledge from unstructured text to populate KBs, becomes a vital task. Recent research proposes two types of solutions that partially address this problem, but the performance of these solutions is limited. The first solution, dynamic KB construction from unstructured text, requires specifications of which predicates are of interest to the KB, which needs preliminary setups and is not suitable for an in-time population scenario. The second solution, Open Information Extraction (Open IE) from unstructured text, has limitations in producing facts that can be directly linked to the target KB without redundancy and ambiguity. In this paper, we present an end-to-end system, KBPearl, for KBP, which takes an incomplete KB and a large corpus of text as input, to (1) organize the noisy extraction from Open IE into canonicalized facts; and (2) populate the KB by joint entity and relation linking, utilizing the context knowledge of the facts and the side information inferred from the source text. We demonstrate the effectiveness and efficiency of KBPearl against the state-of-the-art techniques, through extensive experiments on real-world datasets.

",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2016,26,0,,,
Entity Matching,Open Knowledge Enrichment for Long-tail Entities,"Ermei Cao, Difeng Wang, Jiacheng Huang, Wei Hu","Knowledge bases (KBs) have gradually become a valuable asset for many AI applications. While many current KBs are quite large, they are widely acknowledged as incomplete, especially lacking facts of long-tail entities, e.g., less famous persons. Existing approaches enrich KBs mainly on completing missing links or filling missing values. However, they only tackle a part of the enrichment problem and lack specific considerations regarding long-tail entities. In this paper, we propose a full-fledged approach to knowledge enrichment, which predicts missing properties and infers true facts of long-tail entities from the open Web. Prior knowledge from popular entities is leveraged to improve every enrichment step. Our experiments on the synthetic and real-world datasets and comparison with related work demonstrate the feasibility and superiority of the approach.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2017,26,0,,,
Entity Matching,Automated Extraction of Socio-political Events from News (AESPEN): Workshop and Shared Task Report,"Ali Hürriyetoğlu, Vanni Zavarella, Hristo Tanev, Erdem Yörük, Ali Safaya, Osman Mutlu","We describe our effort on automated extraction of socio-political events from news in the scope of a workshop and a shared task we organized at Language Resources and Evaluation Conference (LREC 2020). We believe the event extraction studies in computational linguistics and social and political sciences should further support each other in order to enable large scale socio-political event information collection across sources, countries, and languages. The event consists of regular research papers and a shared task, which is about event sentence coreference identification (ESCI), tracks. All submissions were reviewed by five members of the program committee. The workshop attracted research papers related to evaluation of machine learning methodologies, language resources, material conflict forecasting, and a shared task participation report in the scope of socio-political event information collection. It has shown us the volume and variety of both the data sources and event information collection approaches related to socio-political events and the need to fill the gap between automated text processing techniques and requirements of social and political sciences.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2018,26,0,,,
Entity Matching,Scalable Knowledge Graph Construction from Text Collections,"Ryan Clancy, Ihab F. Ilyas, Jimmy Lin","We present a scalable, open-source platform that “distills” a potentially large text collection into a knowledge graph. Our platform takes documents stored in Apache Solr and scales out the Stanford CoreNLP toolkit via Apache Spark integration to extract mentions and relations that are then ingested into the Neo4j graph database. The raw knowledge graph is then enriched with facts extracted from an external knowledge graph. The complete product can be manipulated by various applications using Neo4j’s native Cypher query language: We present a subgraph-matching approach to align extracted relations with external facts and show that fact verification, locating textual support for asserted facts, detecting inconsistent and missing facts, and extracting distantly-supervised training data can all be performed within the same framework.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2019,26,0,,,
Entity Matching,New Insights into Cross-Document Event Coreference: Systematic Comparison and a Simplified Approach,"Andres Cremisini, Mark Finlayson","Cross-Document Event Coreference (CDEC) is the task of finding coreference relationships between events in separate documents, most commonly assessed using the Event Coreference Bank+ corpus (ECB+). At least two different approaches have been proposed for CDEC on ECB+ that use only event triggers, and at least four have been proposed that use both triggers and entities. Comparing these approaches is complicated by variation in the systems’ use of gold vs. computed labels, as well as variation in the document clustering pre-processing step. We present an approach that matches or slightly beats state-of-the-art performance on CDEC over ECB+ with only event trigger annotations, but with a significantly simpler framework and much smaller feature set relative to prior work. This study allows us to directly compare with prior systems and draw conclusions about the effectiveness of various strategies. Additionally, we provide the first cross-validated evaluation on the ECB+ dataset; the first explicit evaluation of the pairwise event coreference classification step; and the first quantification of the effect of document clustering on system performance. The last in particular reveals that while document clustering is a crucial pre-processing step, improvements can at most provide for a 3 point improvement in CDEC performance, though this might be attributable to ease of document clustering on ECB+.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2020,26,0,,,
Entity Matching,"Cross-Document, Cross-Language Event Coreference Annotation Using Event Hoppers ","Zhiyi Song, Ann Bies, Justin Mott, Xuansong Li, Stephanie Strassel, Christopher Caruso ","Cross-Document Event Coreference (CDEC) is the task of finding coreference relationships between events in separate documents, most commonly assessed using the Event Coreference Bank+ corpus (ECB+). At least two different approaches have been proposed for CDEC on ECB+ that use only event triggers, and at least four have been proposed that use both triggers and entities. Comparing these approaches is complicated by variation in the systems’ use of gold vs. computed labels, as well as variation in the document clustering pre-processing step. We present an approach that matches or slightly beats state-of-the-art performance on CDEC over ECB+ with only event trigger annotations, but with a significantly simpler framework and much smaller feature set relative to prior work. This study allows us to directly compare with prior systems and draw conclusions about the effectiveness of various strategies. Additionally, we provide the first cross-validated evaluation on the ECB+ dataset; the first explicit evaluation of the pairwise event coreference classification step; and the first quantification of the effect of document clustering on system performance. The last in particular reveals that while document clustering is a crucial pre-processing step, improvements can at most provide for a 3 point improvement in CDEC performance, though this might be attributable to ease of document clustering on ECB+.
",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2021,26,0,,,
Entity Matching,GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System,"Manling Li, Ying Lin, Ananya Subburathinam, Spencer Whitehead, Xiaoman Pan, Di Lu, Qingyun Wang, Tongtao Zhang, Lifu Huang, Heng Ji, Alireza Zareian, Hassan Akbari, Brian Chen, Bo Wu, Emily Allaway, Shih-Fu Chang, Kathleen R. McKeown, Yixiang Yao, Jennifer Chen, Eric Berquist, Kexuan Sun, Xujun Peng, Ryan Gabbard, Marjorie Freedman, Pedro A. Szekely, T. K. Satish Kumar, Arka Sadhu, Ram Nevatia, Miguel E. Rodríguez, Yifan Wang, Yang Bai, Ali Sadeghian, Daisy Zhe Wang","In the past year the GAIA team has improved our
end-to-end knowledge extraction, grounding, inference, clustering and hypothesis generation system that covers all languages (English, Russian
and Ukrainian), data modalities and knowledge element types defined in new AIDA ontologies. We
participated in the evaluations of all tasks within
TA1, TA2 and TA3 and achieved highly competitive performance. Our TA1 system achieves top
performance at both intrinsic evaluation and extrinsic evaluation through TA2 and TA3. The system incorporates a number of impactful and fresh
research innovations:",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2022,26,0,,,
Entity Matching,BioRelEx 1.0: Biological Relation Extraction Benchmark,"Hrant Khachatrian, Lilit Nersisyan, Karen Hambardzumyan, Tigran Galstyan, Anna Hakobyan, Arsen Arakelyan, Andrey Rzhetsky, Aram Galstyan","Automatic extraction of relations and interactions between biological entities from scientific literature remains an extremely challenging problem in biomedical information extraction and natural language processing in general. One of the reasons for slow progress is the relative scarcity of standardized and publicly available benchmarks. In this paper we introduce BioRelEx, a new dataset of fully annotated sentences from biomedical literature that capture binding interactions between proteins and/or biomolecules. To foster reproducible research on the interaction extraction task, we define a precise and transparent evaluation process, tools for error analysis and significance tests. Finally, we conduct extensive experiments to evaluate several baselines, including SciIE, a recently introduced neural multi-task architecture that has demonstrated state-of-the-art performance on several tasks.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2023,26,0,,,
Entity Matching,Cross-lingual Structure Transfer for Zero-resource Event Extraction,"Di Lu, Ananya Subburathinam, Heng Ji, Jonathan May, Shih-Fu Chang, Avi Sil, Clare Voss","Most of the current cross-lingual transfer learning methods for Information Extraction (IE) have been only applied to name tagging. To tackle more complex tasks such as event extraction we need to transfer graph structures (event trigger linked to multiple arguments with various roles) across languages. We develop a novel share-and-transfer framework to reach this goal with three steps: (1) Convert each sentence in any language to language-universal graph structures; in this paper we explore two approaches based on universal dependency parses and complete graphs, respectively. (2) Represent each node in the graph structure with a cross-lingual word embedding so that all sentences in multiple languages can be represented with one shared semantic space. (3) Using this common semantic space, train event extractors from English training data and apply them to languages that do not have any event annotations. Experimental results on three languages (Spanish, Russian and Ukrainian) without any annotations show this framework achieves comparable performance to a state-of-the-art supervised model trained from more than 1,500 manually annotated event mentions.",TAC KBP Chinese Cross-lingual Entity Linking - Comprehensive Training and Evaluation Data 2011-2024,26,0,,,
Entity Matching,Covid-on-the-Web: Knowledge Graph and Services to Advance COVID-19 Research,"Franck Michel, Fabien Gandon, Valentin Ah-Kane, Anna Bobasheva, Elena Cabrio, Olivier Corby, Raphaël Gazzotti, Alain Giboin, Santiago Marro, Tobias Mayer, Mathieu Simon, Serena Villata & Marco Winckler ","Scientists are harnessing their multidisciplinary expertise and resources to fight the COVID-19 pandemic. Aligned with this mind-set, the Covid-on-the-Web project aims to allow biomedical researchers to access, query and make sense of COVID-19 related literature. To do so, it adapts, combines and extends tools to process, analyze and enrich the ""COVID-19 Open Research Dataset"" (CORD-19) that gathers 50,000+ full-text scientific articles related to the coronaviruses. We report on the RDF dataset and software resources produced in this project by leveraging skills in knowledge representation, text, data and argument mining, as well as data visualization and exploration. The dataset comprises two main knowledge graphs describing (1) named entities mentioned in the CORD-19 corpus and linked to DBpedia, Wikidata and other BioPortal vocabularies, and (2) arguments extracted using ACTA, a tool automating the extraction and visualization of argumentative graphs, meant to help clinicians analyze clinical trials and make decisions. On top of this dataset, we provide several visualization and exploration tools based on the Corese Semantic Web platform, MGExplorer visualization library, as well as the Jupyter Notebook technology. All along this initiative, we have been engaged in discussions with healthcare and medical research institutes to align our approach with the actual needs of the biomedical community, and we have paid particular attention to comply with the open and reproducible science goals, and the FAIR principles.",Covid-on-the-Web dataset,27,1,Covid-on-the-Web dataset,"Franck Michel, Fabien Gandon, Valentin Ah-Kane, Anna Bobasheva, Elena Cabrio, Olivier Corby, Raphaël Gazzotti, Alain Giboin, Santiago Marro, Tobias Mayer, Mathieu Simon, Serena Villata & Marco Winckler ","This RDF dataset provides two main knowledge graphs produced by processing the scholarly articles of the COVID-19 Open Research Dataset (CORD-19), a resource of articles about COVID-19 and the coronavirus family of viruses.

The CORD-19 Named Entities Knowledge Graph describes named entities identified and disambiguated by NCBO BioPortal annotator, Entity-fishing and DBpedia Spotlight. The CORD-19 Argumentative Knowledge Graph describes argumentative components and PICO elements extracted from the articles by the Argumentative Clinical Trial Analysis platform (ACTA).

Homepage: https://github.com/Wimmics/CovidOnTheWeb"
Entity Matching,Knowledge Graphs for COVID-19: An Exploratory Review of the Current Landscape,"Avishek Chatterjee, Cosimo Nardi, Cary Oberije, Philippe Lambin"," Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts. The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation. When the answer to a search query requires linking together multiple pieces of information across documents, simple keyword searches are insufficient. To answer such complex information needs, an innovative artificial intelligence (AI) technology named a knowledge graph (KG) could prove to be effective. Methods: We conducted an exploratory literature review of KG applications in the context of COVID-19. The search term used was “covid-19 knowledge graph”. In addition to PubMed, the first five pages of search results for Google Scholar and Google were considered for inclusion. Google Scholar was used to include non-peer-reviewed or non-indexed articles such as pre-prints and conference proceedings. Google was used to identify companies or consortiums active in this domain that have not published any literature, peer-reviewed or otherwise. Results: Our search yielded 34 results on PubMed and 50 results each on Google and Google Scholar. We found KGs being used for facilitating literature search, drug repurposing, clinical trial mapping, and risk factor analysis. Conclusions: Our synopses of these works make a compelling case for the utility of this nascent field of research.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,EABlock: a declarative entity alignment block for knowledge graph creation pipelines,"Samaneh Jozashoori, Ahmad Sakor, Enrique Iglesias, Maria-Esther Vidal","Despite encoding enormous amount of rich and valuable data, existing data sources are mostly created independently, being a significant challenge to their integration. Mapping languages, e.g., RML and R2RML, facilitate declarative specification of the process of applying meta-data and integrating data into a knowledge graph. Mapping rules can also include knowledge extraction functions in addition to expressing correspondences among data sources and a unified schema. Combining mapping rules and functions represents a powerful formalism to specify pipelines for integrating data into a knowledge graph transparently. Surprisingly, these formalisms are not fully adapted, and many knowledge graphs are created by executing ad-hoc programs to pre-process and integrate data. In this paper, we present EABlock, an approach integrating Entity Alignment (EA) as part of RML mapping rules. EABlock includes a block of functions performing entity recognition from textual attributes and link the recognized entities to the corresponding resources in Wikidata, DBpedia, and domain specific thesaurus, e.g., UMLS. EABlock provides agnostic and efficient techniques to evaluate the functions and transfer the mappings to facilitate its application in any RML-compliant engine. We have empirically evaluated EABlock performance, and results indicate that EABlock speeds up knowledge graph creation pipelines that require entity recognition and linking in state-of-the-art RML-compliant engines. EABlock is also publicly available as a tool through a GitHub repository(this https URL) and a DOI(this https URL).",Covid-on-the-Web dataset,27,0,,,
Entity Matching,Mining RDF Data of COVID-19 Scientific Literature for Interesting Association Rules,"Lucie Cadorel, Andrea G. B. Tettamanzi","In the context of the global effort to study, understand, and fight the new Coronavirus, prompted by the
publication of a rich, reusable linked data containing named
entities mentioned in the COVID-19 Open Research Dataset,
a large corpus of scientific articles related to coronaviruses,
we propose a method to discover interesting association rules
from an RDF knowledge graph, by combining clustering,
community detection, and dimensionality reduction, as well as
criteria for filtering the discovered association rules in order to
keep only the most interesting rules. Our results demonstrate
the effectiveness and scalability of the proposed method and
suggest several possible uses of the discovered rules, including
(i) curating the knowledge graph by detecting errors, (ii)
finding relevant and coherent collections of scientific articles,
and (iii) suggesting novel hypotheses to biomedical researchers
for further investigation.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,ARViz: Interactive Visualization of Association Rules for RDF Data Exploration,"Aline Menin, Lucie Cadorel, Andrea Tettamanzi, Alain Giboin, Fabien Gandon, Marco Winckler","Association rule mining often leads the analyst into a rough rummaging process to identify rules that are relevant to understand specific problems. We propose a visualization interface to assist the rule selection process and evaluate it on an RDF knowledge graph derived from the COVID-19 Open Research Dataset. The user interface supports data exploration with focus on the overview of rules through a scatter plot, subsets of rules through a chord diagram chart, and itemsets through an association graph which is dynamically created by entering an item of interest (i.e. a named entity). Further, the analyst can interactively recover a list of publications containing the named entities involved in a particular rule. Among the original aspects of our approach, we highlight the representation of attributes describing measures of interest (i.e. confidence and interestingness), a visual indication of existence (or not) of symmetry in association rules, the exploration of subsets of rules according to clusters of publications and named entities, and an interactive prompting that aims at expanding the discovery of named entities within selected association rules. We assess our approach through a semi-structured interview involving experts in the domains of data mining and biomedicine, whose feedback could assist the refinement of the visual and interaction tools.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,Covid-on-the-Web: Exploring the COVID-19 scientific literature through visualization of linked data from entity and argument mining,"Aline Menin , Franck Michel , Fabien Gandon , Raphaël Gazzotti , Elena Cabrio , Olivier Corby , Alain Giboin , Santiago Marro , Tobias Mayer ,Serena Villata ,  Marco Winckler","The unprecedented mobilization of scientists caused by the COVID-19 pandemic has generated an enormous number of scholarly articles that are impossible for a human being to keep track of and explore without appropriate tool support. In this context, we created the Covid-on-the-Web project, which aims to assist the accessing, querying, and sense-making of COVID-19-related literature by combining efforts from the semantic web, natural language processing, and visualization fields. In particular, in this paper we present an RDF data set (a linked version of the “COVID-19 Open Research Dataset” (CORD-19), enriched via entity linking and argument mining) and the “Linked Data Visualizer” (LDViz), which assists the querying and visual exploration of the referred data set. The LDViz tool assists in the exploration of different views of the data by combining a querying management interface, which enables the definition of meaningful subsets of data through SPARQL queries, and a visualization interface based on a set of six visualization techniques integrated in a chained visualization concept, which also supports the tracking of provenance information. We demonstrate the potential of our approach to assist biomedical researchers in solving domain-related tasks, as well as to perform exploratory analyses through use case scenarios.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,An Analysis of COVID-19 Knowledge Graph Construction and Applications,"Dominic Flocco, Bryce Palmer-Toy, Ruixiao Wang, Hongyu Zhu, Rishi Sonthalia, Junyuan Lin, Andrea L. Bertozzi, P. Jeffrey Brantingham","The construction and application of knowledge graphs have seen a rapid increase across many disciplines in recent years. Additionally, the problem of uncovering relationships between developments in the COVID-19 pandemic and social media behavior is of great interest to researchers hoping to curb the spread of the disease. In this paper we present a knowledge graph constructed from COVID-19 related tweets in the Los Angeles area, supplemented with federal and state policy announcements and disease spread statistics. By incorporating dates, topics, and events as entities, we construct a knowledge graph that describes the connections between these useful information. We use natural language processing and change point analysis to extract tweet-topic, tweet-date, and event-date relations. Further analysis on the constructed knowledge graph provides insight into how tweets reflect public sentiments towards COVID-19 related topics and how changes in these sentiments correlate with real-world events.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,CKGSE: A Prototype Search Engine for Chinese Knowledge Graphs,"Xiaxia Wang, Tengteng Lin, Weiqing Luo, Gong Cheng, Yuzhong Qu","Nowadays, with increasing open knowledge graphs (KGs) being published on the Web, users depend on open data portals and search engines to find KGs. However, existing systems provide search services and present results with only metadata while ignoring the contents of KGs, i.e., triples. It brings difficulty for users' comprehension and relevance judgement. To overcome the limitation of metadata, in this paper we propose a content-based search engine for open KGs named CKGSE. Our system provides keyword search, KG snippet generation, KG profiling and browsing, all based on KGs' detailed, informative contents rather than their brief, limited metadata. To evaluate its usability, we implement a prototype with Chinese KGs crawled from OpenKG.CN and report some preliminary results and findings.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,An Overview of Methods and Tools for Extraction of Knowledge for COVID-19 from Knowledge Graphs,"Mariya Evtimova-Gardair, Nedra Mellouli ","The sanitary crisis provoked from the virus COVID-19 push researchers and practitioners to explore and find solutions to stamp the pandemic problem. Therefore many productions of various scientific papers and knowledge graphs are publicly accessible in internet. In this article is defined an overall description of the search engines available for COVID-19 information. A brief review of the knowledge graphs available for COVID-19 information is performed. This paper is an overview of the main relevant knowledge graph-based methods contributing in COVID-19 knowledge extraction and understanding. Furthermore, it is proposed a state-of-the-art of knowledge reasoning methods on COVID-19.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,Report on the 1st workshop on argumentation knowledge graphs (ArgKG 2021) at AKBC 2021,"Tirthankar Ghosal, Khalid Al-Khatib, Yufang Hou, Anita de Waard, Dayne Freitag","The first workshop on Argumentation Knowledge Graphs (ArgKG) was held virtually at the Automated Knowledge Base Construction (AKBC 2021) conference on October 7, 2021. ArgKG @ AKBC 2021 brought together the Computational Argumentation and Knowledge Graphs communities, aiming to promote cross-pollination of ideas and encourage discussions and collaborations between the two communities. This paper describes the workshop and compiles several of its findings and insights.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,COVID-Forecast-Graph: An Open Knowledge Graph for Consolidating COVID-19 Forecasts and Economic Indicators via Place and Time,"Rui Zhu, Krzysztof Janowicz, Gengchen Mai, Ling Cai, Meilin Shi","The longer the COVID-19 pandemic lasts, the more apparent it becomes that understanding its social drivers may be as important as understanding the virus itself. One such social driver is misinformation and distrust in institutions. This is particularly interesting as the scientific process is more transparent than ever before. Numerous scientific teams have published datasets that cover almost any imaginable aspects of COVID-19 during the last two years. However, consistently and efficiently integrating and making sense of these separate data “silos” to scientists, decision makers, journalists, and more importantly the general public remain a key challenge with important implications for transparency. Several types of knowledge graphs have been published to tackle this issue and to enable data crosswalks by providing rich contextual information. Interestingly, none of these graphs has focused on COVID-19 forecasts despite them acting as the underpinning for decision making. In this work we motivate the need for exposing forecasts as a knowledge graph, showcase queries that run against the graph, and geographically interlink forecasts with indicators of economic impacts.",Covid-on-the-Web dataset,27,0,,,
Entity Matching,Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Ralph Ewerth","The World Wide Web has become a popular source for gathering information and news. Multimodal information, e.g., enriching text with photos, is typically used to convey the news more effectively or to attract attention. Photo content can range from decorative, depict additional important information, or can even contain misleading information. Therefore, automatic approaches to quantify cross-modal consistency of entity representation can support human assessors to evaluate the overall multimodal message, for instance, with regard to bias or sentiment. In some cases such measures could give hints to detect fake news, which is an increasingly important topic in today's society. In this paper, we introduce a novel task of cross-modal consistency verification in real-world news and present a multimodal approach to quantify the entity coherence between image and text. Named entity linking is applied to extract persons, locations, and events from news texts. Several measures are suggested to calculate cross-modal similarity for these entities using state of the art approaches. In contrast to previous work, our system automatically gathers example data from the Web and is applicable to real-world news. Results on two novel datasets that cover different languages, topics, and domains demonstrate the feasibility of our approach. Datasets and code are publicly available to foster research towards this new direction.",TamperedNews Dataset,28,1,TamperedNews Dataset,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Ralph Ewerth","Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency
This repository contains the TamperedNews dataset introduced in the paper:

Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, and Ralph Ewerth. 2020. Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency. In Proceedings of the 2020 International Conference on Multimedia Retrieval (ICMR '20). Association for Computing Machinery, New York, NY, USA, 16–25. DOI: https://doi.org/10.1145/3372278.3390670

Content
tamperednews.tar.gz:
dataset.jsonl containing:
Web links to the news texts
Web links to the news image
Outputs of the named entity recognition and disambiguation (NERD) approach
Untampered and tampered entities
entity_type.jsonl file for each entity type containing the following information for each entity:
Wikidata ID
Wikidata label
Meta information used for tampering
Web links to all reference images crawled from Google, Bing, and Wikidata
splits for testing and validation
tamperednews_features.tar.gz:
Visual features of the news images for persons, locations, and scenes
Visual features of the reference images for persons, locations, and scenes
tamperednews_wordembeddings.tar.gz: Word embeddings of all nouns in the news texts
"
Entity Matching,A Survey on Multimodal Disinformation Detection,"Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri, Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz, Preslav Nakov","Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation -- (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.",TamperedNews Dataset,28,0,,,
Entity Matching,Improving Fake News Detection by Using an Entity-enhanced Framework to Fuse Diverse Multimodal Clues,"Peng Qi, Juan Cao, Xirong Li, Huan Liu, Qiang Sheng, Xiaoyue Mi, Qin He, Yongbiao Lv, Chenyang Guo, Yingchao Yu","Recently, fake news with text and images have achieved more effective diffusion than text-only fake news, raising a severe issue of multimodal fake news detection. Current studies on this issue have made significant contributions to developing multimodal models, but they are defective in modeling the multimodal content sufficiently. Most of them only preliminarily model the basic semantics of the images as a supplement to the text, which limits their performance on detection. In this paper, we find three valuable text-image correlations in multimodal fake news: entity inconsistency, mutual enhancement, and text complementation. To effectively capture these multimodal clues, we innovatively extract visual entities (such as celebrities and landmarks) to understand the news-related high-level semantics of images, and then model the multimodal entity inconsistency and mutual enhancement with the help of visual entities. Moreover, we extract the embedded text in images as the complementation of the original text. All things considered, we propose a novel entity-enhanced multimodal fusion framework, which simultaneously models three cross-modal correlations to detect diverse multimodal fake news. Extensive experiments demonstrate the superiority of our model compared to the state of the art.",TamperedNews Dataset,28,0,,,
Entity Matching,Ontology-Driven Event Type Classification in Images,"Eric Muller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin Mrutzek, Ralph Ewerth","Event classification can add valuable information for semantic search and the increasingly important topic of fact validation in news. So far, only few approaches address image classification for newsworthy event types such as natural disasters, sports events, or elections. Previous work distinguishes only between a limited number of event types and relies on rather small datasets for training. In this paper, we present a novel ontology-driven approach for the classification of event types in images. We leverage a large number of real-world news events to pursue two objectives: First, we create an ontology based on Wikidata comprising the majority of event types. Second, we introduce a novel large-scale dataset that was acquired through Web crawling. Several baselines are proposed including an ontology-driven learning approach that aims to exploit structured information of a knowledge graph to learn relevant event relations using deep neural networks. Experimental results on existing as well as novel benchmark datasets demonstrate the superiority of the proposed ontology-driven approach.",TamperedNews Dataset,28,0,,,
Entity Matching,Multimodal news analytics using measures of cross-modal entity and context consistency,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Sherzod Hakimov, Ralph Ewerth ","The World Wide Web has become a popular source to gather information and news. Multimodal information, e.g., supplement text with photographs, is typically used to convey the news more effectively or to attract attention. The photographs can be decorative, depict additional details, but might also contain misleading information. The quantification of the cross-modal consistency of entity representations can assist human assessors’ evaluation of the overall multimodal message. In some cases such measures might give hints to detect fake news, which is an increasingly important topic in today’s society. In this paper, we present a multimodal approach to quantify the entity coherence between image and text in real-world news. Named entity linking is applied to extract persons, locations, and events from news texts. Several measures are suggested to calculate the cross-modal similarity of the entities in text and photograph by exploiting state-of-the-art computer vision approaches. In contrast to previous work, our system automatically acquires example data from the Web and is applicable to real-world news. Moreover, an approach that quantifies contextual image-text relations is introduced. The feasibility is demonstrated on two datasets that cover different languages, topics, and domains.",TamperedNews Dataset,28,0,,,
Entity Matching,New preventive strategies for respiratory syncytial virus infection in children,"Rebecca Glowinski, Asuncion Mejias, Octavio Ramilo","Respiratory syncytial virus (RSV) infections result in significant morbidity and mortality for young children worldwide. The development of preventive strategies for RSV has faced different challenges, including the legacy of the first vaccine attempt, and an incomplete understanding of the host immune response to the virus. However, promising preventive strategies against RSV are in the pipeline and their development has advanced rapidly in the past decade due in part to our improved knowledge about the structural conformation of key RSV proteins. These strategies include monoclonal antibodies and different vaccines platforms directed towards the main target populations.
",TamperedNews Dataset,28,0,,,
Entity Matching,QuTI! Quantifying Text-Image Consistency in Multimodal Documents,"Matthias Springstein, Eric Müller-Budack, Ralph Ewerth","The World Wide Web and social media platforms have become popular sources for news and information. Typically, multimodal information, e.g., image and text is used to convey information more effectively and to attract attention. While in most cases image content is decorative or depicts additional information, it has also been leveraged to spread misinformation and rumors in recent years. In this paper, we present a Web-based demo application that automatically quantifies the cross-modal relations of entities (persons, locations, and events) in image and text. The applications are manifold. For example, the system can help users to explore multimodal articles more efficiently, or can assist human assessors and fact-checking efforts in the verification of the credibility of news stories, tweets, or other multimodal documents.",TamperedNews Dataset,28,0,,,
Entity Matching,"Survey and challenges of story generation models - A multimodal perspective with five steps: Data embedding, topic modeling, storyline generation, draft story generation, and story evaluation","Sang-Min Park, Young-Gab Kim","The story is the description of events in chronological order that have occurred between people. By delivering facts to the people reading the story, it enables them to feel emotions. Such a story is composed using the following method: each event is analyzed and a storyline is composed, which becomes a skeleton text by linking relationships between major events. As the content of users becomes more diverse, multimodal story composition has become more essential than unimodal text-based story composition. This paper discusses modality integration based on multimodal data types and type conversion for multimodal story composition. We propose a story-graph model to create a story based on the integrated analysis of various modal data. In terms of architecture, the proposed multimodal storytelling model consists of modal data and a topic modeling module that performs clustering based on cross-modal similarities and extracts a topic of clustered modalities. From the perspective of utilization, to visualize a story-graph, the proposed model summarizes nodes with a representative image. Furthermore, the latest techniques are discussed with respect to five main modules and twelve sub-modules for story composition. Lastly, problems that can become issues when composing multimodal story modules are explained.

",TamperedNews Dataset,28,0,,,
Entity Matching,MEG: Multi-Evidence GNN for Multimodal Semantic Forensics,"Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan","Fake news often involves semantic manipulations across modalities such as image, text, location etc and requires the development of multimodal semantic forensics for its detection. Recent research has centered the problem around images, calling it image repurposing -- where a digitally unmanipulated image is semantically misrepresented by means of its accompanying multimodal metadata such as captions, location, etc. The image and metadata together comprise a multimedia package. The problem setup requires algorithms to perform multimodal semantic forensics to authenticate a query multimedia package using a reference dataset of potentially related packages as evidences. Existing methods are limited to using a single evidence (retrieved package), which ignores potential performance improvement from the use of multiple evidences. In this work, we introduce a novel graph neural network based model for multimodal semantic forensics, which effectively utilizes multiple retrieved packages as evidences and is scalable with the number of evidences. We compare the scalability and performance of our model against existing methods. Experimental results show that the proposed model outperforms existing state-of-the-art algorithms with an error reduction of up to 25%.
",TamperedNews Dataset,28,0,,,
Entity Matching,"GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval","Golsa Tahmasebzadeh, Endri Kacupaj, Eric Müller-Budack, Sherzod Hakimov, Jens Lehmann, Ralph Ewerth","In the context of social media, geolocation inference on news or events has become a very important task. In this paper, we present the GeoWINE (Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective modular system for multimodal retrieval which expects only a single image as input. The GeoWINE system consists of five modules in order to retrieve related information from various sources. The first module is a state-of-the-art model for geolocation estimation of images. The second module performs a geospatial-based query for entity retrieval using the Wikidata knowledge graph. The third module exploits four different image embedding representations, which are used to retrieve most similar entities compared to the input image. The embeddings are derived from the tasks of geolocation estimation, place recognition, ImageNet-based image classification, and their combination. The last two modules perform news and event retrieval from EventRegistry and the Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for end-users and is insightful for experts for reconfiguration to individual setups. The GeoWINE achieves promising results in entity label prediction for images on Google Landmarks dataset. The demonstrator is publicly available at this http URL.
",TamperedNews Dataset,28,0,,,
Entity Matching,"Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation","Giscard Biamby, Grace Luo, Trevor Darrell, Anna Rohrbach","Detecting out-of-context media, such as ""mis-captioned"" images on Twitter, is a relevant problem, especially in domains of high public significance. In this work we aim to develop defenses against such misinformation for the topics of Climate Change, COVID-19, and Military Vehicles. We first present a large-scale multimodal dataset with over 884k tweets relevant to these topics. Next, we propose a detection method, based on the state-of-the-art CLIP model, that leverages automatically generated hard image-text mismatches. While this approach works well on our automatically constructed out-of-context tweets, we aim to validate its usefulness on data representative of the real world. Thus, we test it on a set of human-generated fakes created by mimicking in-the-wild misinformation. We achieve an 11% detection improvement in a high precision regime over a strong baseline. Finally, we share insights about our best model design and analyze the challenges of this emerging threat.
",TamperedNews Dataset,28,0,,,
Entity Matching,Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings,"Vasilis Efthymiou, Oktie Hassanzadeh, Mariano Rodriguez-Muro, Vassilis Christophides ","Web tables constitute valuable sources of information for
various applications, ranging from Web search to Knowledge Base (KB)
augmentation. An underlying common requirement is to annotate the
rows of Web tables with semantically rich descriptions of entities published in Web KBs. In this paper, we evaluate three unsupervised annotation methods: (a) a lookup-based method which relies on the minimal
entity context provided in Web tables to discover correspondences to
the KB, (b) a semantic embeddings method that exploits a vectorial
representation of the rich entity context in a KB to identify the most relevant subset of entities in the Web table, and (c) an ontology matching
method, which exploits schematic and instance information of entities
available both in a KB and a Web table. Our experimental evaluation
is conducted using two existing benchmark data sets in addition to a
new large-scale benchmark created using Wikipedia tables. Our results
show that: 1) our novel lookup-based method outperforms state-of-theart lookup-based methods, 2) the semantic embeddings method outperforms lookup-based methods in one benchmark data set, and 3) the lack
of a rich schema in Web tables can limit the ability of ontology matching
tools in performing high-quality table annotation. As a result, we propose a hybrid method that significantly outperforms individual methods
on all the benchmarks.",Annotated Web Tables,29,1,Annotated Web Tables,"Vasilis Efthymiou, Oktie Hassanzadeh, Mariano Rodriguez-Muro, Vassilis Christophides ","Data sets used for experimental evaluation in the related publication:Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity EmbeddingsInternational Semantic Web Conference (1) 2017: 260-277Vasilis Efthymiou Oktie Hassanzadeh Mariano Rodríguez-Muro Vassilis ChristophidesThe gold standard data sets are collections of web tables:T2D (v1) consists of a schema-level gold standard of 1,748 Web tables, manually annotated with class- and property-mappings, as well as an entity-level gold standard of 233 Web tables.Limaye consists of 400 manually annotated Web tables with entity-, class-, and property-level correspondences, where single cells (not rows) are mapped to entities. The corrected version of this gold standard is adapted to annotate rows with entities, from the annotations of the label column cells.WikipediaGS is an instance-level gold standard developed from 485K Wikipedia tables, in which links in the label column are used to infer the annotation of a row to a DBpedia entity. Note on license: please refer to the README.txt. Data is derived from Wikipedia and other sources may have different licenses.Wikipedia contents can be shared under the terms of Creative Commons Attribution-ShareAlike Licenseas outlined on Wikipedia: https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_contentThe correspondences of the T2D Gold standard is provided under the terms of the Apache license. The Web tables are provided according the same terms of use, disclaimer of warranties and limitation of liabilities that apply to the Common Crawl corpus. The DBpedia subset is licensed under the terms of the Creative Commons Attribution-ShareAlike License and the GNU Free Documentation License that applies to DBpedia.Limaye gold standard is downloaded from: http://websail-fe.cs.northwestern.edu/TabEL/ (download date: August 25, 2016). Please refer to the original website and the following paper for more details and citation information:G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating and Searching Web Tables Using Entities, Types and Relationships. PVLDB, 3(1):1338â€“1347, 2010.THIS DATA IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"
Entity Matching,ColNet: Embedding the Semantics of Web Tables for Column Type Prediction,"Jiaoyan Chen, Ernesto Jiménez-Ruiz, Ian Horrocks, Charles Sutton","Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.
",Annotated Web Tables,29,0,,,
Entity Matching,"Web Table Extraction, Retrieval, and Augmentation: A Survey ","Shuo Zhang, Krisztian Balog","Tables are powerful and popular tools for organizing and manipulating data. A vast number of tables can be found on the Web, which represent a valuable knowledge resource. The objective of this survey is to synthesize and present two decades of research on web tables. In particular, we organize existing literature into six main categories of information access tasks: table extraction, table interpretation, table search, question answering, knowledge base augmentation, and table augmentation. For each of these tasks, we identify and describe seminal approaches, present relevant resources, and point out interdependencies among the different tasks.

",Annotated Web Tables,29,0,,,
Entity Matching,SemTab 2019: Resources to Benchmark Tabular Data to Knowledge Graph Matching Systems,"Ernesto Jiménez-Ruiz, Oktie Hassanzadeh, Vasilis Efthymiou, Jiaoyan Chen, Kavitha Srinivas ","Tabular data to Knowledge Graph matching is the process of assigning semantic tags from knowledge graphs (e.g., Wikidata or DBpedia) to the elements of a table. This task is a challenging problem for various reasons, including the lack of metadata (e.g., table and column names), the noisiness, heterogeneity, incompleteness and ambiguity in the data. The results of this task provide significant insights about potentially highly valuable tabular data, as recent works have shown, enabling a new family of data analytics and data science applications. Despite significant amount of work on various flavors of this problem, there is a lack of a common framework to conduct a systematic evaluation of state-of-the-art systems. The creation of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab) aims at filling this gap. In this paper, we report about the datasets, infrastructure and lessons learned from the first edition of the SemTab challenge.

",Annotated Web Tables,29,0,,,
Entity Matching,An Overview of End-to-End Entity Resolution for Big Data,"Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George Papadakis, Kostas Stefanidis","One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.

",Annotated Web Tables,29,0,,,
Entity Matching,TURL: Table Understanding through Representation Learning,"Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu","Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning. Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.
",Annotated Web Tables,29,0,,,
Entity Matching,Learning Semantic Annotations for Tabular Data,"Jiaoyan Chen, Ernesto Jimenez-Ruiz, Ian Horrocks, Charles Sutton","The usefulness of tabular data such as web tables critically depends on understanding their semantics. This study focuses on column type prediction for tables without any meta data. Unlike traditional lexical matching-based methods, we propose a deep prediction model that can fully exploit a table's contextual semantics, including table locality features learned by a Hybrid Neural Network (HNN), and inter-column semantics features learned by a knowledge base (KB) lookup and query answering this http URL exhibits good performance not only on individual table sets, but also when transferring from one table set to another.",Annotated Web Tables,29,0,,,
Entity Matching,End-to-End Entity Resolution for Big Data: A Survey,"Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George Papadakis, Kostas Stefanidis","One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.
",Annotated Web Tables,29,0,,,
Entity Matching,Novel Entity Discovery from Web Tables,"Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda ","When working with any sort of knowledge base (KB) one has to make sure it is as complete and also as up-to-date as possible. Both tasks are non-trivial as they require recall-oriented efforts to determine which entities and relationships are missing from the KB. As such they require a significant amount of labor. Tables on the Web, on the other hand, are abundant and have the distinct potential to assist with these tasks. In particular, we can leverage the content in such tables to discover new entities, properties, and relationships. Because web tables typically only contain raw textual content we first need to determine which cells refer to which known entities---a task we dub table-to-KB matching. This first task aims to infer table semantics by linking table cells and heading columns to elements of a KB. Then second task builds upon these linked entities and properties to not only identify novel ones in the same table but also to bootstrap their type and additional relationships. We refer to this process as novel entity discovery and, to the best of our knowledge, it is the first endeavor on mining the unlinked cells in web tables. Our method identifies not only out-of-KB (``novel'') information but also novel aliases for in-KB (``known'') entities. When evaluated using three purpose-built test collections, we find that our proposed approaches obtain a marked improvement in terms of precision over our baselines whilst keeping recall stable.",Annotated Web Tables,29,0,,,
Entity Matching,Tough Tables: Carefully Evaluating Entity Linking for Tabular Data,"Vincenzo Cutrona, Federico Bianchi, Ernesto Jiménez-Ruiz, Matteo Palmonari ","Table annotation is a key task to improve querying the Web and support the Knowledge Graph population from legacy sources (tables). Last year, the SemTab challenge was introduced to unify different efforts to evaluate table annotation algorithms by providing a common interface and several general-purpose datasets as a ground truth. The SemTab dataset is useful to have a general understanding of how these algorithms work, and the organizers of the challenge included some artificial noise to the data to make the annotation trickier. However, it is hard to analyze specific aspects in an automatic way. For example, the ambiguity of names at the entity-level can largely affect the quality of the annotation. In this paper, we propose a novel dataset to complement the datasets proposed by SemTab. The dataset consists of a set of high-quality manually-curated tables with non-obviously linkable cells, i.e., where values are ambiguous names, typos, and misspelled entity names not appearing in the current version of the SemTab dataset. These challenges are particularly relevant for the ingestion of structured legacy sources into existing knowledge graphs. Evaluations run on this dataset show that ambiguity is a key problem for entity linking algorithms and encourage a promising direction for future work in the field.",Annotated Web Tables,29,0,,,
Entity Matching,On extracting data from tables that are encoded using HTML,"Juan C.Roldán, Patricia Jiménez, Rafael Corchuelo","Tables are a common means to display data in human-friendly formats. Many authors have worked on proposals to extract those data back since this has many interesting applications. In this article, we summarise and compare many of the proposals to extract data from tables that are encoded using HTML and have been published between 2000 and 2018. We first present a vocabulary that homogenises the terminology used in this field; next, we use it to summarise the proposals; finally, we compare them side by side. Our analysis highlights several challenges to which no proposal provides a conclusive solution and a few more that have not been addressed sufficiently; simply put, no proposal provides a complete solution to the problem, which seems to suggest that this research field shall keep active in the near future. We have also realised that there is no consensus regarding the datasets and the methods used to evaluate the proposals, which hampers comparing the experimental results.",Annotated Web Tables,29,0,,,
Entity Matching,VTKEL: a resource for visual-textual-knowledge entity linking,"Luciano Serafini, Marco Rospocher, Lamberto Ballan, Alessandro Sperduti","“A picture is worth a thousand words”, the adage reads.
However, pictures cannot replace words in terms of their ability to
efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed, reveal different and complementary
information that, if combined, result in more information than the
sum of that contained in the single media. The combination of visual and textual information can be obtained by linking the entities
mentioned in the text with those shown in the pictures. To further integrate this with agent background knowledge, an additional step is
necessary. That is, either finding the entities in the agent knowledge
base that correspond to those mentioned in the text or shown in the
picture or, extending the knowledge base with the newly discovered
entities. We call this complex task Visual-Textual-Knowledge Entity
Linking (VTKEL). In this paper, we precisely define the VTKEL
task and present two datasets composed of 1k and 30k pictures, annotated with visual and textual entities and linked to the YAGO ontology. Successively, we develop the first unsupervised algorithm for
the solution of VTKEL task. The evaluation of the algorithm shows
promising results on both 1k and 30k VTKEL datasets.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,1,VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,"Luciano Serafini, Marco Rospocher, Lamberto Ballan, Alessandro Sperduti","VTKL dataset, contains documents composed of pictures with five corresponding textual captions for each image. The VTKL dataset is obtained by extending the Flikr30k dataset, designed for visual-textual mention alignment, with links to YAGO ontolgy, one of the largest web knowledge base. These links are obtained automatically by processing each image caption with PIKES, an NLP tool for entity recognition and linking.
"
Entity Matching,Human-Centered Artificial Intelligence for Designing Accessible Cultural Heritage,"Galena Pisoni, Natalia Díaz-Rodríguez, Hannie Gijlers,Linda Tonolli","This paper reviews the literature concerning technology used for creating and delivering accessible museum and cultural heritage sites experiences. It highlights the importance of the delivery suited for everyone from different areas of expertise, namely interaction design, pedagogical and participatory design, and it presents how recent and future artificial intelligence (AI) developments can be used for this aim, i.e.,improving and widening online and in situ accessibility. From the literature review analysis, we articulate a conceptual framework that incorporates key elements that constitute museum and cultural heritage online experiences and how these elements are related to each other. Concrete opportunities for future directions empirical research for accessibility of cultural heritage contents are suggested and further discussed.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,On Visual-Textual-Knowledge Entity Linking,"Shahi Dost; Luciano Serafini, Marco Rospocher, Lamberto Ballan, Alessandro Sperduti",,VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,Jointly Linking Visual and Textual Entity Mentions with Background Knowledge,"Shahi Dost, Luciano Serafini, Marco Rospocher, Lamberto Ballan & Alessandro Sperduti ","“A picture is worth a thousand words”, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed, reveal different and complementary information that, if combined, result in more information than the sum of that contained in the single media. The combination of visual and textual information can be obtained through linking the entities mentioned in the text with those shown in the pictures. To further integrate this with agent background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this paper, after providing a precise definition of the VTKEL task, we present a dataset composed of about 30K commented pictures, annotated with visual and textual entities, and linked to the YAGO ontology. Successively, we develop a purely unsupervised algorithm for the solution of the VTKEL tasks. The evaluation on the VTKEL dataset shows promising results.
",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,VT-LINKER: Visual-Textual-Knowledge Entity Linker,"Shahi Dost, Luciano Serafini, Marco Rospocher, Lamberto Ballan, Alessandro Sperduti","“A picture is worth a thousand words”, the adage reads.
However, pictures cannot replace words in terms of their ability to
efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed, reveal different and complementary
information that, if combined, result in more information than the
sum of that contained in the single media. The combination of visual and textual information can be obtained by linking the entities
mentioned in the text with those shown in the pictures. To further integrate this with agent background knowledge, an additional step is
necessary. That is, either finding the entities in the agent knowledge
base that correspond to those mentioned in the text or shown in the
picture or, extending the knowledge base with the newly discovered
entities. We call this complex task Visual-Textual-Knowledge Entity
Linking (VTKEL). In this paper, we precisely define the VTKEL
task and present two datasets composed of 1k and 30k pictures, annotated with visual and textual entities and linked to the YAGO ontology. Successively, we develop the first unsupervised algorithm for
the solution of VTKEL task. The evaluation of the algorithm shows
promising results on both 1k and 30k VTKEL datasets.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,A better loss for visual-textual grounding,"Davide Rigoni, Luciano Serafini, Alessandro Sperduti","Given a textual phrase and an image, the visual grounding problem is the task of locating the content of the image referenced by the sentence. It is a challenging task that has several real-world applications in human-computer interaction, image-text reference resolution, and video-text reference resolution. In the last years, several works have addressed this problem by proposing more and more large and complex models that try to capture visual-textual dependencies better than before. These models are typically constituted by two main components that focus on how to learn useful multi-modal features for grounding and how to improve the predicted bounding box of the visual mention, respectively. Finding the right learning balance between these two sub-tasks is not easy, and the current models are not necessarily optimal with respect to this issue. In this work, we propose a loss function based on bounding boxes classes probabilities that: (i) improves the bounding boxes selection; (ii) improves the bounding boxes coordinates prediction. Our model, although using a simple multi-modal feature fusion component, is able to achieve a higher accuracy than state-of-the-art models on two widely adopted datasets, reaching a better learning balance between the two sub-tasks mentioned above.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,"Aligning and linking entity mentions in image, text, and knowledge base","Shahi Dostab, Luciano Serafinib, Marco Rospocherc, Lamberto Ballana, Alessandro Sperdutia","A picture is worth a thousand words, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed reveal different and complementary information that, if combined will result in more information than the sum of that contained in a single media. The combination of visual and textual information can be obtained by linking the entities mentioned in the text with those shown in the pictures. To further integrate this with the agent’s background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this article, after providing a precise definition of the VTKEL task, we present two datasets called VTKEL1k* and VTKEL30k. These datasets consisting of images and corresponding captions, in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology. The datasets can be used for training and evaluating algorithms of the VTKEL task. Successively, we introduce a baseline algorithm called VT-LinKEr (Visual-Textual-Knowledge Entity Linker) for the solution of the VTKEL task. We evaluate the performances of VT-LinKEr on both datasets. We then contribute a supervised algorithm called ViTKan (Visual-Textual-Knowledge Alignment Network). We trained the ViTKan algorithm using features data of the VTKEL1k* dataset. The experimental results on VTKEL1k* and VTKEL30k datasets show that ViTKan substantially outperforms the baseline algorithm.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,Extending ImageNet to Arabic using Arabic WordNet,Abdulkareem Alsudais,"This paper investigates the extension of ImageNet and its millions of English-labeled images to Arabic using Arabic WordNet. The primary finding is the identification of Arabic synsets for 1219 of the 21,841 synsets used in ImageNet, which represents 1.1 million images. By leveraging the parent-child structure of synsets in ImageNet, this dataset is extended to 10,462 synsets (and 7.1 million images) that have an Arabic label, which is either a match or a direct hypernym, and to 17,438 synsets (and 11 million images) when a hypernym of a hypernym is included. Samples evaluated suggest that generating Arabic labels for images in ImageNet using hypernyms does indeed produce meaningful results. The precision values for seven evaluated samples exceeded 90%. Moreover, when all the images in the samples were combined, the precision value equaled 93%. For the entire ImageNet, when all hypernyms for a node are considered, an Arabic synset is found for all but four synsets. This represents the major contribution of this work: a dataset of 14,195,756 images that have Arabic labels. The resulting dataset presents Arabic labels for 99.9% of the images in ImageNet.",VTKEL: A resource for Visual-Textual-Knowledge Entity Linking,30,0,,,
Entity Matching,Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Ralph Ewerth","The World Wide Web has become a popular source for gathering information and news. Multimodal information, e.g., enriching text with photos, is typically used to convey the news more effectively or to attract attention. Photo content can range from decorative, depict additional important information, or can even contain misleading information. Therefore, automatic approaches to quantify cross-modal consistency of entity representation can support human assessors to evaluate the overall multimodal message, for instance, with regard to bias or sentiment. In some cases such measures could give hints to detect fake news, which is an increasingly important topic in today's society. In this paper, we introduce a novel task of cross-modal consistency verification in real-world news and present a multimodal approach to quantify the entity coherence between image and text. Named entity linking is applied to extract persons, locations, and events from news texts. Several measures are suggested to calculate cross-modal similarity for these entities using state of the art approaches. In contrast to previous work, our system automatically gathers example data from the Web and is applicable to real-world news. Results on two novel datasets that cover different languages, topics, and domains demonstrate the feasibility of our approach. Datasets and code are publicly available to foster research towards this new direction.",News400 Dataset,31,1,News400 Dataset,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Ralph Ewerth","Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency
This repository contains the News400 dataset introduced in the paper:

Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, and Ralph Ewerth. 2020. Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency. In Proceedings of the 2020 International Conference on Multimedia Retrieval (ICMR '20). Association for Computing Machinery, New York, NY, USA, 16–25. DOI: https://doi.org/10.1145/3372278.3390670

Content
news400.tar.gz:
dataset.jsonl containing:
Web links to the news texts
Web links to the news image
Outputs of the named entity recognition and disambiguation (NERD) approach
Untampered and tampered entities
<entity>.jsonl file for each entity type containing the following information for each entity:
Wikidata ID
Wikidata label
Meta information used for tampering
Web links to all reference images crawled from Google, Bing, and Wikidata
splits for testing and validation
news400_features.tar.gz:
Visual features of the news images for persons, locations, and scenes
Visual features of the reference images for persons, locations, and scenes
news400_wordembeddings.tar.gz: Word embeddings of all nouns in the news texts"
Entity Matching,A Survey on Multimodal Disinformation Detection,"Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri, Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz, Preslav Nakov","Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation -- (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.
",News400 Dataset,31,0,,,
Entity Matching,Improving Fake News Detection by Using an Entity-enhanced Framework to Fuse Diverse Multimodal Clues,"Peng Qi, Juan Cao, Xirong Li, Huan Liu, Qiang Sheng, Xiaoyue Mi, Qin He, Yongbiao Lv, Chenyang Guo, Yingchao Yu","Recently, fake news with text and images have achieved more effective diffusion than text-only fake news, raising a severe issue of multimodal fake news detection. Current studies on this issue have made significant contributions to developing multimodal models, but they are defective in modeling the multimodal content sufficiently. Most of them only preliminarily model the basic semantics of the images as a supplement to the text, which limits their performance on detection. In this paper, we find three valuable text-image correlations in multimodal fake news: entity inconsistency, mutual enhancement, and text complementation. To effectively capture these multimodal clues, we innovatively extract visual entities (such as celebrities and landmarks) to understand the news-related high-level semantics of images, and then model the multimodal entity inconsistency and mutual enhancement with the help of visual entities. Moreover, we extract the embedded text in images as the complementation of the original text. All things considered, we propose a novel entity-enhanced multimodal fusion framework, which simultaneously models three cross-modal correlations to detect diverse multimodal fake news. Extensive experiments demonstrate the superiority of our model compared to the state of the art.",News400 Dataset,31,0,,,
Entity Matching,Ontology-Driven Event Type Classification in Images,"Eric Muller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin Mrutzek, Ralph Ewerth","Event classification can add valuable information for semantic search and the increasingly important topic of fact validation in news. So far, only few approaches address image classification for newsworthy event types such as natural disasters, sports events, or elections. Previous work distinguishes only between a limited number of event types and relies on rather small datasets for training. In this paper, we present a novel ontology-driven approach for the classification of event types in images. We leverage a large number of real-world news events to pursue two objectives: First, we create an ontology based on Wikidata comprising the majority of event types. Second, we introduce a novel large-scale dataset that was acquired through Web crawling. Several baselines are proposed including an ontology-driven learning approach that aims to exploit structured information of a knowledge graph to learn relevant event relations using deep neural networks. Experimental results on existing as well as novel benchmark datasets demonstrate the superiority of the proposed ontology-driven approach.",News400 Dataset,31,0,,,
Entity Matching,Multimodal news analytics using measures of cross-modal entity and context consistency,"Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, Sherzod Hakimov, Ralph Ewerth ","The World Wide Web has become a popular source to gather information and news. Multimodal information, e.g., supplement text with photographs, is typically used to convey the news more effectively or to attract attention. The photographs can be decorative, depict additional details, but might also contain misleading information. The quantification of the cross-modal consistency of entity representations can assist human assessors’ evaluation of the overall multimodal message. In some cases such measures might give hints to detect fake news, which is an increasingly important topic in today’s society. In this paper, we present a multimodal approach to quantify the entity coherence between image and text in real-world news. Named entity linking is applied to extract persons, locations, and events from news texts. Several measures are suggested to calculate the cross-modal similarity of the entities in text and photograph by exploiting state-of-the-art computer vision approaches. In contrast to previous work, our system automatically acquires example data from the Web and is applicable to real-world news. Moreover, an approach that quantifies contextual image-text relations is introduced. The feasibility is demonstrated on two datasets that cover different languages, topics, and domains.",News400 Dataset,31,0,,,
Entity Matching,New preventive strategies for respiratory syncytial virus infection in children,"Rebecca Glowinski, Asuncion Mejias, Octavio Ramilo","Respiratory syncytial virus (RSV) infections result in significant morbidity and mortality for young children worldwide. The development of preventive strategies for RSV has faced different challenges, including the legacy of the first vaccine attempt, and an incomplete understanding of the host immune response to the virus. However, promising preventive strategies against RSV are in the pipeline and their development has advanced rapidly in the past decade due in part to our improved knowledge about the structural conformation of key RSV proteins. These strategies include monoclonal antibodies and different vaccines platforms directed towards the main target populations.",News400 Dataset,31,0,,,
Entity Matching,QuTI! Quantifying Text-Image Consistency in Multimodal Documents,"Matthias Springstein, Eric Müller-Budack, Ralph Ewerth","The World Wide Web and social media platforms have become popular sources for news and information. Typically, multimodal information, e.g., image and text is used to convey information more effectively and to attract attention. While in most cases image content is decorative or depicts additional information, it has also been leveraged to spread misinformation and rumors in recent years. In this paper, we present a Web-based demo application that automatically quantifies the cross-modal relations of entities (persons, locations, and events) in image and text. The applications are manifold. For example, the system can help users to explore multimodal articles more efficiently, or can assist human assessors and fact-checking efforts in the verification of the credibility of news stories, tweets, or other multimodal documents.",News400 Dataset,31,0,,,
Entity Matching,"Survey and challenges of story generation models - A multimodal perspective with five steps: Data embedding, topic modeling, storyline generation, draft story generation, and story evaluation","Sang-Min Park, Young-Gab Kim","The story is the description of events in chronological order that have occurred between people. By delivering facts to the people reading the story, it enables them to feel emotions. Such a story is composed using the following method: each event is analyzed and a storyline is composed, which becomes a skeleton text by linking relationships between major events. As the content of users becomes more diverse, multimodal story composition has become more essential than unimodal text-based story composition. This paper discusses modality integration based on multimodal data types and type conversion for multimodal story composition. We propose a story-graph model to create a story based on the integrated analysis of various modal data. In terms of architecture, the proposed multimodal storytelling model consists of modal data and a topic modeling module that performs clustering based on cross-modal similarities and extracts a topic of clustered modalities. From the perspective of utilization, to visualize a story-graph, the proposed model summarizes nodes with a representative image. Furthermore, the latest techniques are discussed with respect to five main modules and twelve sub-modules for story composition. Lastly, problems that can become issues when composing multimodal story modules are explained.",News400 Dataset,31,0,,,
Entity Matching,MEG: Multi-Evidence GNN for Multimodal Semantic Forensics,"Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan","Fake news often involves semantic manipulations across modalities such as image, text, location etc and requires the development of multimodal semantic forensics for its detection. Recent research has centered the problem around images, calling it image repurposing -- where a digitally unmanipulated image is semantically misrepresented by means of its accompanying multimodal metadata such as captions, location, etc. The image and metadata together comprise a multimedia package. The problem setup requires algorithms to perform multimodal semantic forensics to authenticate a query multimedia package using a reference dataset of potentially related packages as evidences. Existing methods are limited to using a single evidence (retrieved package), which ignores potential performance improvement from the use of multiple evidences. In this work, we introduce a novel graph neural network based model for multimodal semantic forensics, which effectively utilizes multiple retrieved packages as evidences and is scalable with the number of evidences. We compare the scalability and performance of our model against existing methods. Experimental results show that the proposed model outperforms existing state-of-the-art algorithms with an error reduction of up to 25%.",News400 Dataset,31,0,,,
Entity Matching,"GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval","Golsa Tahmasebzadeh, Endri Kacupaj, Eric Müller-Budack, Sherzod Hakimov, Jens Lehmann, Ralph Ewerth","In the context of social media, geolocation inference on news or events has become a very important task. In this paper, we present the GeoWINE (Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective modular system for multimodal retrieval which expects only a single image as input. The GeoWINE system consists of five modules in order to retrieve related information from various sources. The first module is a state-of-the-art model for geolocation estimation of images. The second module performs a geospatial-based query for entity retrieval using the Wikidata knowledge graph. The third module exploits four different image embedding representations, which are used to retrieve most similar entities compared to the input image. The embeddings are derived from the tasks of geolocation estimation, place recognition, ImageNet-based image classification, and their combination. The last two modules perform news and event retrieval from EventRegistry and the Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for end-users and is insightful for experts for reconfiguration to individual setups. The GeoWINE achieves promising results in entity label prediction for images on Google Landmarks dataset. The demonstrator is publicly available at this http URL.",News400 Dataset,31,0,,,
Entity Matching,"Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation","Giscard Biamby, Grace Luo, Trevor Darrell, Anna Rohrbach","Detecting out-of-context media, such as ""mis-captioned"" images on Twitter, is a relevant problem, especially in domains of high public significance. In this work we aim to develop defenses against such misinformation for the topics of Climate Change, COVID-19, and Military Vehicles. We first present a large-scale multimodal dataset with over 884k tweets relevant to these topics. Next, we propose a detection method, based on the state-of-the-art CLIP model, that leverages automatically generated hard image-text mismatches. While this approach works well on our automatically constructed out-of-context tweets, we aim to validate its usefulness on data representative of the real world. Thus, we test it on a set of human-generated fakes created by mimicking in-the-wild misinformation. We achieve an 11% detection improvement in a high precision regime over a strong baseline. Finally, we share insights about our best model design and analyze the challenges of this emerging threat.",News400 Dataset,31,0,,,
Entity Matching,Anatomical Entity Recognition with a Hierarchical Framework Augmented by External Resources,"Yan Xu,Ji Hua, Zhaoheng Ni, Qinlang Chen,Yubo Fan, Sophia Ananiadou,Eric I-Chao Chang, Junichi Tsujii",Performance of individual feature added to baseline using gold standard medical concepts.,Performance of individual feature added to baseline using gold standard medical concepts.,32,1,Performance of individual feature added to baseline using gold standard medical concepts.,"Yan Xu,Ji Hua, Zhaoheng Ni, Qinlang Chen,Yubo Fan, Sophia Ananiadou,Eric I-Chao Chang, Junichi Tsujii",Performance of individual feature added to baseline using gold standard medical concepts.
Entity Matching,Clinical concept extraction: A methodology review,"Sunyang Fu, David Chen, Huan He, Sijia Liu, Sungrim Moon, Kevin  J.Peterson, Feichen Shen,  Liwei Wang, Yanshan Wang, Andrew Wen, Yiqing Zhao, Sunghwan Sohn, Hongfang Liu","Background: Concept extraction, a subdomain of natural language processing (NLP) with a focus on extracting
concepts of interest, has been adopted to computationally extract clinical information from text for a wide range
of applications ranging from clinical decision support to care quality improvement.
Objectives: In this literature review, we provide a methodology review of clinical concept extraction, aiming to
catalog development processes, available methods and tools, and specific considerations when developing
clinical concept extraction applications.
Methods: Based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guide-
lines, a literature search was conducted for retrieving EHR-based information extraction articles written in En-
glish and published from January 2009 through June 2019 from Ovid MEDLINE In-Process & Other Non-Indexed
Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and the ACM Digital Library.
Results: A total of 6,686 publications were retrieved. After title and abstract screening, 228 publications were
selected. The methods used for developing clinical concept extraction applications were discussed in this review.",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,EXTRACT: interactive extraction of environment metadata and term suggestion for metagenomic sample annotation ,"Evangelos Pafilis, Pier Luigi Buttigieg, Barbra Ferrell, Emiliano Pereira, Julia Schnetzer, Christos Arvanitidis, Lars Juhl Jensen","The microbial and molecular ecology research communities have made substantial progress on developing standards for annotating samples with environment metadata. However, sample manual annotation is a highly labor intensive process and requires familiarity with the terminologies used. We have therefore developed an interactive annotation tool, EXTRACT, which helps curators identify and extract standard-compliant terms for annotation of metagenomic records and other samples. Behind its web-based user interface, the system combines published methods for named entity recognition of environment, organism, tissue and disease terms. The evaluators in the BioCreative V Interactive Annotation Task found the system to be intuitive, useful, well documented and sufficiently accurate to be helpful in spotting relevant text passages and extracting organism and environment terms. Comparison of fully manual and text-mining-assisted curation revealed that EXTRACT speeds up annotation by 15–25% and helps curators to detect terms that would otherwise have been missed.",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Essential Elements of Natural Language Processing: What the Radiologist Should Know,"Po-HaoChenMD, MBA","Natural language is ubiquitous in the workflow of medical imaging. Radiologists create and consume free text in their daily work, some of which can be amenable to enhancements through automatic processing. Recent advancements in deep learning and “artificial intelligence” have had a significant positive impact on natural language processing (NLP). This article discusses the history of how researchers have extracted data and encoded natural language information for analytical processing, starting from NLP's humble origins in hand-curated, linguistic rules. The evolution of medical NLP including vectorization, word embedding, classification, as well as its use in automated speech recognition, are also explored. Finally, the article will discuss the role of machine learning and neural networks in the context of significant, if incremental, improvements in NLP.",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Clinical named entity recognition: Challenges and opportunities,"Srinivasa Rao Kundeti,J Vijayananda, Srikanth Mujjiga, M Kalyan","Information Extraction (IE), one of the important tasks in text analysis and Natural Language Processing (NLP), involves extracting meaningful pieces of knowledge from unstructured information sources, as unstructured data is computationally opaque. The intent of IE is to produce a knowledge base i.e. organize the information in a way that it is useful to people and arrange the information in a semantic way so that algorithms can make certain useful inferences from it. Named Entity Recognition (NER) is a sub-task of IE which finds and classifies the names/entities. Once these Named Entities (NE) are extracted, they can then be indexed and made searchable, relations can be derived, questions can be answered and many more. NER techniques are different for different domains, because of the uniqueness that exists in each of the domains, although the process depends on a number of fundamental Natural Language Processing (NLP) steps such as tokenization, part-of-speech tagging, parsing and model building. As an example, NER in the medical domain involves handling of a number of vital tasks such as identification of medical terms, attributes such as negation, severity, identification of relationships between entities and mapping terms in the document to concepts in domain specific ontologies. There is also a heavy dependence on domain specific resources such as medical dictionaries and ontologies such as the Unified Medical Language System (UMLS)[34]. In this paper, we focus on NER in the clinical domain. In particular, we will focus on the NER challenges and the qualitative analysis of clinical reports on the approaches we took for the named entities: anatomies, findings, location qualifier, and procedures.
",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Category Multi-representation: A Unified Solution for Named Entity Recognition in Clinical Texts,"Jiangtao Zhang, Juanzi Li, Shuai Wang, Yan Zhang, Yixin Cao, Lei Hou, Xiao-Li Li ","Clinical Named Entity Recognition (CNER), the task of identifying the entity boundaries in clinical texts, is essential for many applications. Previous methods usually follow the traditional NER methods that heavily rely on language specific features (i.e. linguistics and lexicons) and high quality annotated data. However, due to the problem of Limited Availability of Annotated Data and Informal Clinical Texts, CNER becomes more challenging. In this paper, we propose a novel method that learn multiple representations for each category, namely category-multi-representation (CMR) that captures the semantic relatedness between words and clinical categories from different perspectives. CMR is learned based on a large scale unannotated corpus and a small set of annotated data, which greatly alleviates the burden of human effort. Instead of the language specific features, our proposed method uses more evidential features without any additional NLP tools, and enjoys a lightweight adaption among languages. We conduct a series of experiments to verify our new CMR features can further improve the performance of NER significantly without leveraging any external lexicons.

",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Gold-standard ontology-based anatomical annotation in the CRAFT Corpus ,"Michael Bada, Nicole Vasilevsky, William A Baumgartner, Jr, Melissa Haendel, Lawrence E Hunter ","Gold-standard annotated corpora have become important resources for the training and testing of natural-language-processing (NLP) systems designed to support biocuration efforts, and ontologies are increasingly used to facilitate curational consistency and semantic integration across disparate resources. Bringing together the respective power of these, the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of full-length, open-access biomedical journal articles with extensive manually created syntactic, formatting and semantic markup, was previously created and released. This initial public release has already been used in multiple projects to drive development of systems focused on a variety of biocuration, search, visualization, and semantic and syntactic NLP tasks. Building on its demonstrated utility, we have expanded the CRAFT Corpus with a large set of manually created semantic annotations relying on Uberon, an ontology representing anatomical entities and life-cycle stages of multicellular organisms across species as well as types of multicellular organisms defined in terms of life-cycle stage and sexual characteristics. This newly created set of annotations, which has been added for v2.1 of the corpus, is by far the largest publicly available collection of gold-standard anatomical markup and is the first large-scale effort at manual markup of biomedical text relying on the entirety of an anatomical terminology, as opposed to annotation with a small number of high-level anatomical categories, as performed in previous corpora. In addition to presenting and discussing this newly available resource, we apply it to provide a performance baseline for the automatic annotation of anatomical concepts in biomedical text using a prominent concept recognition system. The full corpus, released with a CC BY 3.0 license, may be downloaded from http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml.

",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Mapping anatomical related entities to human body parts based on wikipedia in discharge summaries,"Yipei Wang, Xingyu Fan, Luoxin Chen, Eric I-Chao Chang, Sophia Ananiadou, Junichi Tsujii, Yan Xu ","Background Consisting of dictated free-text documents such as discharge summaries, medical narratives are widely
used in medical natural language processing. Relationships between anatomical entities and human body parts are
crucial for building medical text mining applications. To achieve this, we establish a mapping system consisting of a
Wikipedia-based scoring algorithm and a named entity normalization method (NEN). The mapping system makes full
use of information available on Wikipedia, which is a comprehensive Internet medical knowledge base. We also built a
new ontology, Tree of Human Body Parts (THBP), from core anatomical parts by referring to anatomical experts and
Unified Medical Language Systems (UMLS) to make the mapping system efficacious for clinical treatments.
Result The gold standard is derived from 50 discharge summaries from our previous work, in which 2,224 anatomical
entities are included. The F1-measure of the baseline system is 70.20%, while our algorithm based on Wikipedia
achieves 86.67% with the assistance of NEN.
Conclusions We construct a framework to map anatomical entities to THBP ontology using normalization and a
scoring algorithm based on Wikipedia. The proposed framework is proven to be much more effective and efficient
than the main baseline system.",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Information extraction from clinical and radiology notes for liver cancer staging,"Yim, Wen-wai","Medical practice involves an astonishing amount of variation across individual clinicians, departments, and institutions. Adding to this condition, with the exponential pace of new discoveries in biomedical research, medical professionals, often understaffed and overworked, have little time and resources to analyze or incorporate the latest research into clinical practice. The accelerated adoption of electronic medical records (EMRs) brings about great opportunities to mitigate these issues. In computable form, large volumes of medical information can now be stored and queried, so that optimization of treatments based on patient characteristics, institutional resources, and patient preferences may be data driven. Thus, instead of relying on the skillsets of patients' support network and medical teams, patient outcomes can at least have some statistical guarantees. In this dissertation, we focused specifically on the task of hepatocellular carcinoma (HCC) liver cancer staging using natural language processing (NLP) techniques. Staging, or categorizing cancer patients by extent of diseases, is important for normalizing over patient characteristics. Normalized stages, can then be used to facilitate research in evidence-based medicine to optimize for treatments and outcomes. NLP is necessary, as with other clinical tasks, a majority of staging information is trapped in free text clinical data. This thesis proposes an approach to liver cancer stage phenotype classification using a mixture of rule-based and machine learning techniques for text extraction. Included in this approach is a careful, layered design for annotation and classification. Each constituent part of our system was characterized by detailed quantitative and qualitative analysis. Two important modules in this thesis are a framework for normalizing text evidence related to specific conditions and an algorithm for tumor reference resolution. The overall results of our system revealed an F1 performance of 0.55, 0.50, 0.43 for AJCC, BCLC, and CLIP liver cancer stages, respectively. Although outperforming baseline classifications, these accuracies are not viable for clinical use. Analysis of error suggests that performance for some constituent stage parameters would improve through additional annotation. However, one identified crippling bottleneck was the requirement of reference resolution and discourse-level reasoning to determine the number of tumors in a patient, a crucial part of cancer staging. Still our work provides a methodology to classify a complex phenotype, whose strength includes its interpretability and modularity while maintaining ability to scale and improve with greater amounts of data. Furthermore, submodules of our system, for which perform at higher accuracies, may be used as tools to decrease annotation costs.
",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Medical Imaging Report Indexing: Enrichment of Index through an Algorithm of Spreading over a Lexico-semantic Network,"Mathieu Lafourcade, Lionel Ramadier","In medical imaging domain, digitized data is rapidly expanding Therefore it is of
major interest for radiologists to be able
to do an efficient and accurate extraction
of imaging and clinical data (radiology
reports) which are essential for a rigorous
diagnosis and for a better management of
patients. In daily practice, radiology reports are written using a nonstandardized language which is often
ambiguous and noisy. The queries of radiological images can be greatly facilitated through textual indexing of associated
reports. In order to improve the quality
of the analysis of such reports, it is desirable to specify an index enlargement algorithm based on spreading activations
over a general lexical-semantic network.
In this paper, we present such an algorithm along with its qualitative evaluation.
",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,Chinese Clinical Named Entity Recognition with ALBERT and MHA Mechanism,"Dongmei Li, Jiao Long , Jintao Qu,  Xiaoping Zhang 
","Traditional clinical named entity recognition methods fail to balance the effectiveness of feature extraction of unstructured text and the complexity of neural network models. We propose a model based on ALBERT and a multihead attention (MHA) mechanism to solve this problem. Structurally, the model first obtains character-level word embeddings through the ALBERT pretraining language model, then inputs the word embeddings into the iterated dilated convolutional neural network model to quickly extract global semantic information, and decodes the predicted labels through conditional random fields to obtain the optimal label sequence. Also, we apply the MHA mechanism to capture intercharacter dependencies from multiple aspects. Furthermore, we use the RAdam optimizer to boost the convergence speed and improve the generalization ability of our model. Experimental results show that our model achieves an F1 score of 85.63% on the CCKS-2019 dataset—an increase of 4.36% compared to the baseline model.

",Performance of individual feature added to baseline using gold standard medical concepts.,32,0,,,
Entity Matching,A transition-based neural framework for Chinese information extraction,"Wenzhi Huang,Junchi Zhang ,Donghong Ji ","Chinese information extraction is traditionally performed in the process of word segmentation, entity recognition, relation extraction and event detection. This pipelined approach suffers from two limitations: 1) It is prone to introduce propagated errors from upstream tasks to subsequent applications; 2) Mutual benefits of cross-task dependencies are hard to be introduced in non-overlapping models. To address these two challenges, we propose a novel transition-based model that jointly performs entity recognition, relation extraction and event detection as a single task. In addition, we incorporate subword-level information into character sequence with the use of a hybrid lattice structure, removing the reliance of external word tokenizers. Results on standard ACE benchmarks show the benefits of the proposed joint model and lattice network, which gives the best result in the literature.

","Joint model and pipeline model results on the ACE2005 test set.",33,1,"Joint model and pipeline model results on the ACE2005 test set.
","Wenzhi Huang,Junchi Zhang ,Donghong Ji ","Joint model and pipeline model results on the ACE2005 test set.
"
Entity Matching,Correction: A transition-based neural framework for Chinese information extraction,"Wenzhi Huang, Junchi Zhang, Donghong Ji",Correction: A transition-based neural framework for Chinese information extraction,"Joint model and pipeline model results on the ACE2005 test set.",33,0,,,
Entity Matching,Artificial Intelligence for Screening Chinese Electronic Medical Record and Biobank Information,"Xiaoqing Li, Jiang Han, Shaodian Zhang, Ken Chen, Liebin Zhao, Yi He, Shijian Liu","Objective: To establish a structured and integrated platform of clinical data and biobank data, and a client to retrieve these data.

Study Design: Initially, the hospital information system (HIS) and biobank information system (BIS) were integrated through the patients' ID numbers. Then, natural language processing (NLP) was used to process the integrated unstructured clinical information. A query interface was designed for this system, which enabled researchers to retrieve clinical or biobank data. Finally, several queries were listed and manually checked to test the retrieval performance of the system.

Results: The construction of the biobank screening system (BSS) was completed, and the data were structured. The BSS took an average of 2 seconds to perform a search for target patients/samples. The retrieval results were consistent with the HIS and BIS. For complex queries, we manually checked the retrieved patients/samples, and the system's accuracy was 100%.

Conclusion: This NLP-based system improved biological sample screening and using of clinical data. We will continue to improve this system, enhance resource sharing, and promote the development of translational medicine.","Joint model and pipeline model results on the ACE2005 test set.",33,0,,,
Entity Matching,BERT-based Natural Language Processing of Drug Labeling Documents: A Case Study for Classifying Drug-Induced Liver Injury Risk,"Yue Wu, Zhichao Liu, Leihong Wu, Minjun Chen, Weida Tong","Background & Aims: The United States Food and Drug Administration (FDA) regulates a broad range of consumer products, which account for about 25% of the United States market. The FDA regulatory activities often involve producing and reading of a large number of documents, which is time consuming and labor intensive. To support regulatory science at FDA, we evaluated artificial intelligence (AI)-based natural language processing (NLP) of regulatory documents for text classification and compared deep learning-based models with a conventional keywords-based model.

Methods: FDA drug labeling documents were used as a representative regulatory data source to classify drug-induced liver injury (DILI) risk by employing the state-of-the-art language model BERT. The resulting NLP-DILI classification model was statistically validated with both internal and external validation procedures and applied to the labeling data from the European Medicines Agency (EMA) for cross-agency application.

Results: The NLP-DILI model developed using FDA labeling documents and evaluated by cross-validations in this study showed remarkable performance in DILI classification with a recall of 1 and a precision of 0.78. When cross-agency data were used to validate the model, the performance remained comparable, demonstrating that the model was portable across agencies. Results also suggested that the model was able to capture the semantic meanings of sentences in drug labeling.

Conclusion: Deep learning-based NLP models performed well in DILI classification of drug labeling documents and learned the meanings of complex text in drug labeling. This proof-of-concept work demonstrated that using AI technologies to assist regulatory activities is a promising approach to modernize and advance regulatory science.",Table1_BERT-Based Natural Language Processing of Drug Labeling Documents: A Case Study for Classifying Drug-Induced Liver Injury Risk.DOCX,34,1,Table1_BERT-Based Natural Language Processing of Drug Labeling Documents: A Case Study for Classifying Drug-Induced Liver Injury Risk.DOCX,"Yue Wu, Zhichao Liu, Leihong Wu, Minjun Chen, Weida Tong","Background & Aims: The United States Food and Drug Administration (FDA) regulates a broad range of consumer products, which account for about 25% of the United States market. The FDA regulatory activities often involve producing and reading of a large number of documents, which is time consuming and labor intensive. To support regulatory science at FDA, we evaluated artificial intelligence (AI)-based natural language processing (NLP) of regulatory documents for text classification and compared deep learning-based models with a conventional keywords-based model.Methods: FDA drug labeling documents were used as a representative regulatory data source to classify drug-induced liver injury (DILI) risk by employing the state-of-the-art language model BERT. The resulting NLP-DILI classification model was statistically validated with both internal and external validation procedures and applied to the labeling data from the European Medicines Agency (EMA) for cross-agency application.Results: The NLP-DILI model developed using FDA labeling documents and evaluated by cross-validations in this study showed remarkable performance in DILI classification with a recall of 1 and a precision of 0.78. When cross-agency data were used to validate the model, the performance remained comparable, demonstrating that the model was portable across agencies. Results also suggested that the model was able to capture the semantic meanings of sentences in drug labeling.Conclusion: Deep learning-based NLP models performed well in DILI classification of drug labeling documents and learned the meanings of complex text in drug labeling. This proof-of-concept work demonstrated that using AI technologies to assist regulatory activities is a promising approach to modernize and advance regulatory science."
Entity Matching,An Invitation to Greater Use of Matthews Correlation Coefficient in Robotics and Artificial Intelligence,"Davide Chicco, Giuseppe Jurman","A binary classification is a computational procedure that labels data elements as members of one or another category. In machine learning and computational statistics, input data elements which are part of two classes are usually encoded as 0’s or –1’s (negatives) and 1’s (positives). During a binary classification, a method assigns each data element to one of the two categories, usually after a machine learning phase. ",Table1_BERT-Based Natural Language Processing of Drug Labeling Documents: A Case Study for Classifying Drug-Induced Liver Injury Risk.DOCX,34,0,,,
Entity Matching,"Grid Content: Services, Tools and Components","Ian Roberts , Andres Garcia Silva , Miroslav Janosik ,
Andis Lagzdiņš , Nils Feldhus, Georg Rehm ,
Dimitris Galanis , Dusan Varis , Ulrich Germann","This document details the process carried out within the ELG project to identify the tools, services and components available among the ELG project partners, and prioritise which tools to integrate in the ELG platform at
which of the three principal release stages (M14, M22, M34). For those tools identified as priorities for integration in the first release, this document also gives details of the approach taken and the work done to integrate
each tool. We also include a summary of the API with which tools must comply to be runnable within the ELG
platform.",Lynx LER,35,1,Lynx LER,"Elena Leitner, Georg Rehm, Julian Moreno-Schneider","The service for Legal named entity recognition (LER) includes the elaboration of corresponding semantic classes and the preparation of a German language data set. Several state of the art models were trained, i.e., Conditional Random Fields (CRFs) and bidirectional Long-Short Term Memory Networks (BiLSTMs). For training and evaluating the system we used a data set of German court decisions that was manually annotated with seven coarse-grained and 19 fine-grained classes: names and citations of people (person, judge, lawyer), location (country, city, street, area), organisation (organisation, company, institution, court, brand), legal norm (law, legal regulation, European legal norm), case-by-case regulation (regulation, contract), case law, and legal literature. The data set consists of approximately 67,000 sentences and around 54,000 annotated entities. For the experiment, two tools for sequence labeling were chosen. These are sklearn-crfsuite (CRFs) ( https://sklearn-crfsuite.readthedocs.io/en/latest/) and UKPLab-BiLSTM (BiLSTMs) (https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf).
"
Entity Matching,"Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm","Martin Pérez-Pérez, Gael Pérez-Rodríguez, Aitor Blanco-Míguez, Florentino Fdez-Riverola, Alfonso Valencia, Martin Krallinger, Anália Lourenço ","Shared tasks and community challenges represent key instruments to promote research, collaboration and determine the state of the art of biomedical and chemical text mining technologies. Traditionally, such tasks relied on the comparison of automatically generated results against a so-called Gold Standard dataset of manually labelled textual data, regardless of efficiency and robustness of the underlying implementations. Due to the rapid growth of unstructured data collections, including patent databases and particularly the scientific literature, there is a pressing need to generate, assess and expose robust big data text mining solutions to semantically enrich documents in real time. To address this pressing need, a novel track called “Technical interoperability and performance of annotation servers” was launched under the umbrella of the BioCreative text mining evaluation effort. The aim of this track was to enable the continuous assessment of technical aspects of text annotation web servers, specifically of online biomedical named entity recognition systems of interest for medicinal chemistry applications.

","MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm",36,1,"MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm
","Martin Pérez-Pérez, Gael Pérez-Rodríguez, Aitor Blanco-Míguez, Florentino Fdez-Riverola, Alfonso Valencia, Martin Krallinger, Anália Lourenço ","Additional file 1. Description of the structure and the restrictions of the supported formats."
Entity Matching,"Named Entity Recognition, Concept Normalization and
Clinical Coding: Overview of the Cantemist Track for
Cancer Text Mining in Spanish, Corpus, Guidelines,
Methods and Results","Antonio Miranda-Escalada, Eulàlia Farré, Martin Krallinger","Cancer still represents one of the leading causes of death worldwide, resulting in a considerable healthcare impact. Recent research efforts from the clinical and molecular oncology scientific communities were able to increase considerably life expectancy of patients for some cancer types. Most of the current cancer diagnoses are primarily determined by pathology laboratories, providing an essential source for information to guide the treatment of patients with cancer. Pathology observations essentially characterize the results of microscopic or macroscopic studies of cells or tissues following a biopsy or surgery. Clinicians and researchers alike, require systems that automatically detect, read and generate structured data representations from pathology examinations. The resulting structured or coded clinical information, normalized using controlled vocabularies like the ICD-O or SNOMED-CT is critical for large-scale analysis of specific tumor types or to determine response to specific treatments or prognosis. Text mining and NLP approaches are showing promising results to transform medical text into useful clinical information, bridging the gap between free-text and structured representation of clinical information. Nonetheless, in the case of cancer text mining approaches, most efforts were exclusively focused on medical records in English. Moreover, due to the lack of high quality manually labeled clinical texts annotated by oncology experts most previous efforts, even for English relied mainly on customized dictionaries of names or rules to recognize clinical concept mentions despite the promising results of advanced deep learning technologies. To address these issues we have organized the Cantemist (CANcer TExt Mining Shared Task) track at IberLEF 2020. It represents the first community effort to evaluate and promote the development of resources for named entity recognition, concept normalization and clinical coding specifically focusing on cancer data in Spanish. Evaluation of participating systems was done using the Cantemist corpus, a publicly accessible dataset (together with annotation consistency analysis and guidelines) of manually annotated mentions of tumor morphology entities and their mappings to the Spanish version of ICD-O. We received a total of 121 systems or runs from 25 teams for one of the three Cantemist sub-tasks, obtaining very competitive results. Most participants implemented sophisticated AI approaches; mainly deep learning algorithms based on LongShort Term Memory Units and language models (BERT, BETO, RoBERTa, etc) with a classifier layer such as a Conditional Random Field. In addition to using pre-trained language models, word and character embeddings were also explored. Cantemist corpus: https://doi.org/10.5281/zenodo.3773228","MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm",36,0,,,
Entity Matching,MER: a shell script and annotation server for minimal named entity recognition and linking,"Francisco M. Couto, Andre Lamurias ","Named-entity recognition aims at identifying the fragments of text that mention entities of interest, that afterwards could be linked to a knowledge base where those entities are described. This manuscript presents our minimal named-entity recognition and linking tool (MER), designed with flexibility, autonomy and efficiency in mind. To annotate a given text, MER only requires: (1) a lexicon (text file) with the list of terms representing the entities of interest; (2) optionally a tab-separated values file with a link for each term; (3) and a Unix shell. Alternatively, the user can provide an ontology from where MER will automatically generate the lexicon and links files. The efficiency of MER derives from exploring the high performance and reliability of the text processing command-line tools grep and awk, and a novel inverted recognition technique. MER was deployed in a cloud infrastructure using multiple Virtual Machines to work as an annotation server and participate in the Technical Interoperability and Performance of annotation Servers task of BioCreative V.5. The results show that our solution processed each document (text retrieval and annotation) in less than 3 s on average without using any type of cache. MER was also compared to a state-of-the-art dictionary lookup solution obtaining competitive results not only in computational performance but also in precision and recall. MER is publicly available in a GitHub repository (https://github.com/lasigeBioTM/MER) and through a RESTful Web service (http://labs.fc.ul.pt/mer/).

","MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm
",36,0,,,
Entity Matching,Statistical principle-based approach for gene and protein related object recognition,"Po-Ting Lai, Ming-Siang Huang, Ting-Hao Yang, Wen-Lian Hsu, Richard Tzong-Han Tsai ","The large number of chemical and pharmaceutical patents has attracted researchers doing biomedical text mining to extract valuable information such as chemicals, genes and gene products. To facilitate gene and gene product annotations in patents, BioCreative V.5 organized a gene- and protein-related object (GPRO) recognition task, in which participants were assigned to identify GPRO mentions and determine whether they could be linked to their unique biological database records. In this paper, we describe the system constructed for this task. Our system is based on two different NER approaches: the statistical-principle-based approach (SPBA) and conditional random fields (CRF). Therefore, we call our system SPBA-CRF. SPBA is an interpretable machine-learning framework for gene mention recognition. The predictions of SPBA are used as features for our CRF-based GPRO recognizer. The recognizer was developed for identifying chemical mentions in patents, and we adapted it for GPRO recognition. In the BioCreative V.5 GPRO recognition task, SPBA-CRF obtained an F-score of 73.73% on the evaluation metric of GPRO type 1 and an F-score of 78.66% on the evaluation metric of combining GPRO types 1 and 2. Our results show that SPBA trained on an external NER dataset can perform reasonably well on the partial match evaluation metric. Furthermore, SPBA can significantly improve performance of the CRF-based recognizer trained on the GPRO dataset.

","MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm",36,0,,,
Entity Matching,A neural network approach to chemical and gene/protein entity recognition in patents,"Ling Luo, Zhihao Yang, Pei Yang, Yin Zhang, Lei Wang, Jian Wang,Hongfei Lin ","In biomedical research, patents contain the significant amount of information, and biomedical text mining has received much attention in patents recently. To accelerate the development of biomedical text mining for patents, the BioCreative V.5 challenge organized three tracks, i.e., chemical entity mention recognition (CEMP), gene and protein related object recognition (GPRO) and technical interoperability and performance of annotation servers, to focus on biomedical entity recognition in patents. This paper describes our neural network approach for the CEMP and GPRO tracks. In the approach, a bidirectional long short-term memory with a conditional random field layer is employed to recognize biomedical entities from patents. To improve the performance, we explored the effect of additional features (i.e., part of speech, chunking and named entity recognition features generated by the GENIA tagger) for the neural network model. In the official results, our best runs achieve the highest performances (a precision of 88.32%, a recall of 92.62%, and an F-score of 90.42% in the CEMP track; a precision of 76.65%, a recall of 81.91%, and an F-score of 79.19% in the GPRO track) among all participating teams in both tracks.

","MOESM1 of Next generation community assessment of biomedical entity recognition web servers: metrics, performance, interoperability aspects of BeCalm",36,0,,,
Entity Matching,"Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
","Christopher S. Funk, K. Bretonnel Cohen, Lawrence E. Hunter, Karin M. Verspoor ","Background
Gene Ontology (GO) terms represent the standard for annotation and representation of molecular functions, biological processes and cellular compartments, but a large gap exists between the way concepts are represented in the ontology and how they are expressed in natural language text. The construction of highly specific GO terms is formulaic, consisting of parts and pieces from more simple terms.

Results
We present two different types of manually generated rules to help capture the variation of how GO terms can appear in natural language text. The first set of rules takes into account the compositional nature of GO and recursively decomposes the terms into their smallest constituent parts. The second set of rules generates derivational variations of these smaller terms and compositionally combines all generated variants to form the original term. By applying both types of rules, new synonyms are generated for two-thirds of all GO terms and an increase in F-measure performance for recognition of GO on the CRAFT corpus from 0.498 to 0.636 is observed. Additionally, we evaluated the combination of both types of rules over one million full text documents from Elsevier; manual validation and error analysis show we are able to recognize GO concepts with reasonable accuracy (88 %) based on random sampling of annotations.

Conclusions
In this work we present a set of simple synonym generation rules that utilize the highly compositional and formulaic nature of the Gene Ontology concepts. We illustrate how the generated synonyms aid in improving recognition of GO concepts on two different biomedical corpora. We discuss other applications of our rules for GO ontology quality assurance, explore the issue of overgeneration, and provide examples of how similar methodologies could be applied to other biomedical terminologies. Additionally, we provide all generated synonyms for use by the text-mining community.","Additional file 5 of Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
",37,1,"Additional file 5 of Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
","Christopher S. Funk, K. Bretonnel Cohen, Lawrence E. Hunter, Karin M. Verspoor ","The list of novel synonyms generated by this method. (TXT 129024 kb)
"
Entity Matching,"Entity recognition in the biomedical domain using a hybrid approach
","Marco Basaldella, Lenz Furrer, Carlo Tasso , Fabio Rinaldi ","Background
This article describes a high-recall, high-precision approach for the extraction of biomedical entities from scientific articles.

Method
The approach uses a two-stage pipeline, combining a dictionary-based entity recognizer with a machine-learning classifier. First, the OGER entity recognizer, which has a bias towards high recall, annotates the terms that appear in selected domain ontologies. Subsequently, the Distiller framework uses this information as a feature for a machine learning algorithm to select the relevant entities only. For this step, we compare two different supervised machine-learning algorithms: Conditional Random Fields and Neural Networks.

Results
In an in-domain evaluation using the CRAFT corpus, we test the performance of the combined systems when recognizing chemicals, cell types, cellular components, biological processes, molecular functions, organisms, proteins, and biological sequences. Our best system combines dictionary-based candidate generation with Neural-Network-based filtering. It achieves an overall precision of 86% at a recall of 60% on the named entity recognition task, and a precision of 51% at a recall of 49% on the concept recognition task.

Conclusion
These results are to our knowledge the best reported so far in this particular task.","Additional file 5 of Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
",37,0,,,
Entity Matching,"Improved characterisation of clinical text through ontology-based vocabulary expansion","Luke T. Slater, William Bradlow, Simon Ball, Robert Hoehndorf , Georgios V Gkoutos ","Biomedical ontologies contain a wealth of metadata that constitutes a fundamental infrastructural resource for text mining. For several reasons, redundancies exist in the ontology ecosystem, which lead to the same entities being described by several concepts in the same or similar contexts across several ontologies. While these concepts describe the same entities, they contain different sets of complementary metadata. Linking these definitions to make use of their combined metadata could lead to improved performance in ontology-based information retrieval, extraction, and analysis tasks.

Results
We develop and present an algorithm that expands the set of labels associated with an ontology class using a combination of strict lexical matching and cross-ontology reasoner-enabled equivalency queries. Across all disease terms in the Disease Ontology, the approach found 51,362 additional labels, more than tripling the number defined by the ontology itself. Manual validation by a clinical expert on a random sampling of expanded synonyms over the Human Phenotype Ontology yielded a precision of 0.912. Furthermore, we found that annotating patient visits in MIMIC-III with an extended set of Disease Ontology labels led to semantic similarity score derived from those labels being a significantly better predictor of matching first diagnosis, with a mean average precision of 0.88 for the unexpanded set of annotations, and 0.913 for the expanded set.

Conclusions
Inter-ontology synonym expansion can lead to a vast increase in the scale of vocabulary available for text mining applications. While the accuracy of the extended vocabulary is not perfect, it nevertheless led to a significantly improved ontology-based characterisation of patients from text in one setting. Furthermore, where run-on error is not acceptable, the technique can be used to provide candidate synonyms which can be checked by a domain expert.","Additional file 5 of Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
",37,0,,,
Entity Matching,"Automated, Efficient, and Accelerated Knowledge Modeling of the Cognitive Neuroimaging Literature Using the ATHENA Toolkit
","Michael C. Riedel, Taylor Salo, Jason Hays, Matthew D. Turner,Matthew T. Sutherl, Jessica A. Turner, Angela R. Laird","Neuroimaging research is growing rapidly, providing expansive resources for
synthesizing data. However, navigating these dense resources is complicated by the
volume of research articles and variety of experimental designs implemented across
studies. The advent of machine learning algorithms and text-mining techniques has
advanced automated labeling of published articles in biomedical research to alleviate
such obstacles. As of yet, a comprehensive examination of document features and
classifier techniques for annotating neuroimaging articles has yet to be undertaken.
Here, we evaluated which combination of corpus (abstract-only or full-article text),
features (bag-of-words or Cognitive Atlas terms), and classifier (Bernoulli naïve Bayes,
k-nearest neighbors, logistic regression, or support vector classifier) resulted in the
highest predictive performance in annotating a selection of 2,633 manually annotated
neuroimaging articles. We found that, when utilizing full article text, data-driven features
derived from the text performed the best, whereas if article abstracts were used for
annotation, features derived from the Cognitive Atlas performed better. Additionally, we
observed that when features were derived from article text, anatomical terms appeared
to be the most frequently utilized for classification purposes and that cognitive concepts
can be identified based on similar representations of these anatomical terms. Optimizing
parameters for the automated classification of neuroimaging articles may result in a
larger proportion of the neuroimaging literature being annotated with labels supporting
the meta-analysis of psychological constructs","Additional file 5 of Gene Ontology synonym generation rules lead to increased performance in biomedical concept recognition
",37,0,,,
Entity Matching,"Parallel sequence tagging for concept recognition","Lenz Furrer, Joseph Cornelius & Fabio Rinaldi ","Named Entity Recognition (NER) and Normalisation (NEN) are core components of any text-mining system for biomedical texts. In a traditional concept-recognition pipeline, these tasks are combined in a serial way, which is inherently prone to error propagation from NER to NEN. We propose a parallel architecture, where both NER and NEN are modeled as a sequence-labeling task, operating directly on the source text. We examine different harmonisation strategies for merging the predictions of the two classifiers into a single output sequence.

","Additional file 2 of Parallel sequence tagging for concept recognition
",38,1,,,
Entity Matching,"Annotating the Pandemic: Named Entity Recognition and Normalisation in COVID-19 Literature ","Nico Colic, Lenz Furrer, Fabio Rinaldi","The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up.
We are presenting a publicly available pipeline to perform named entity recognition and normalisation in parallel to help find relevant publications and to aid in downstream NLP tasks such as text summarisation. In our approach, we are using a dictionary-based system for its high recall in conjunction with two models based on BioBERT for their accuracy. Their outputs are combined according to different strategies depending on the entity type. In addition, we are using a manually crafted dictionary to increase performance for new concepts related to COVID-19.
We have previously evaluated our work on the CRAFT corpus, and make the output of our pipeline available on two visualisation platforms.","Additional file 2 of Parallel sequence tagging for concept recognition
",38,0,,,
Entity Matching,"Fast and Effective Biomedical Entity Linking Using a Dual Encoder","Rajarshi Bhowmik, Karl Stratos, Gerard de Melo","Biomedical entity linking is the task of identifying mentions of biomedical concepts in text documents and mapping them to canonical entities in a target thesaurus. Recent advancements in entity linking using BERT-based models follow a retrieve and rerank paradigm, where the candidate entities are first selected using a retriever model, and then the retrieved candidates are ranked by a reranker model. While this paradigm produces state-of-the-art results, they are slow both at training and test time as they can process only one mention at a time. To mitigate these issues, we propose a BERT-based dual encoder model that resolves multiple mentions in a document in one shot. We show that our proposed model is multiple times faster than existing BERT-based models while being competitive in accuracy for biomedical entity linking. Additionally, we modify our dual encoder model for end-to-end biomedical entity linking that performs both mention span detection and entity disambiguation and out-performs two recently proposed models.","Additional file 2 of Parallel sequence tagging for concept recognition
",38,0,,,
Entity Matching,Systematic Association of Genes to Phenotypes by Genome and Literature Mining,"Jan O Korbel ,Tobias Doerks ,Lars J Jensen,Carolina Perez-Iratxeta,Szymon Kaczanowski,Sean D Hooper,Miguel A Andrade,Peer Bork 
","One of the major challenges of functional genomics is to unravel the connection between genotype and phenotype. So far no global analysis has attempted to explore those connections in the light of the large phenotypic variability seen in nature. Here, we use an unsupervised, systematic approach for associating genes and phenotypic characteristics that combines literature mining with comparative genome analysis. We first mine the MEDLINE literature database for terms that reflect phenotypic similarities of species. Subsequently we predict the likely genomic determinants: genes specifically present in the respective genomes. In a global analysis involving 92 prokaryotic genomes we retrieve 323 clusters containing a total of 2,700 significant gene–phenotype associations. Some clusters contain mostly known relationships, such as genes involved in motility or plant degradation, often with additional hypothetical proteins associated with those phenotypes. Other clusters comprise unexpected associations; for example, a group of terms related to food and spoilage is linked to genes predicted to be involved in bacterial food poisoning. Among the clusters, we observe an enrichment of pathogenicity-related associations, suggesting that the approach reveals many novel genes likely to play a role in infectious diseases.

","Named Entity Recognition for Bacterial Type IV Secretion Systems",39,1,"Named Entity Recognition for Bacterial Type IV Secretion Systems
","Sophia Ananiadou, Dan Sullivan, William Black, Gina-Anne Levow, Joseph J. Gillespie, Chunhong Mao, Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii, Bruno Sobral
",
Entity Matching,"Corpus-Based Correlational Study of Terms and Quality in Business English Writing","Shili Ge, Jingchao Zhang, Xiaoxiao Chen ","One of the most important tasks in automated essay scoring (AES) is feature selection. Terms are indispensable in Business English (BE) writing. In order to analyze the possibility of involving terms in BE writing automated scoring feature set, the strength of correlations between terminological features and writing quality or scores is studied. A Business English term bank (BETB) was built based on a term dictionary. With BETB and a self-coded Python program, business terms and their categories in a BE writing corpus were identified and extracted. The analysis shows that, among ten categories of terms and total term numbers in BE writing, human resource terms and total term numbers have a moderate correlation with writing scores. This result means business terms, especially writing content related terms, should be covered in business AES feature set, which can improve the performance of AES systems and facilitate BE learners’ writing proficiency.
","Named Entity Recognition for Bacterial Type IV Secretion Systems",39,0,,,
Entity Matching,"Design, implementation, and operation of a rapid, robust named entity recognition web service","Sune Pletscher-Frankild , Lars Juhl Jensen ","Most BioCreative tasks to date have focused on assessing the quality of text-mining annotations in terms of precision and recall. Interoperability, speed, and stability are, however, other important factors to consider for practical applications of text mining. For about a decade, we have run named entity recognition (NER) web services, which are designed to be efficient, implemented using a multi-threaded queueing system to robustly handle many simultaneous requests, and hosted at a supercomputer facility. To participate in this new task, we extended the existing NER tagging service with support for the BeCalm API. The tagger suffered no downtime during the challenge and, as in earlier tests, proved to be highly efficient, consistently processing requests of 5000 abstracts in less than half a minute. In fact, the majority of this time was spent not on the NER task but rather on retrieving the document texts from the challenge servers. The latter was found to be the main bottleneck even when hosting a copy of the tagging service on a Raspberry Pi 3, showing that local document storage or caching would be desirable features to include in future revisions of the API standard.
","MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",40,1,"MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",,"Research on specialized biological systems is often hampered by a lack of consistent terminology, especially across species. In bacterial Type IV secretion systems genes within one set of orthologs may have over a dozen different names. Classifying research publications based on biological processes, cellular components, molecular functions, and microorganism species should improve the precision and recall of literature searches allowing researchers to keep up with the exponentially growing literature, through resources such as the Pathosystems Resource Integration Center (PATRIC, patricbrc.org). We developed named entity recognition (NER) tools for four entities related to Type IV secretion systems: 1) bacteria names, 2) biological processes, 3) molecular functions, and 4) cellular components. These four entities are important to pathogenesis and virulence research but have received less attention than other entities, e.g., genes and proteins. Based on an annotated corpus, large domain terminological resources, and machine learning techniques, we developed recognizers for these entities. High accuracy rates (>80%) are achieved for bacteria, biological processes, and molecular function. Contrastive experiments highlighted the effectiveness of alternate recognition strategies; results of term extraction on contrasting document sets demonstrated the utility of these classes for identifying T4SS-related documents.
"
Entity Matching,"Geneshot: search engine for ranking genes from arbitrary text queries ","Alexander Lachmann, Brian M Schilder, Megan L Wojciechowicz, Denis Torre, Maxim V Kuleshov, Alexandra B Keenan, Avi Ma’ayan","The frequency by which genes are studied correlates with the prior knowledge accumulated about them. This leads to an imbalance in research attention where some genes are highly investigated while others are ignored. Geneshot is a search engine developed to illuminate this gap and to promote attention to the under-studied genome. Through a simple web interface, Geneshot enables researchers to enter arbitrary search terms, to receive ranked lists of genes relevant to the search terms. Returned ranked gene lists contain genes that were previously published in association with the search terms, as well as genes predicted to be associated with the terms based on data integration from multiple sources. The search results are presented with interactive visualizations. To predict gene function, Geneshot utilizes gene–gene similarity matrices from processed RNA-seq data, or from gene–gene co-occurrence data obtained from multiple sources. In addition, Geneshot can be used to analyze the novelty of gene sets and augment gene sets with additional relevant genes. The Geneshot web-server and API are freely and openly available from https://amp.pharm.mssm.edu/geneshot.
","MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",40,0,,,
Entity Matching,"Improving spaCy dependency annotation and PoS tagging web service using independent NER services
","Nico Colic,  Fabio Rinaldi ","Dependency parsing is often used as a component in many text analysis pipelines. However, performance, especially in specialized domains, suffers from the presence of complex terminology. Our hypothesis is that including named entity annotations can improve the speed and quality of dependency parses. As part of BLAH5, we built a web service delivering improved dependency parses by taking into account named entity annotations obtained by third party services. Our evaluation shows improved results and better speed.","MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",40,0,,,
Entity Matching,"LOCALE: A rule-based location named-entity recognition
method for Latin text","Ivona Milanova, Jurij Silc, Miha Seruˇcnik, Tome Eftimov,  Hristijan Gjoreski","For creation of digital textual corpora of preserved historical sources, automatic or
semi-automatic extraction of specific types of information is becoming a requested tool for
many researchers active in the field of digital humanities. With such tools, the efforts in
digitization and semantic annotation will be greatly aided. For this reason, we propose a
rule-based named-entity recognition system that can be used for location extraction from
Latin text (so-call LOCALE). It is based on a set of computational linguistics rules for
Latin language. Experimental results obtained on a set of 100 documents, which were
further manually evaluated by human experts, showed that very promising results are
achieved.","MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",40,0,,,
Entity Matching,"Microbe-set enrichment analysis facilitates functional interpretation of microbiome profiling data","Yan Kou, Xiaomin Xu, Zhengnong Zhu, Lei Dai , Yan Tan","The commensal microbiome is known to influence a variety of host phenotypes. Microbiome profiling followed by differential abundance analysis has been established as an effective approach to study the mechanisms of host-microbiome interactions. However, it is challenging to interpret the collective functions of the resultant microbe-sets due to the lack of well-organized functional characterization of commensal microbiome. We developed microbe-set enrichment analysis (MSEA) to enable the functional interpretation of microbe-sets by examining the statistical significance of their overlaps with annotated groups of microbes that share common attributes such as biological function or phylogenetic similarity. We then constructed microbe-set libraries by query PubMed to find microbe-mammalian gene associations and disease associations by parsing the Disbiome database. To demonstrate the utility of our novel MSEA methodology, we carried out three case studies using publicly available curated knowledge resource and microbiome profiling datasets focusing on human diseases. We found MSEA not only yields consistent findings with the original studies, but also recovers insights about disease mechanisms that are supported by the literature. Overall, MSEA is a useful knowledge-based computational approach to interpret the functions of microbes, which can be integrated with microbiome profiling pipelines to help reveal the underlying mechanism of host-microbiome interactions.

","MOESM1 of Design, implementation, and operation of a rapid, robust named entity recognition web service",40,0,,,
Image classification,Privacy-aware image classification and search,"Sergej Zerr, Stefan Siersdorfer, Jonathon Hare, Elena Demidova","Modern content sharing environments such as Flickr or YouTube contain a large amount of private resources such as photos showing weddings, family holidays, and private parties. These resources can be of a highly sensitive nature, disclosing many details of the users' private sphere. In order to support users in making privacy decisions in the context of image sharing and to provide them with a better overview on privacy related visual content available on the Web, we propose techniques to automatically detect private images, and to enable privacy-oriented image search. To this end, we learn privacy classifiers trained on a large set of manually assessed Flickr photos, combining textual metadata of images with a variety of visual features. We employ the resulting classification models for specifically searching for private photos, and for diversifying query results to provide users with a better coverage of private and public content. Large-scale classification experiments reveal insights into the predictive performance of different visual and textual features, and a user evaluation of query result rankings demonstrates the viability of our approach.",Privacy-aware image classification and search,41,1,Privacy-aware image classification and search,"Sergej Zerr, Stefan Siersdorfer, Hare Jonathon, Elena Demidova","Modern content sharing environments such as Flickr or YouTube contain a large number of private resources such as photos showing weddings, family holidays, and private parties. These resources can be of a highly sensitive nature, disclosing many details of the users' private sphere. In order to support users in making privacy decisions in the context of image sharing and to provide them with a better overview of privacy-related visual content available on the Web, we propose techniques to automatically detect private images and to enable privacy-oriented image search. In order to classify images, we use the metadata like title and tags and plan to use visual features which are described in our scientific paper. The data set used in the paper is now available.

Picalet! cleaned dataset - ( recommended for experiments)
userstudy - (images annotated with queries, anonymized user id and privacy value)"
Image classification,iPrivacy: Image Privacy Protection by Identifying Sensitive Objects via Deep Multi-Task Learning,"Jun Yu, Baopeng Zhang, Zhengzhong Kuang, Dan Lin, Jianping Fan","To achieve automatic recommendation of privacy
settings for image sharing, a new tool called iPrivacy (image
privacy) is developed for releasing the burden from users on
setting the privacy preferences when they share their images for
special moments. Specifically, this paper consists of the following
contributions: 1) massive social images and their privacy settings
are leveraged to learn the object-privacy relatedness effectively
and identify a set of privacy-sensitive object classes automatically;
2) a deep multi-task learning algorithm is developed to jointly
learn more representative deep convolutional neural networks
and more discriminative tree classifier, so that we can achieve
fast and accurate detection of large numbers of privacy-sensitive
object classes; 3) automatic recommendation of privacy settings
for image sharing can be achieved by detecting the underlying privacy-sensitive objects from the images being shared,
recognizing their classes, and identifying their privacy settings
according to the object-privacy relatedness; and 4) one simple
solution for image privacy protection is provided by blurring
the privacy-sensitive objects automatically. We have conducted
extensive experimental studies on real-world images and the
results have demonstrated both the efficiency and effectiveness
of our proposed approach.
Index Terms— Image sharing, privacy setting recommendation,
object-privacy alignment, image privacy protection, privacysensitive object classes, deep multi-task learning, tree classifier
for hierarchical object detection.",Privacy-aware image classification and search,41,0,,,
Image classification,Leveraging Content Sensitiveness and User Trustworthiness to Recommend Fine-Grained Privacy Settings for Social Image Sharing,"Jun Yu, Zhenzhong Kuang, Baopeng Zhang, Wei Zhang, Dan Lin, Jianping Fan","To configure successful privacy settings for social image sharing, two issues are inseparable: 1) content sensitiveness of the images being shared; and 2) trustworthiness of the users being granted to see the images. This paper aims to consider these two inseparable issues simultaneously to recommend fine-grained privacy settings for social image sharing. For achieving more compact representation of image content sensitiveness (privacy), two approaches are developed: 1) a deep network is adapted to extract 1024-D discriminative deep features; and 2) a deep multiple instance learning algorithm is adopted to identify 280 privacy-sensitive object classes and events. Second, users on the social network are clustered into a set of representative social groups to generate a discriminative dictionary for user trustworthiness characterization. Finally, both the image content sensitiveness and the user trustworthiness are integrated to train a tree classifier to recommend fine-grained privacy settings for social image sharing. Our experimental studies have demonstrated both the efficiency and the effectiveness of our proposed algorithms.",Privacy-aware image classification and search,41,0,,,
Image classification,PicAlert!: a system for privacy-aware image classification and retrieval,"Sergej Zerr, Stefan Siersdorfer, Jonathon Hare","Photo publishing in Social Networks and other Web2.0 applications has become very popular due to the pervasive
availability of cheap digital cameras, powerful batch upload
tools and a huge amount of storage space. A portion of
uploaded images are of a highly sensitive nature, disclosing
many details of the users’ private life. We have developed a
web service which can detect private images within a user’s
photo stream and provide support in making privacy decisions in the sharing context. In addition, we present a
privacy-oriented image search application which automatically identifies potentially sensitive images in the result set
and separates them from the remaining pictures.",Privacy-aware image classification and search,41,0,,,
Image classification,Privacy Policy Inference of User-Uploaded Images on Content Sharing Sites,"Anna Cinzia Squicciarini, Dan Lin, Smitha Sundareswaran, Joshua Wede","With the increasing volume of images users share through social sites, maintaining privacy has become a major problem, as demonstrated by a recent wave of publicized incidents where users inadvertently shared personal information. In light of these incidents, the need of tools to help users control access to their shared content is apparent. Toward addressing this need, we propose an Adaptive Privacy Policy Prediction (A3P) system to help users compose privacy settings for their images. We examine the role of social context, image content, and metadata as possible indicators of users' privacy preferences. We propose a two-level framework which according to the user's available history on the site, determines the best available privacy policy for the user's images being uploaded. Our solution relies on an image classification framework for image categories which may be associated with similar policies, and on a policy prediction algorithm to automatically generate a policy for each newly uploaded image, also according to users' social features. Overtime, the generated policies will follow the evolution of users' privacy attitude. We provide the results of our extensive evaluation over 5,000 policies, which demonstrate the effectiveness of our system, with prediction accuracies over 90 percent.
",Privacy-aware image classification and search,41,0,,,
Image classification,SnapMe if you can: privacy threats of other peoples' geo-tagged media and what we can do about it,"Benjamin Henne, Christian Szongott, Matthew Smith","The amount of media uploaded to the Web is still rapidly expanding. The ease-of-use of modern smartphones in combination with the proliferation of high-speed mobile networks facilitates a culture of spontaneous and often carefree sharing of user-generated content, especially photos and videos. An increasing number of modern devices are capable of embedding location information and other metadata into created content. However, currently there is not much user awareness of possible privacy consequences of such data. While in most cases users upload their own media consciously, the flood of media uploaded by others is so huge that it is almost impossible for users to stay aware of all media that might be relevant to them. Current social network services and photo-sharing sites mainly focus on the privacy of users' own media in terms of access control, but offer few possibilities to deal with privacy implications created by other users' actions. We conducted an online survey with 414 participants. The results show that users would like to get more information about media shared by others. Based on an analysis of prevalent sharing services like Flickr, Facebook, or Google+ and an analysis of metadata of three different sets of crawled photos, we discuss privacy implications and potentials of the emerging trend of (geo-)tagged media. Finally, we present a novel concept on how location information can actually help users to control the flood of potentially infringing or interesting media.
",Privacy-aware image classification and search,41,0,,,
Image classification,PIC: Enable Large-Scale Privacy Preserving Content-Based Image Search on Cloud,"Lan Zhang, Taeho Jung, Kebin Liu, Xiang-Yang Li, Xuan Ding, Jiaxi Gu, Yunhao Liu","Many cloud platforms emerge to meet urgent requirements for large-volume personal image store, sharing and search. Though most would agree that images contain rich sensitive information (e.g., People, location and event) and people's privacy concerns hinder their participation into untrusted services, today's cloud platforms provide little support for image privacy protection. Facing large-scale images from multiple users, it is extremely challenging for the cloud to maintain the index structure and schedule parallel computation without learning anything about the image content and indices. In this work, we introduce a novel system PIC: a Privacy-preserving Image search system on Cloud, which is a step towards feasible cloud services which provide secure content-based large-scale image search with fine-grained access control. Users can search on others' images if they are authorized by the image owners. Majority of the computationally intensive jobs are handled by the cloud, and a querier can now simply send the query and receive the result. Specially, to deal with massive images, we design our system suitable for distributed and parallel computation and introduce several optimizations to further expedite the search process. Our security analysis and prototype system evaluation results show that PIC successfully protects the image privacy at a low cost of computation and communication.
",Privacy-aware image classification and search,41,0,,,
Image classification,When Machine Learning Meets Privacy: A Survey and Outlook,"Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin","The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.
",Privacy-aware image classification and search,41,0,,,
Image classification,Survey on Access Control for Community-Centered Collaborative Systems,"Federica Paci, Anna Squicciarini, Nicola Zannone","The last decades have seen a growing interest and demand for community-centered collaborative systems and platforms. These systems and platforms aim to provide an environment in which users can collaboratively create, share, and manage resources. While offering attractive opportunities for online collaboration and information sharing, they also open several security and privacy issues. This has attracted several research efforts toward the design and implementation of novel access control solutions that can handle the complexity introduced by collaboration. Despite these efforts, transition to practice has been hindered by the lack of maturity of the proposed solutions. The access control mechanisms typically adopted by commercial collaborative systems like online social network websites and collaborative editing platforms, are still rather rudimentary and do not provide users with a sufficient control over their resources. This survey examines the growing literature on access control for collaborative systems centered on communities, and identifies the main challenges to be addressed in order to facilitate the adoption of collaborative access control solutions in real-life settings. Based on the literature study, we delineate a roadmap for future research in the area of access control for community-centered collaborative systems.

",Privacy-aware image classification and search,41,0,,,
Image classification,Privacy-CNH: A Framework to Detect Photo Privacy with Convolutional Neural Network Using Hierarchical Features,"Lam Tran, Deguang Kong, Hongxia Jin, Ji Liu","Photo privacy is a very important problem in the digital age where photos are commonly shared on social networking sites and mobile devices. The main challenge in photo privacy detection is how to generate discriminant features to accurately detect privacy at risk photos. Existing photo privacy detection works, which rely on low-level vision features, are non-informative to the users regarding what privacy information is leaked from their photos. In this paper, we propose a new framework called Privacy-CNH that utilizes hierarchical features which include both object and convolutional features in a deep learning model to detect privacy at risk photos. The generation of object features enables our model to better inform the users about the reason why a photo has privacy risk. The combination of convolutional and object features provide a richer model to understand photo privacy from different aspects, thus improving photo privacy detection accuracy. Experimental results demonstrate that the proposed model outperforms the state-of-the-art work and the standard convolutional neural network (CNN) with low-level features on photo privacy detection tasks.

",Privacy-aware image classification and search,41,0,,,
Image classification,Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images,"Tribhuvanesh Orekondy, Mario Fritz, Bernt Schiele;","Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a redaction by segmentation paradigm. Hence, we propose the first sizable dataset of private images ""in the wild"" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information. It is effective at achieving various privacy-utility trade-offs within 83% of the performance of manual redaction.",Privacy-aware image classification and search,41,0,,,
Image classification,Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Map,"Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza Mohammadi","One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increases the number of models' weights. ResNet and next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution and also not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification Challenge, MIT Indoors Scenes, and a part of the ImageNet dataset. We investigated the implementation of our architecture on several models of Inception, ResNet and DensNet families. Applying our architecture has revealed a significant effect on increasing convergence speed and accuracy. Our Experiments on images with 224x224 resolution increased the Top-1 accuracy between 2% to 8% on different datasets and models. Running our models on 512x512 resolution images of the MIT Indoors Scenes dataset showed a notable result of improving the Top-1 accuracy within 3% to 26%. We will also demonstrate the GAP layer's disadvantage when the input images are large and the number of classes is not few. In this circumstance, our proposed architecture can do a great help in enhancing classification results. The code is shared at this https URL.",Intel Image Classification Dataset,42,1,Intel Image Classification Dataset,"Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza Mohammadi","Context This is image data of Natural Scenes around the world.

Content This Data contains around 25k images of size 150x150 distributed under 6 categories. {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }

The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction. This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.

Acknowledgements Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data

Photo by Jan Böttinger on Unsplash

Inspiration Want to build powerful Neural network that can classify these images with more accuracy."
Image classification,ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images,"Mohammad Rahimzadeh, Mahmoud Reza Mohammadi","Optical coherence tomography (OCT) imaging is a well-known technology for visualizing retinal layers and helps ophthalmologists to detect possible diseases. Accurate and early diagnosis of common retinal diseases can prevent the patients from suffering critical damages to their vision. Computer-aided diagnosis (CAD) systems can significantly assist ophthalmologists in improving their examinations. This paper presents a new enhanced deep ensemble convolutional neural network for detecting retinal diseases from OCT images. Our model generates rich and multi-resolution features by employing the learning architectures of two robust convolutional models. Spatial resolution is a critical factor in medical images, especially the OCT images that contain tiny essential points. To empower our model, we apply a new post-architecture model to our ensemble model for enhancing spatial resolution learning without increasing computational costs. The introduced post-architecture model can be deployed to any feature extraction model to improve the utilization of the feature map's spatial values. We have collected two open-source datasets for our experiments to make our models capable of detecting six crucial retinal diseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen alongside the normal cases. Our experiments on two datasets and comparing our model with some other well-known deep convolutional neural networks have proven that our architecture can increase the classification accuracy up to 5%. We hope that our proposed methods create the next step of CAD systems development and help future researches. The code of this paper is shared at this https URL.",Intel Image Classification Dataset,42,0,,,
Image classification,Personalizing Pre-trained Models,"Mina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali Hazariwala, Pattie Maes","Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.",Intel Image Classification Dataset,42,0,,,
Image classification,Process Construction of VR Technology in Interior Design: From Image Feature Analysis to Global Texture Modelling,"Fu Yi, Yu Chunjiang","Process construction of VR technology in interior design from image feature analysis to global texture modelling is studied in this paper. According to the driving method of the virtual finger, the pose data of the operator’s palm is converted and mapped to the pose data of the virtual palm, and then the entity translation and rotation functions are called to change the pose of the virtual palm in real time. Therefore, this article introduces general human-computer interaction technology to enable users to select important areas to ensure that they can be more protected when zooming in and reduce the probability of distortion in the area. With this target, the VR system is enhanced based on the image global feature analysis. For testing the results, the novel data sets are collected and the proposed model is compared with the other approaches.",Intel Image Classification Dataset,42,0,,,
Image classification,Multi-Class Weather Classification Using ResNet-18 CNN for Autonomous IoT and CPS Applications,"Qasem Abu Al-Haija, Mahmoud A. Smadi, Saleh Zein-Sabatto","Severe circumstances of outdoor weather might have a significant influence on the road traffic. However, the early weather condition warning and detection can provide a significant chance for correct control and survival. Therefore, the auto-recognition models of weather situations with high level of confidence are essentially needed for several autonomous IoT systems, self-driving vehicles and transport control systems. In this work, we propose an accurate and precise self-reliant framework for weather recognition using ResNet-18 convolutional neural network to provide multi-class weather classification. The proposed model employs transfer learning technique of the powerful ResNet-18 CNN pretrained on ImageNet to train and classify weather recognition images dataset into four classes including: sunrise, shine, rain, and cloudy. The simulation results showed that our proposed model achieves remarkable classification accuracy of 98.22% outperforming other compared models trained on the same dataset.",Multi-class Weather Dataset for Image Classification,43,1,Multi-class Weather Dataset for Image Classification,Gbeminiyi Ajayi,"Multi-class weather dataset(MWD) for image classification is a valuable dataset used in the research paper entitled “Multi-class weather recognition from still image using heterogeneous ensemble method”. The dataset provides a platform for outdoor weather analysis by extracting various features for recognizing different weather conditions.
"
Image classification,Multiclass Classification of Firewall Log Files Using Shallow Neural Network for Network Security Applications,"Qasem Abu Al-Haija, Abdelraouf Ishtaiwi","Firewalls are essential devices to protect the communication networks by means of filtering out all incoming (and sometimes outgoing) traffic packets. The filtration process is performed by matching the traffic packets against predefined rules aiming to preclude cyber-threats from getting into the network. Accordingly, the firewall system proceeds with either to “allow,” “deny,” or “drop/reset” the incoming packet. Thus, an automated smart actions’ classification process is essential for improved firewall operations. In this paper, we propose an intelligent classification model that can be employed in the firewall systems to produce proper action for every communicated packet by analyzing packet attributes using a shallow neural network (SNN). Specifically, the proposed model employs SNN with 150-neurons at the hidden layer to train and classify the Internet Firewall-2019 (IFW-2019) dataset into three classes, including: “allow, “deny,” and “drop/reset.” The experimental results exhibited our classification model's superiority, scoring an overall accuracy of 98.5% with a cross-entropy loss of 0.022 attained after 381 epochs for the 3-class classifier. Also, the proposed model was evaluated using several other evaluation metrics, including confusion matrix parameters, positive predictive value, true positive rate, harmonic mean, and false positive/negative rates. Eventually, the proposed model outperformed many other recent firewall classification systems in the same area of study.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Identifying Phasic dopamine releases using DarkNet-19 Convolutional Neural Network,Qasem Abu Al-Haija; Mahmoud Smadi; Osama M. Al-Bataineh,"Understanding the role of neurotransmitter dopamine in brain function under normal or pathological states is one of the most active areas of research in neurosciences. Failures in dopamine neurotransmission affects tremendous amount of brain abilities including movement, mental, and motivation and reward systems of the brain. Ability to measure phasic release of dopamine in specific locations of the brain will lead to a powerful tool for the neuroscientists, However, the tremendous amount of image-formed data as produced from different locations of the brain makes the manual analysis of these data cumbersome. Luckily, image processing techniques will help in solving these problems effortlessly to ease and speed the analysis for neuro-physicians. In this paper, we propose a deep-learning based identification scheme to identify the release case of phasic dopamine by examining the dopamine analysis (DA) imaging attributes using a convolutional neural network (CNN). More precisely, the proposed scheme exploits the transfer learning based DarkNet-19 network to train and identify the phasic dopamine release-2019 (PDR19) dataset into two-classes; namely, “release images,” or “non-release images” The experimental outcomes demonstrated the distinction of our identification scheme, recording an identification accuracy of 99.1% with a cross entropy loss of 0.022 attained after 25 epochs each with 100 iterations (i.e., 2500 iterations) for the 2-class classifier. Besides, our identification scheme was assessed using many other assessment factors, such as the identification precision percentage (IPP), the identification sensitivity percentage (ISnP), the identification specificity percentage (ISpP), and the identification weighted average percentage (F1P). Consequently, the performance of the proposed scheme surpassed several existing dopamine identification schemes.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Attack-Aware IoT Network Traffic Routing Leveraging Ensemble Learning,"Qasem Abu Al-Haija, Ahmad Al-Badawi","Network Intrusion Detection Systems (NIDSs) are indispensable defensive tools against various cyberattacks. Lightweight, multipurpose, and anomaly-based detection NIDSs employ several methods to build profiles for normal and malicious behaviors. In this paper, we design, implement, and evaluate the performance of machine-learning-based NIDS in IoT networks. Specifically, we study six supervised learning methods that belong to three different classes: (1) ensemble methods, (2) neural network methods, and (3) kernel methods. To evaluate the developed NIDSs, we use the distilled-Kitsune-2018 and NSL-KDD datasets, both consisting of a contemporary real-world IoT network traffic subjected to different network attacks. Standard performance evaluation metrics from the machine-learning literature are used to evaluate the identification accuracy, error rates, and inference speed. Our empirical analysis indicates that ensemble methods provide better accuracy and lower error rates compared with neural network and kernel methods. On the other hand, neural network methods provide the highest inference speed which proves their suitability for high-bandwidth networks. We also provide a comparison with state-of-the-art solutions and show that our best results are better than any prior art by 1~20%.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Machine Learning Based Model to Identify Firewall Decisions to Improve Cyber-Defense,"Qasem Abu Al-Haijaa, Abdelraouf Ishtaiwia","A firewall system is a security system to ensure traffic control for incoming and outgoing packets passing through communication networks by applying specific decisions to improve cyber-defense and decide against malicious packets. The filtration process matches the traffic packets against predefined rules to preclude cyber threats from getting into the network. Accordingly, the firewall system proceeds with either to “allow,” “deny,” or “drop/reset” the incoming packet. This paper proposes an intelligent classification model that can be employed in the firewall systems to produce proper action for every communicated packet by analyzing packet attributes using two machine learning methods, namely, shallow neural network (SNN), and optimizable decision tree (ODT). Specifically, the proposed models have used to train and classify the Internet Firewall-2019 dataset into three classes: “allow, “deny,” and “drop/reset.” The experimental results exhibited our classification model's superiority, scoring an overall accuracy of 99.8%, and 98.5% for ODT, and SNN respectively. Besides, the suggested system was evaluated using many evaluation metrics, including confusion matrix parameters (TP, TN, FP, FN), true positive rate (TPR), false-negative rate (FNR), positive predictive value (PPV), false discovery rate (FDR), and the receiver operating characteristic (ROC) curves for the developed three-class classifier. Ultimately, the proposed system outpaced many existing up-to-date firewall classification systems in the same area of study.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Autonomous Driving in Adverse Weather Conditions: A Survey,"Yuxiao Zhang, Alexander Carballo, Hanting Yang, Kazuya Takeda","Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, autonomous driving under adverse weather conditions has been the problem that keeps autonomous vehicles (AVs) from going to level 4 or higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in an analytic and statistical way, and surveys the solutions against inclement weather conditions. State-of-the-art techniques on perception enhancement with regard to each kind of weather are thoroughly reported. External auxiliary solutions like V2X technology, weather conditions coverage in currently available datasets, simulators, and experimental facilities with weather chambers are distinctly sorted out. By pointing out all kinds of major weather problems the autonomous driving field is currently facing, and reviewing both hardware and computer science solutions in recent years, this survey contributes a holistic overview on the obstacles and directions of ADS development in terms of adverse weather driving conditions.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,High-performance intrusion detection system for networked UAVs via deep learning,"Qasem Abu Al-Haija, Ahmad Al Badawi","Recently, Unmanned Aerial Vehicles (UAVs) have become a widely popular technology with remarkable growth and unprecedented attention. However, UAV communication networks are susceptible to various cyber-intrusions/threats due to their limited computation and communication capabilities. Such intrusions/misbehaviors tend to be processed as normal packets through the UAV communication networks. In this work, we present an autonomous intrusion detection system that can efficiently detect the malicious threats invading UAV using deep convolutional neural networks (UAV-IDS-ConvNet). Specifically, the proposed system considers encrypted Wi-Fi traffic data records of three types of commonly used UAVs: Parrot Bebop UAVs, DBPower UDI UAVs, and DJI Spark UAVs. To evaluate the developed system, we employed the UAV-IDS-2020 dataset which includes various attacks on UAV networks in unidirectional and bidirectional communication flow modes. Moreover, we emulate the context of homogeneous and heterogeneous networked UAVs. Our best experimental outcomes exhibited a victorious intrusion detection accuracy of 99.50% for the two-class classifier model (normal UAV vs. anomaly) with 2.77 ms prediction time. Besides, the proposed system was evaluated using other performance metrics including confusion matrix parameters, false alarm rate, detection precision, detection sensitivity, and prediction overhead. The performance analysis showed that our UAV-IDS-ConvNet system outperforms several recent existing intrusion detection systems developed to secure the UAV communication networks by (6–23) %.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Magnetic resonance image diagnosis of femoral head necrosis based on ResNet18 network,"Yan Liua, Guo-rong She, Shu-xaing Chen","Purpose: In order to enhance the practicability of the application of Magnetic Resonance Imaging (MRI) in the diagnosis of femoral head necrosis, combined with the convolutional neural network (CNN), we propose an automatic identification of femoral head necrosis model based on the ResNet18 network.

Methods
In order to verify that MRI has a higher detection rate for early femoral head necrosis, we collected 360 cases of femoral MRI and the same number of femoral CT. Combining this method with ResNet18, AlexNet, and VGG16, compare the clinical staging and typical signs of femoral head necrosis with 8 diagnostic methods.

Results
The total detection rate of MRI combined with ResNet18 is as high as 99.27%, which is much higher than the other three comparison methods. The sensitivity is 97%, the specificity is 98.99%, and the accuracy is 98.23%. The difference is statistically significant.

Conclusion
The automatic recognition femoral MRI model based on the ResNet18 network has a high detection rate for early femoral head necrosis, and can effectively detect bone marrow edema, line-like signs and other signs, providing a reliable reference for early treatment.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Early Stage Diabetes Risk Prediction via Machine Learning,"Qasem Abu Al-Haija, Mahmoud Smadi, Osama M. Al-Bataineh","Diabetes disease is triggering many health problems including microvascular diseases, macrovascular abnormalities, and neuropathic diseases. From an economic perspective, diabetes is one of the costliest diseases, moreover, high percentage of adults with diabetes live in low- and middle-income countries triggering more economic troubles to these countries. Diagnosing the risk of diabetes will help combat against this silent killer. In this paper, we propose an inclusive machine learning based predictive model for diagnosing the risk of having diabetes using a recent dataset of signs and symptoms, known as Diabetes Risk Prediction (DRP2020). We employ more than twenty ML techniques on DRP2020 and we evaluate all ML based models using different performance evaluation metrics including accuracy, precision, recall, harmonic mean, prediction speed and alarm errors. We provide extensive simulation results and compare the performance of various ML based models. Accordingly, the model based shallow neural networks (SNN) has been elected as the optimum model for constructing of the early stage diabetes risk prediction scoring a 99.23% and 99.38% for prediction accuracy and the harmonic mean of precision and recall, respectively. The obtained results exhibit the proficiency and distinction of our model over other state-of art models. Eventually, the proposed system can be proficiently deployed as a clinical tool to assist in provide early stage diabetes risk prediction.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Detection in Adverse Weather Conditions for Autonomous Vehicles via Deep Learning,"Qasem Abu Al-Haija, Manaf Gharaibeh, Ammar Odeh","Weather detection systems (WDS) have an indispensable role in supporting the decisions of autonomous vehicles, especially in severe and adverse circumstances. With deep learning techniques, autonomous vehicles can effectively identify outdoor weather conditions and thus make appropriate decisions to easily adapt to new conditions and environments. This paper proposes a deep learning (DL)-based detection framework to categorize weather conditions for autonomous vehicles in adverse or normal situations. The proposed framework leverages the power of transfer learning techniques along with the powerful Nvidia GPU to characterize the performance of three deep convolutional neural networks (CNNs): SqueezeNet, ResNet-50, and EfficientNet. The developed models have been evaluated on two up-to-date weather imaging datasets, namely, DAWN2020 and MCWRD2018. The combined dataset has been used to provide six weather classes: cloudy, rainy, snowy, sandy, shine, and sunrise. Experimentally, all models demonstrated superior classification capacity, with the best experimental performance metrics recorded for the weather-detection-based ResNet-50 CNN model scoring 98.48%, 98.51%, and 98.41% for detection accuracy, precision, and sensitivity. In addition to this, a short detection time has been noted for the weather-detection-based ResNet-50 CNN model, involving an average of 5 (ms) for the time-per-inference step using the GPU component. Finally, comparison with other related state-of-art models showed the superiority of our model which improved the classification accuracy for the six weather conditions classifiers by a factor of 0.5–21%. Consequently, the proposed framework can be effectively implemented in real-time environments to provide decisions on demand for autonomous vehicles with quick, precise detection capacity.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,URL-based Phishing Websites Detection via Machine Learning,"Qasem Abu Al-Haija, Ahmad Al Badawi","Phishing is a cybersecurity attack that is used to trick victim users to provide sensitive information or deploy malicious software on their infrastructure. Depending on the target system and users, these attacks can inflict severe negative impacts on the system. Therefore, researchers have been working on developing phishing detection and prevention techniques to thwart these attacks. In this paper, we present an efficient phishing websites detection system that analyzes the phishing websites URL addresses to learn data patterns that can identify authentic and phishing websites. Our system employs machine learning techniques such as neural networks and decision trees to learn data patterns in websites URLs. We evaluate our system on a recent phishing websites dataset using classification accuracy as a performance indicator. Our best result shows that decision trees models provide 97.40% classification accuracy on the almost balanced-class dataset.",Multi-class Weather Dataset for Image Classification,43,0,,,
Image classification,Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2,"Michael A. Tabak, Mohammad S. Norouzzadeh, David W. Wolfson, Erica J. Newton, Raoul K. Boughton, Jacob S. Ivan, Eric A. Odell,Eric S. Newkirk,Reesa Y. Conrey, Jennifer Stenglein, Fabiola Iannarilli, John Erb,Ryan K. Brook, Amy J. Davis, Jesse Lewis, Daniel P. Walsh, James C. Beasley, Kurt C. VerCauteren, Jeff Clune, Ryan S. Miller","Motion-activated wildlife cameras (or ""camera traps"") are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the ""species model,"" and one that determines if an image is empty or if it contains an animal, the ""empty-animal model."" Our species model and empty-animal model had accuracies of 96.8% and 97.3%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91% accuracy on species from Canada (accuracy range 36%-91% across all out-of-sample datasets) and the emptyanimal model achieved an accuracy of 91%-94% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in | 10375 TABAK eT Al.",Wildlife image classification dataset,44,1,Wildlife image classification dataset,"Bu, Qiong","The dataset contains two files, each containing classifications collected from the crowd( from FigureEight platform). The files contain image id, species identified by the crowd, and the number of animals of the identified species. It covers the two comparison experiments in chapter 4 and 6 in the thesis Bu (2020) An investigation into the impact of workflow design and aggregation on achieving quality result in crowdsourcing classification tasks, University of Southampton.
"
Image classification,The iWildCam 2021 Competition Dataset,"Sara Beery, Arushi Agarwal, Elijah Cole, Vighnesh Birodkar","Camera traps enable the automatic collection of large quantities of image data. Ecologists use camera traps to monitor animal populations all over the world. In order to estimate the abundance of a species from camera trap data, ecologists need to know not just which species were seen, but also how many individuals of each species were seen. Object detection techniques can be used to find the number of individuals in each image. However, since camera traps collect images in motion-triggered bursts, simply adding up the number of detections over all frames is likely to lead to an incorrect estimate. Overcoming these obstacles may require incorporating spatio-temporal reasoning or individual re-identification in addition to traditional species detection and classification.
We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species and count individual animals across sequences in the test cameras.",Wildlife image classification dataset,44,0,,,
Image classification,Next-Generation Camera Trapping: Systematic Review of Historic Trends Suggests Keys to Expanded Research Applications in Ecology and Conservation,"Zackary J. Delisle, Elizabeth A. Flaherty, Mackenzie R. Nobbe, Cole M. Wzientek, Robert K. Swihart","Camera trapping is an effective non-invasive method for collecting data on wildlife species to address questions of ecological and conservation interest. We reviewed 2,167 camera trap (CT) articles from 1994 to 2020. Through the lens of technological diffusion, we assessed trends in: (1) CT adoption measured by published research output, (2) topic, taxonomic, and geographic diversification and composition of CT applications, and (3) sampling effort, spatial extent, and temporal duration of CT studies. Annual publications of CT articles have grown 81-fold since 1994, increasing at a rate of 1.26 (SE = 0.068) per year since 2005, but with decelerating growth since 2017. Topic, taxonomic, and geographic richness of CT studies increased to encompass 100% of topics, 59.4% of ecoregions, and 6.4% of terrestrial vertebrates. However, declines in per article rates of accretion and plateaus in Shannon's H for topics and major taxa studied suggest upper limits to further diversification of CT research as currently practiced. Notable compositional changes of topics included a decrease in capture-recapture, recent decrease in spatial-capture-recapture, and increases in occupancy, interspecific interactions, and automated image classification. Mammals were the dominant taxon studied; within mammalian orders carnivores exhibited a unimodal peak whereas primates, rodents and lagomorphs steadily increased. Among biogeographic realms we observed decreases in Oceania and Nearctic, increases in Afrotropic and Palearctic, and unimodal peaks for Indomalayan and Neotropic. Camera days, temporal extent, and area sampled increased, with much greater rates for the 0.90 quantile of CT studies compared to the median. Next-generation CT studies are poised to expand knowledge valuable to wildlife ecology and conservation by posing previously infeasible questions at unprecedented spatiotemporal scales, on a greater array of species, and in a wider variety of environments. Converting potential into broad-based application will require transferable models of automated image classification, and data sharing among users across multiple platforms in a coordinated manner. Further taxonomic diversification likely will require technological modifications that permit more efficient sampling of smaller species and adoption of recent improvements in modeling of unmarked populations. Environmental diversification can benefit from engineering solutions that expand ease of CT sampling in traditionally challenging sites.",Wildlife image classification dataset,44,0,,,
Image classification,A systematic literature review on deep learning applications for precision cattle farming,"Md Sultan Mahmud, Azlan Zahid, Anup Kumar Das, Muhammad Muzammil, Muhammad Usman Khan","In animal agriculture, deep learning-based approaches have been widely implemented as a decision support tool for precision farming. Several deep learning models have been applied to solve diverse problems related to cattle health and identification. However, an overview of the state-of-the-art of deep learning in precision cattle farming is needed, for which we performed a systematic literature review (SLR). This study aims to provide an overview of the recent progress in deep learning applications for precision cattle farming, in particular health and identification. In the initial search, we retrieved 678 studies from different electronic databases. Only 56 studies qualify the selection criteria, which were then analyzed to extract the data to answer the research questions. The two major applications of deep learning for cattle farming were identified: identification and health monitoring. About 58% of the selected studies are dedicated to cattle identification and the rest for health monitoring. We identified 20 deep learning models that were used to solve different problems, and Convolutional Neural Networks (CNNs) is the most adopted model than others, including Long Short-Term Memory (LSTM), Mask-Region Based Convolutional Neural Networks (Mask-RCNN), and Faster-RCNN. We identified 19 training networks and of which ResNet is by far the most used. From our selection, 12 model evaluation parameters were determined, of which seven were used more than five times. The challenges most encountered with image quality, data processing speed, dataset size, redundant information, and motion of the cattle during data acquisition. In closing, we consider that this SLR study will pave the way for future research towards developing automatic systems for cattle farming.",Wildlife image classification dataset,44,0,,,
Image classification,ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification,"Peter Kulits, Jake Wall, Anka Bedetti, Michelle Henley, Sara Beery","African elephants are vital to their ecosystems, but their populations are threatened by a rise in human-elephant conflict and poaching. Monitoring population dynamics is essential in conservation efforts; however, tracking elephants is a difficult task, usually relying on the invasive and sometimes dangerous placement of GPS collars. Although there have been many recent successes in the use of computer vision techniques for automated identification of other species, identification of elephants is extremely difficult and typically requires expertise as well as familiarity with elephants in the population. We have built and deployed a web-based platform and database for human-in-the-loop re-identification of elephants combining manual attribute labeling and state-of-the-art computer vision algorithms, known as ElephantBook. Our system is currently in use at the Mara Elephant Project, helping monitor the protected and at-risk population of elephants in the Greater Maasai Mara ecosystem. ElephantBook makes elephant re-identification usable by non-experts and scalable for use by multiple conservation NGOs.",Wildlife image classification dataset,44,0,,,
Image classification,Iterative human and automated identification of wildlife images,"Zhongqi Miao, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S. Palmer, Stella X. Yu & Wayne M. Getz","Camera trapping is increasingly used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has significantly advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static data sets when wildlife data is intrinsically dynamic and involves long-tailed distributions. These two drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning that facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve a ~90% accuracy employing only ~20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative on-going annotation tool that vastly relieves the burden of human annotation and enables efficient and constant model updates.",Wildlife image classification dataset,44,0,,,
Image classification,Outreach increases detections of an invasive species in a crowdsourced monitoring program,"Erin L. Koen, Erica J. Newton","Crowdsourcing can be a useful tool for the early detection of invasive species. Invasive wild pigs (Sus scrofa) have been reported in Ontario, Canada. We compared trends in reporting frequency of wild pig sightings to trends in media events that included directions for the public on where to submit their wild pig sightings. We found that media events occurring on the same week, and in up to two weeks before the sighting was reported significantly increased the number of wild pig reports we received. Our findings suggest that media can be used to increase participation by the public. Because of this relationship, our findings also imply that reporting frequency alone cannot accurately index real changes in wild pig numbers—participants were more likely to report sightings after they had been exposed to relevant media, and spikes in the number of reports tracked our outreach efforts. Despite this limitation, reports of wild pig sightings from community members remain a cost-effective tool to detect low-density invasive species across large regions, and participation in the program can be increased with periodic news media and social media blitzes.",Wildlife image classification dataset,44,0,,,
Image classification,Automated classification of bat echolocation call recordings with artificial intelligence,"Michael A.Tabak, Kevin L.Murray, Ashley M.Reed, John A.Lombardi, Kimberly J.Bay","Acoustic recorders are commonly used to remotely monitor and collect data on bats (Order Chiroptera). These efforts result in many acoustic recordings that must be classified by a bat biologist with expertise in call classification in order to obtain useful information. The rarity of this expertise and time constraints have prompted efforts to automatically classify bat species in acoustic recordings using a variety of learning methods. There are several software programs available for this purpose, but they are imperfect and the United States Fish and Wildlife Service often recommends that a qualified acoustic analyst review bat call identifications even if using these software programs. We sought to build a model to classify bat species using modern computer vision techniques. We used images of bat echolocation calls (i.e., plots of the pulses) to train deep learning computer vision models that automatically classify bat calls to species. Our model classifies 10 species, five of which are protected under the Endangered Species Act. We evaluated our models using standard model validation procedures, and performed two external tests. For these tests, an entire dataset was withheld from the procedure before splitting the data into training and validation sets. We found that our validation accuracy (92%) and testing accuracy (90%) were higher than when we used Kaleidoscope Pro and BCID software (65% and 61% accuracy, respectively) to evaluate the same calls. Our results suggest that our approach is effective at classifying bat species from acoustic recordings, and our trained model will be incorporated into new bat call identification software: WEST-EchoVision.

",Wildlife image classification dataset,44,0,,,
Image classification,Domain-Aware Neural Architecture Search for Classifying Animals in Camera Trap Images,"Liang Jia,Ye Tian, Junguo Zhang","Camera traps acquire visual data in a non-disturbing and round-the-clock manner, so they are popular for ecological researchers observing wildlife. Each camera trap may record thousands of images of diverse species and bring about millions of images that need to be classified. Many methods have been proposed to classify camera trap images, but almost all methods rely on very deep convolutional neural networks that require intensive computational resources. Such resources may be unavailable and become formidable in cases where the surveillance area is large or becomes greatly expanded. We turn our attention to camera traps organized as groups, where each group produces images that are processed by the edge device with lightweight networks tailored for images produced by the group. To achieve this goal, we propose a method to automatically design networks deployable for edge devices with respect to given images. With the proposed method, researchers without any experience in designing neural networks can develop networks applicable for edge devices. Thus, camera trap images can be processed in a distributed manner through edge devices, lowering the costs of transferring and processing data accumulated at camera traps.",Wildlife image classification dataset,44,0,,,
Image classification,An IoT System Using Deep Learning to Classify Camera Trap Images on the Edge,"Imran Zualkernan,Salam Dhou, Jacky Judas, Ali Reza Sajun, Brylle Ryan Gomez,Lana Alhaj Hussain","Camera traps deployed in remote locations provide an effective method for ecologists to monitor and study wildlife in a non-invasive way. However, current camera traps suffer from two problems. First, the images are manually classified and counted, which is expensive. Second, due to manual coding, the results are often stale by the time they get to the ecologists. Using the Internet of Things (IoT) combined with deep learning represents a good solution for both these problems, as the images can be classified automatically, and the results immediately made available to ecologists. This paper proposes an IoT architecture that uses deep learning on edge devices to convey animal classification results to a mobile app using the LoRaWAN low-power, wide-area network. The primary goal of the proposed approach is to reduce the cost of the wildlife monitoring process for ecologists, and to provide real-time animal sightings data from the camera traps in the field. Camera trap image data consisting of 66,400 images were used to train the InceptionV3, MobileNetV2, ResNet18, EfficientNetB1, DenseNet121, and Xception neural network models. While performance of the trained models was statistically different (Kruskal–Wallis: Accuracy H(5) = 22.34, p < 0.05; F1-score H(5) = 13.82, p = 0.0168), there was only a 3% difference in the F1-score between the worst (MobileNet V2) and the best model (Xception). Moreover, the models made similar errors (Adjusted Rand Index (ARI) > 0.88 and Adjusted Mutual Information (AMU) > 0.82). Subsequently, the best model, Xception (Accuracy = 96.1%; F1-score = 0.87; F1-Score = 0.97 with oversampling), was optimized and deployed on the Raspberry Pi, Google Coral, and Nvidia Jetson edge devices using both TenorFlow Lite and TensorRT frameworks. Optimizing the models to run on edge devices reduced the average macro F1-Score to 0.7, and adversely affected the minority classes, reducing their F1-score to as low as 0.18. Upon stress testing, by processing 1000 images consecutively, Jetson Nano, running a TensorRT model, outperformed others with a latency of 0.276 s/image (s.d. = 0.002) while consuming an average current of 1665.21 mA. Raspberry Pi consumed the least average current (838.99 mA) with a ten times worse latency of 2.83 s/image (s.d. = 0.036). Nano was the only reasonable option as an edge device because it could capture most animals whose maximum speeds were below 80 km/h, including goats, lions, ostriches, etc. While the proposed architecture is viable, unbalanced data remain a challenge and the results can potentially be improved by using object detection to reduce imbalances and by exploring semi-supervised learning.
",Wildlife image classification dataset,44,0,,,
Image classification,Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intelligence,"Juliana Vélez, Paula J. Castiblanco-Camacho, Michael A. Tabak, Carl Chalmers, Paul Fergus, John Fieberg","Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., ""blank"" or ""animal""), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks.",Wildlife image classification dataset,44,0,,,
Image classification,CNN Classification of the Cultural Heritage Images,"Marijana Ćosović, Radmila Janković","The cultural heritage image classification represents one of the most important tasks in the process of digitalization. In this paper, a deep learning neural network was applied in order to classify images of architectural heritage belonging to ten categories, in particular: (i) bell tower, (ii) stained glass, (iii) vault, (iv) column, (v) outer dome, (vi) altar, (vii) apse, (viii) inner dome, (ix) flying buttress, and (x) gargoyle. The Convolutional neural network was used for image classification, with the same architecture applied on two sets of the data: the full dataset consisting of 10 categories as well as dataset with 5 different image categories. The results show that both architectures performed well and obtained accuracy of up to 90%.",Architectural Heritage Elements image Dataset,45,1,Architectural Heritage Elements image Dataset,Ivan Kobzev,Architectural Heritage Elements Dataset (AHE) is an image dataset for developing deep learning algorithms and specific techniques in the classification of architectural heritage images. This dataset consists of 10235 images classified in 10 categories: Altar: 829 images; Apse: 514 images; Bell tower: 1059 images; Column: 1919 images; Dome (inner): 616 images; Dome (outer): 1177 images; Flying buttress: 407 images; Gargoyle (and Chimera): 1571 images; Stained glass: 1033 images; Vault: 1110 images. It is inspired by the CIFAR-10 dataset but with the objective in mind of developing tools that facilitate the tasks of classifying images in the field of cultural heritage documentation. Most of the images have been obtained from Flickr and Wikimedia Commons (all of them under creative commons license).
Image classification,Deep Learning dalam Mengindetifikasi Jenis Bangunan Heritage dengan Algoritma Convolutional Neural Network,"Sri Winiarti, Mochammad Yulianto Andi Saputro, Sunardi Sunardi","A heritage building is a building that has a distinctive style or tradition from a culture whose activities are carried out continuously until now and are used as a characteristic of that culture. The problems that occur in the community are the lack of knowledge to recognize the types of heritage buildings and the lack of digital documentation. Another problem that occurs in identifying heritage buildings is that there are similarities between heritage buildings and new buildings that imitate the architectural style of heritage buildings from ornaments. This can raise doubts in the information related to the original history of heritage buildings for the public or visitors. This study aims to apply the Convolutional Neural Network (CNN) to identify the types of heritage buildings. The benefits of this research can be found in the characteristics of a building based on ornaments so that it can be used to obtain information about the types of heritage buildings in Indonesia. A dataset of 7184 images of ornaments from heritage buildings were used which were taken directly at the Yogyakarta location, namely; Mataram Grand Mosque, Taqwa Wonokromo Mosque, Kalang House, Joglo KH Ahmad Dahlan and Ketandan. It is necessary to identify the heritage building because the object of the building can become extinct at any time, so to maintain it, documentation is needed as an effort to preserve culture and for education. Based on the evaluation of the performance of the tests carried out using the confusion matrix method from 391 ornamental images, the results obtained are 98% accuracy",Architectural Heritage Elements image Dataset,45,0,,,
Image classification,Cultural Heritage Image Classification,"Marijana Cosovic, Radmila Jankovic, Belma Ramic-Brkic","Image classification in cultural heritage context represents one of the most important tasks in the process of digitalization. In these terms, classification can be particularly challenging due to a high number of different image categories, feature variability, and the need for high reliability. Recent research shows that various machine learning techniques can be utilized for image classification purposes and that algorithms such as artificial neural networks, decision trees, and support vector machines are able to obtain high performances. This chapter explores the deep learning architectures used for classification models. Furthermore, we are conducting research on the image classification of Eastern Orthodox cultural heritage, which may assist in the future process of digitalization. In particular, we created a dataset, as such to our knowledge does not exist, containing images of Eastern Orthodox cultural heritage, namely frescoes and sacral objects. The dataset is available for the public, and it represents an additional novelty of this research. Different classification methods are applied to the dataset with the aim of finding the most suitable configuration that will yield high classification performance.",Architectural Heritage Elements image Dataset,45,0,,,
Image classification,An AI Lens on Historic Cairo,"Islam Zohier, Ahmed El Antably, Ahmed S. Madani","Reports show that numerous heritage sites are in danger due to conflicts and heritage
mismanagement in many parts of the world. Experts have resorted to digital tools to
attempt to conserve and preserve endangered and damaged sites. To that end, in this
applied research, we aim to develop a deep learning framework applied to the decaying
tangible heritage of Historic Cairo, known as “The City of a Thousand Minarets.” The
proposed framework targets Cairo’s historic minaret styles as a test case study for the
broader applications of deep learning in digital heritage. It comprises recognition and
segmentation tasks, which use a deep learning semantic segmentation model trained
on two data sets representing the two most dominant minaret styles in the city, Mamluk
(1250–1517 CE) and Ottoman (1517–1952 CE). The proposed framework aims to classify these two types using images. It can help create a multidimensional model from just
a photograph of a historic building, which can quickly catalog and document a historic
building or element. The study also sheds light on the obstacles preventing the exploration
and implementation of deep learning techniques in digital heritage. The research presented
in this paper is a work-in-progress of a larger applied research concerned with implementing deep learning techniques in the digital heritage domain",Architectural Heritage Elements image Dataset,45,0,,,
Image classification,Visual Classification of Intangible Cultural Heritage Images in the Mekong Delta,"Thanh-Nghi Do, The-Phi Pham, Huu-Hoa Nguyen, Nguyen-Khang Pham","Our investigation aims to classify Intangible Cultural Heritage (ICH) images in the Mekong Delta, Vietnam. To pursue this goal, we collect an images dataset of 17 ICH categories and manually annotate them. We start with fine-tuning recent pre-trained deep learning models such as VGG19, ResNet50, Inception-v3, and Xception for classifying our own dataset. Followed which, we propose to train support vector machine (SVM) models using many popular visual features including the handcrafted features such as the scale-invariant feature transform (SIFT) and the bag-of-words (BoW) model, the histogram of oriented gradients (HOG), the GIST, and the automated deep learning of invariant features extracted by VGG19, ResNet50, Inception-v3, and Xception. The comparative study of the classification for the images dataset of 17 ICH categories shows that fine-tuning pre-trained deep learning models and SVM models using automated deep learning of invariant features outperform SVM models trained on handcrafted features. Fine-tuning Inception-v3, Xception, and two non-linear SVM models learned from Inception-v3 and Xception features achieve 60.46%, 61.54%, 61.54%, and 62.89% accuracy, respectively. We propose to combine non-linear SVM models using different visual features to improve the classification result performed by any single one. Both the triplets SVM-Xception, SVM-Inception-v3, and SVM-VGG19 and SVM-Xception, SVM-Inception-v3, and SVM-SIFT-BoW achieve equal accuracy of 65.32%.",Architectural Heritage Elements image Dataset,45,0,,,
Image classification,Digitization of the Museum Materials of the „Museum Herzegovina Trebinje,"Mirjana Jokanović Đajić, Danijel Mijic, Mirjana Miljanovic, Srđan Ćurić","The paper presents the implementation and results of the project ""Digitization of museum
material of the Museum of Herzegovina Trebinje"", started in 2019, with the aim to show all the
possibilities that digitization provides and relates to the manipulation, use, distribution, and especially
storage of material for the future. Given the large amount of material that makes up the cultural, historical
and scientific heritage, and kept in the Museum, it was necessary to approach the process of protection of
the original, and with the help of digital photocopies to improve the availability of material. Digitized
objects are mostly three-dimensional objects for the digitization of which a 3D scanner with
accompanying equipment and software was used. Based on the results of the research, it's assumed that
access to and use of the Museum's collection will increase, valuable specimens will be protected and
easier use of frequently used material will be enabled.",Architectural Heritage Elements image Dataset,45,0,,,
Image classification,Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection,"Fan Yang, Lei Zhang, Sijia Yu, Danil Prokhorov, Xue Mei, Haibin Ling","Pavement crack detection is a critical task for insuring road safety. Manual crack detection is extremely time-consuming. Therefore, an automatic road crack detection method is required to boost this progress. However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavements and possible shadows with similar intensity. Inspired by recent advances of deep learning in computer vision, we propose a novel network architecture, named Feature Pyramid and Hierarchical Boosting Network (FPHBN), for pavement crack detection. The proposed network integrates semantic information to low-level features for crack detection in a feature pyramid way. And, it balances the contribution of both easy and hard samples to loss by nested sample reweighting in a hierarchical way. To demonstrate the superiority and generality of the proposed method, we evaluate the proposed method on five crack datasets and compare it with state-of-the-art crack detection, edge detection, semantic segmentation methods. Extensive experiments show that the proposed method outperforms these state-of-the-art methods in terms of accuracy and generality.",CRACK500,46,1,CRACK500,"Fan Yang, Lei Zhang, Sijia Yu, Danil Prokhorov, Xue Mei, Haibin Ling  
"," Pavement crack detection is a critical task for insuring road safety. Manual crack detection is extremely time-consuming. Therefore, an automatic road crack detection method is required to boost this progress. However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavements and possible shadows with similar intensity. Inspired by recent advances of deep learning in computer vision, we propose a novel network architecture, named Feature Pyramid and Hierarchical Boosting Network (FPHBN), for pavement crack detection. The proposed network integrates semantic information to low-level features for crack detection in a feature pyramid way. And, it balances the contribution of both easy and hard samples to loss by nested sample reweighting in a hierarchical way. To demonstrate the superiority and generality of the proposed method, we evaluate the proposed method on five crack datasets and compare it with state-of-the-art crack detection, edge detection, semantic segmentation methods. Extensive experiments show that the proposed method outperforms these state-of-the-art methods in terms of accuracy and generality.      
Source:https://github.com/fyangneil/pavement-crack-detection"
Image classification,A review of computer vision–based structural health monitoring at local and global levels,"Chuan-Zhi Dong, F Necati Catbas","Structural health monitoring at local and global levels using computer vision technologies has gained much attention in the structural health monitoring community in research and practice. Due to the computer vision technology application advantages such as non-contact, long distance, rapid, low cost and labor, and low interference to the daily operation of structures, it is promising to consider computer vision–structural health monitoring as a complement to the conventional structural health monitoring. This article presents a general overview of the concepts, approaches, and real-life practice of computer vision–structural health monitoring along with some relevant literature that is rapidly accumulating. The computer vision–structural health monitoring covered in this article at local level includes applications such as crack, spalling, delamination, rust, and loose bolt detection. At the global level, applications include displacement measurement, structural behavior analysis, vibration serviceability, modal identification, model updating, damage detection, cable force monitoring, load factor estimation, and structural identification using input–output information. The current research studies and applications of computer vision–structural health monitoring mainly focus on the implementation and integration of two-dimensional computer vision techniques to solve structural health monitoring problems and the projective geometry methods implemented are utilized to convert the three-dimensional problems into two-dimensional problems. This review mainly puts emphasis on two-dimensional computer vision–structural health monitoring applications. Subsequently, a brief review of representative developments of three-dimensional computer vision in the area of civil engineering is presented along with the challenges and opportunities of two-dimensional and three-dimensional computer vision–structural health monitoring. Finally, the article presents a forward look to the future of computer vision–structural health monitoring.

",CRACK500,46,0,,,
Image classification,PGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection,"Hongwen Dong, Kechen Song, Yu He, Jing Xu, Yunhui Yan, Qinggang Meng","Surface defect detection is a critical task in industrial production process. Nowadays, there are lots of detection methods based on computer vision and have been successfully applied in industry, they also achieved good results. However, achieving full automation of surface defect detection remains a challenge, due to the complexity of surface defect, in intraclass. While the defects between interclass contain similar parts, there are large differences in appearance of the defects. To address these issues, this article proposes a pyramid feature fusion and global context attention network for pixel-wise detection of surface defect, called PGA-Net. In the framework, the multiscale features are extracted at first from backbone network. Then the pyramid feature fusion module is used to fuse these features into five resolutions through some efficient dense skip connections. Finally, the global context attention module is applied to the fusion feature maps of adjacent resolution, which allows effective information propagate from low-resolution fusion feature maps to high-resolution fusion ones. In addition, the boundary refinement block is added to the framework to refine the boundary of defect and improve the result of the prediction. The final prediction is the fusion of the five resolutions fusion feature maps. The results of evaluation on four real-world defect datasets demonstrate that the proposed method outperforms the state-of-the-art methods on mean intersection of union and mean pixel accuracy (NEU-Seg: 82.15%, DAGM 2007: 74.78%, MT_defect: 71.31%, Road_defect: 79.54%).
",CRACK500,46,0,,,
Image classification,Review of Pavement Defect Detection Methods,"Wenming Cao, Qifan Liu, Zhiquan He","Road pavement cracks detection has been a hot research topic for quite a long time due to the practical importance of crack detection for road maintenance and traffic safety. Many methods have been proposed to solve this problem. This paper reviews the three major types of methods used in road cracks detection: image processing, machine learning and 3D imaging based methods. Image processing algorithms mainly include threshold segmentation, edge detection and region growing methods, which are used to process images and identify crack features. Crack detection based traditional machine learning methods such as neural network and support vector machine still relies on hand-crafted features using image processing techniques. Deep learning methods have fundamentally changed the way of crack detection and greatly improved the detection performance. In this work, we review and compare the deep learning neural networks proposed in crack detection in three ways, classification based, object detection based and segmentation based. We also cover the performance evaluation metrics and the performance of these methods on commonly-used benchmark datasets. With the maturity of 3D technology, crack detection using 3D data is a new line of research and application. We compare the three types of 3D data representations and study the corresponding performance of the deep neural networks for 3D object detection. Traditional and deep learning based crack detection methods using 3D data are also reviewed in detail.
",CRACK500,46,0,,,
Image classification,A cost effective solution for pavement crack inspection using cameras and deep neural networks,"Qipei Mei, Mustafa Gül","Automatic crack detection on pavement surfaces is an important research field in the scope of developing an intelligent transportation infrastructure system. In this paper, a cost effective solution for road crack inspection by mounting the commercial grade sport camera, GoPro, on the rear of the moving vehicle is introduced. Also, a novel method called ConnCrack combining conditional Wasserstein generative adversarial network and connectivity maps is proposed for road crack detection. In this method, a 121-layer densely connected neural network with deconvolution layers for multi-level feature fusion is used as generator, and a 5-layer fully convolutional network is used as discriminator. To overcome the scattered output issue related to deconvolution layers, connectivity maps are introduced to represent the crack information within the proposed ConnCrack. The proposed method is tested on a publicly available dataset as well our collected data. The results show that the proposed method achieves state-of-the-art performance compared with other existing methods in terms of precision, recall and F1 score.",CRACK500,46,0,,,
Image classification,Automated pavement crack detection and segmentation based on two-step convolutional neural network,"Jingwei Liu, Xu Yang, Stephen Lau, Xin Wang, Sang Luo, Vincent Cheng-Siong Lee, Ling Ding","Cracking is a common pavement distress that would cause further severe problems if not repaired timely, which means that it is important to accurately extract the information of pavement cracks through detection and segmentation. Automated pavement crack detection and segmentation using deep learning are more efficient and accurate than conventional methods, which could be further improved. While many existing studies have utilized deep learning in pavement crack segmentation, which segments cracks from non-crack regions, few studies have taken the exact pavement crack detection into account, which identifies cracks in the images from other objects. A two-step pavement crack detection and segmentation method based on convolutional neural network was proposed in this paper. An automated pavement crack detection algorithm was developed using the modified You Only Look Once 3rd version in the first step. The proposed crack segmentation method in the second step was based on the modified U-Net, whose encoder was replaced with a pre-trained ResNet-34 and the up-sample part was added with spatial and channel squeeze and excitation (SCSE) modules. Proposed method combines pavement crack detection and segmentation together, so that the detected cracks from the first step are segmented in the second step to improve the accuracy. A dataset of pavement crack images in different circumstances were also built for the study. The F1 score of proposed crack detection and segmentation methods are 90.58% and 95.75%, respectively, which are higher than other state-of-the-art methods. Compared with existing one-step pavement crack detection or segmentation methods, proposed two-step method showed advantages of accuracy.",CRACK500,46,0,,,
Image classification,Machine Learning for Crack Detection: Review and Model Performance Comparison,"Yung-An Hsieh, Yichang James Tsai,","With the advancement of machine learning (ML) and deep learning (DL), there is a great opportunity to enhance the development of automatic crack detection algorithms. In this paper, the authors organize and provide up-to-date information on on ML-based crack detection algorithms for researchers to more efficiently seek potential focus and direction. The authors first reviewed 68 ML-based crack detection methods to identify the current trend of development, pixel-level crack segmentation. The authors then conducted a performance evaluation on 8 ML-based crack segmentation models using consistent evaluation metrics and three-dimensional (3D) pavement images with diverse conditions to identify remaining challenges and potential directions for future development. Based on the comparison results, deeper backbone networks in FCN models and skip connections in U-Net both improved the performance. Within different categories of pavement images, except for the Other Distress category, FCN and U-Net scored over 90 on the enhanced Hausdorff distance metric. Results showed that solving the false-positive problem is an important step in further improving ML-based crack detection models.",CRACK500,46,0,,,
Image classification,Densely connected deep neural network considering connectivity of pixels for automatic crack detection,"Qipei Mei, Mustafa Gül, Md Riasat Azim","In order to develop smart cities, the demand for assessing the condition of existing infrastructure systems in an automated manner is burgeoning rapidly. Among all the early signs of potential damage in infrastructure systems, formation of cracks is a critical one because it is directly related to the structural capacity and could significantly affect the serviceability of the infrastructure. This paper proposes a novel deep learning-based method considering the connectivity of pixels for automatic pavement crack detection which has the potential to complement the current practice involving visual inspection which is costly, inefficient and time-consuming. In the proposed method, the convolutional layers are densely connected in a feed-forward fashion to reuse features from multiple layers, and transposed convolution layers are used for multiple level feature fusion. A novel loss function considering the connectivity of pixels is introduced to overcome the issues related to the output of transposed convolution layers. The proposed method is tested on two datasets, where the first one is collected from a handheld smartphone and the second one is collected from a high-speed camera mounted on the rear of a moving car. In both datasets, the proposed method shows superior performance than other available methods.",CRACK500,46,0,,,
Image classification,Automated Pavement Crack Segmentation Using U-Net-Based Convolutional Neural Network,"Stephen L. H. Lau, Edwin K. P. Chong, Xu Yang, Xin Wang","Automated pavement crack image segmentation is challenging because of inherent irregular patterns, lighting conditions, and noise in images. Conventional approaches require a substantial amount of feature engineering to differentiate crack regions from non-affected regions. In this paper, we propose a deep learning technique based on a convolutional neural network to perform segmentation tasks on pavement crack images. Our approach requires minimal feature engineering compared to other machine learning techniques. We propose a U-Net-based network architecture in which we replace the encoder with a pretrained ResNet-34 neural network. We use a ""one-cycle"" training schedule based on cyclical learning rates to speed up the convergence. Our method achieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset, outperforming other algorithms tested on these datasets. We perform ablation studies on various techniques that helped us get marginal performance boosts, i.e., the addition of spatial and channel squeeze and excitation (SCSE) modules, training with gradually increasing image sizes, and training various neural network layers with different learning rates.",CRACK500,46,0,,,
Image classification,CrackGAN: Pavement Crack Detection Using Partially Accurate Ground Truths Based on Generative Adversarial Learning,"Kaige Zhang, Yingtao Zhang, Heng-Da Cheng","Fully convolutional network is a powerful tool for per-pixel semantic segmentation/detection. However, it is problematic when coping with crack detection using partially accurate ground truths (GTs): the network may easily converge to the status that treats all the pixels as background (BG) and still achieves a very good loss, named “All Black” phenomenon, due to the unavailability of accurate GTs and the data imbalance. To tackle this problem, we propose crack-patch-only (CPO) supervised generative adversarial learning for end-to-end training, which forces the network to always produce crack-GT images while reserves both crack and BG-image translation abilities by feeding a larger-size crack image into an asymmetric U-shape generator to overcome the “All Black” issue. The proposed approach is validated using four crack datasets; and achieves state-of-the-art performance comparing with that of the recently published works in efficiency and accuracy.",CRACK500,46,0,,,
Image classification,Automatic Crack Detection on Road Pavements Using Encoder-Decoder Architecture,"Zhun Fan, Chong Li, Ying Chen, Jiahong Wei, Giuseppe Loprencipe, Xiaopeng Chen, Paola Di Mascio","Automatic crack detection from images is an important task that is adopted to ensure road safety and durability for Portland cement concrete (PCC) and asphalt concrete (AC) pavement. Pavement failure depends on a number of causes including water intrusion, stress from heavy loads, and all the climate effects. Generally, cracks are the first distress that arises on road surfaces and proper monitoring and maintenance to prevent cracks from spreading or forming is important. Conventional algorithms to identify cracks on road pavements are extremely time-consuming and high cost. Many cracks show complicated topological structures, oil stains, poor continuity, and low contrast, which are difficult for defining crack features. Therefore, the automated crack detection algorithm is a key tool to improve the results. Inspired by the development of deep learning in computer vision and object detection, the proposed algorithm considers an encoder-decoder architecture with hierarchical feature learning and dilated convolution, named U-Hierarchical Dilated Network (U-HDN), to perform crack detection in an end-to-end method. Crack characteristics with multiple context information are automatically able to learn and perform end-to-end crack detection. Then, a multi-dilation module embedded in an encoder-decoder architecture is proposed. The crack features of multiple context sizes can be integrated into the multi-dilation module by dilation convolution with different dilatation rates, which can obtain much more cracks information. Finally, the hierarchical feature learning module is designed to obtain a multi-scale features from the high to low- level convolutional layers, which are integrated to predict pixel-wise crack detection. Some experiments on public crack databases using 118 images were performed and the results were compared with those obtained with other methods on the same images. The results show that the proposed U-HDN method achieves high performance because it can extract and fuse different context sizes and different levels of feature maps than other algorithms. View Full-Text
",CRACK500,46,0,,,
Image classification,CoastSat: A Google Earth Engine-enabled Python toolkit to extract shorelines from publicly available satellite imagery,"Kilian Vos, Kristen D.Splinter, Mitchell D.Harley, Joshua A.Simmons, Ian L.Turner","CoastSat is an open-source software toolkit written in Python that enables the user to obtain time-series of shoreline position at any sandy coastline worldwide from 30+ years (and growing) of publicly available satellite imagery. The toolkit exploits the capabilities of Google Earth Engine to efficiently retrieve Landsat and Sentinel-2 images cropped to any user-defined region of interest. The resulting images are pre-processed to remove cloudy pixels and enhance spatial resolution, before applying a robust and generic shoreline detection algorithm. This novel shoreline detection technique combines a supervised image classification and a sub-pixel resolution border segmentation to map the position of the shoreline with an accuracy of ~10 m. The purpose of CoastSat is to provide coastal managers, engineers and scientists a user-friendly and practical toolkit to monitor and explore their coastlines. The software is freely-available on GitHub (https://github.com/kvos/CoastSat) and is accompanied by guided examples (Jupyter Notebook) plus step-by-step README documentation.

",CoastSat image classification,47,1,CoastSat image classification,Kilian Vos,"CoastSat image classification training data

CoastSat is an open-source global shoreline mapping toolbox, available at https://github.com/kvos/CoastSat, which enables users to extract time-series of shoreline change from 30+ years of publicly available satellite imagery (Landsat 5, 7, 8 and Sentinel-2).

The automated shoreline extraction relies on a classifier (Multilayer Perceptron from scikit-learn) which labels each pixels on the images with one of four classes: sand, water, white-water and other land features.

The data used to train the classifier is stored here, the README.md file provides information on the data organisation and content of each file."
Image classification,Sub-annual to multi-decadal shoreline variability from publicly available satellite imagery,"KilianVos, Mitchell D.Harley, Kristen D.Splinter, Joshua A.SimmonsIan, L.Turner","The ability to repeatedly observe and quantify the changing position of the shoreline is key to present-day coastal management and future coastal planning. This study evaluates the capability of satellite remote sensing to resolve at differing temporal scales the variability and trends in shoreline position along sandy coastlines. Shorelines are extracted from 30 + years of publicly available satellite imagery and compared to long-term in-situ measurements at 5 diverse test sites in Europe, Australia, the USA and New Zealand. These sites span a range of different beach characteristics including wave energy and tide range as well as timescales of observed shoreline variability, from strongly seasonal (e.g., Truc Vert, France), to storm-dominated (e.g., Narrabeen-Collaroy, Australia), to only minor annual to multi-annual signals (e.g., Duck, USA). For the 5 sites, the observed typical horizontal errors varied between a root-mean-squared error (RMSE) of 7.3 m and 12.7 m. An analysis of the typical magnitudes of shoreline variability at temporal scales ranging from a single month up to 10 years indicates that, by the implementation of targeted image pre-processing then the application of a robust sub-pixel shoreline extraction technique, the resulting satellite-derived shorelines are generally able to resolve (signal-to-noise ratio > 1) the observed shoreline variance at timescales of 6 months and longer. The only exception to this is along coastlines where minimal annual to multi-annual shoreline variability occurs (e.g. Duck, USA); at these sites decadal-scale variations are successfully captured. The results of this analysis demonstrate that satellite-derived shorelines spanning the past 30 years as well as into the future can be used to explore and quantify intra- and inter-annual shoreline behaviour at a wide range of beaches around the world. Moreover, it is demonstrated that present-day satellite observations are also capable of capturing event-scale shoreline changes (e.g. individual storms) that occur at timescales shorter than 6 months, where this rapid response exceeds the typical magnitude of shoreline variability. Finally, several practical coastal engineering applications are presented, demonstrating the use of freely-available satellite imagery to monitor inter-annual embayed beach rotation, rapid storm-induced shoreline retreat and a major sand nourishment.

",CoastSat image classification,47,0,,,
Image classification,Beach Slopes From Satellite-Derived Shorelines,"Kilian Vos, Mitchell D. Harley, Kristen D. Splinter, Andrew Walker, Ian L. Turner","The steepness of the beach face is a fundamental parameter for coastal morphodynamic research. Despite its importance, it remains extremely difficult to obtain reliable estimates of the beach-face slope over large spatial scales (thousands of km of coastline). In this letter, a novel approach to estimate this slope from time series of satellite-derived shoreline positions is presented. This new technique uses a frequency domain analysis to find the optimum slope that minimizes high-frequency tidal fluctuations relative to lower-frequency erosion/accretion signals. A detailed assessment of this new approach at eight locations spanning a range of tidal regimes, wave climates, and sediment grain sizes shows strong agreement (R2 = 0.93) with field measurements. The automated technique is then applied across thousands of beaches in eastern Australia and California, USA, revealing similar regional-scale distributions along these two contrasting coastlines and highlights the potential for new global-scale insight to beach-face slope spatial distribution, variability, and trends.",CoastSat image classification,47,0,,,
Image classification,Sub-Pixel Waterline Extraction: Characterising Accuracy and Sensitivity to Indices and Spectra,"Robbi Bishop-Taylor, Stephen Sagar, Leo Lymburner, Imam Alam, Joshua Sixsmith","Accurately mapping the boundary between land and water (the ‘waterline’) is critical for tracking change in vulnerable coastal zones, and managing increasingly threatened water resources. Previous studies have largely relied on mapping waterlines at the pixel scale, or employed computationally intensive sub-pixel waterline extraction methods that are impractical to implement at scale. There is a pressing need for operational methods for extracting information from freely available medium resolution satellite imagery at spatial scales relevant to coastal and environmental management. In this study, we present a comprehensive evaluation of a promising method for mapping waterlines at sub-pixel accuracy from satellite remote sensing data. By combining a synthetic landscape approach with high resolution WorldView-2 satellite imagery, it was possible to rapidly assess the performance of the method across multiple coastal environments with contrasting spectral characteristics (sandy beaches, artificial shorelines, rocky shorelines, wetland vegetation and tidal mudflats), and under a range of water indices (Normalised Difference Water Index, Modified Normalised Difference Water Index, and the Automated Water Extraction Index) and thresholding approaches (optimal, zero and automated Otsu’s method). The sub-pixel extraction method shows a strong ability to reproduce both absolute waterline positions and relative shape at a resolution that far exceeds that of traditional whole-pixel methods, particularly in environments without extreme contrast between the water and land (e.g., accuracies of up to 1.50–3.28 m at 30 m Landsat resolution using optimal water index thresholds). We discuss key challenges and limitations associated with selecting appropriate water indices and thresholds for sub-pixel waterline extraction, and suggest future directions for improving the accuracy and reliability of extracted waterlines. The sub-pixel waterline extraction method has a low computational overhead and is made available as an open-source tool, making it suitable for operational continental-scale or full time-depth analyses aimed at accurately mapping and monitoring dynamic waterlines through time and space. View Full-Text
",CoastSat image classification,47,0,,,
Image classification,Applications of Google Earth Engine in fluvial geomorphology for detecting river channel change,"Richard J. Boothroyd, Richard D. Williams, Trevor B. Hoey, Brian Barrett, Octria A. Prasojo","Cloud-based computing, access to big geospatial data, and virtualization, whereby users are freed from computational hardware and data management logistics, could revolutionize remote sensing applications in fluvial geomorphology. Analysis of multitemporal, multispectral satellite imagery has provided fundamental geomorphic insight into the planimetric form and dynamics of large river systems, but information derived from these applications has largely been used to test existing concepts in fluvial geomorphology, rather than for generating new concepts or theories. Traditional approaches (i.e., desktop computing) have restricted the spatial scales and temporal resolutions of planimetric river channel change analyses. Google Earth Engine (GEE), a cloud-based computing platform for planetary-scale geospatial analyses, offers the opportunity to relieve these spatiotemporal restrictions. We summarize the big geospatial data flows available to fluvial geomorphologists within the GEE data catalog, focus on approaches to look beyond mapping wet channel extents and instead map the wider riverscape (i.e., water, sediment, vegetation) and its dynamics, and explore the unprecedented spatiotemporal scales over which GEE analyses can be applied. We share a demonstration workflow to extract active river channel masks from a section of the Cagayan River (Luzon, Philippines) then quantify centerline migration rates from multitemporal data. By enabling fluvial geomorphologists to take their algorithms to petabytes worth of data, GEE is transformative in enabling deterministic science at scales defined by the user and determined by the phenomena of interest. Equally as important, GEE offers a mechanism for promoting a cultural shift toward open science, through the democratization of access and sharing of reproducible code.

",CoastSat image classification,47,0,,,
Image classification,SWOT-AHP analysis of the Korean satellite and space industry: Strategy recommendations for development,"Junho Lee, Ikjun Kim, Hyomin Kim, Juyoung Kang","The global satellite industry is growing in the private market with the advent of SpaceX Corp., Planet Labs Corp., and so forth. Following this trend, the Korean government established the National Space Commission to construct a space development plan that encourages private companies’ participation. Korea has the potential to create various satellite services based on its information technology and semiconductor industries. Thus, this study aims to present strategies to accelerate the growth of the satellite industry by considering various internal and external factors, such as the limitations and strengths of the Korean space and satellite industry. This study employs the SWOT-AHP method, to measure these items factors priorities’ and suggest in-depth strategies. The SWOT matrix is constructed based on prior research. Then, an AHP survey of 32 experts is conducted, and strength-threat and strength-opportunity strategies are developed using the top eight relatively important items. The analysis finds that opportunity items have high priorities. Thus, a strategy is devised to meet demand in the public sector and vitalize private businesses. Lastly, topics are collected using latent Dirichlet allocation topic modeling to identify the media's perspective, which we find is similar to the opinions of the experts chosen for this study.

",CoastSat image classification,47,0,,,
Image classification,AgKit4EE: A toolkit for agricultural land use modeling of the conterminous United States based on Google Earth Engine,"Chen Zhang, Liping Di, Zhengwei Yang, Li Lin, Pengyu Haoa","Google Earth Engine (GEE) is an ideal platform for large-scale geospatial agricultural and environmental modeling based on its diverse geospatial datasets, easy-to-use application programming interface (API), rich reusable library, and high-performance computational capacity. However, using GEE to prepare geospatial data requires not only the skills of programming languages like JavaScript and Python, but also the knowledge of GEE APIs and data catalog. This paper presents the AgKit4EE toolkit to facilitate the use of the Cropland Data Layer (CDL) product over the GEE platform. This toolkit contains a variety of frequently used functions for use of CDL including crop sequence modeling, crop frequency modeling, confidence layer modeling, and land use change analysis. The experimental results suggest that the proposed software can significantly reduce the workload for modelers who conduct geospatial agricultural and environmental modeling with CDL data as well as developers who build the GEE-enabled geospatial cyberinfrastructure for agricultural land use modeling of the conterminous United States. AgKit4EE is an open source and it is free to use, modify, and distribute. The latest release of AgKit4EE can be imported to any modeling workflow developed using GEE Code Editor (https://code.earthengine.google.com/?accept_repo=users/czhang11/agkit4ee). The source code, examples, documentation, user community, and wiki pages are available on GitHub (https://github.com/czhang11/agkit4ee).

",CoastSat image classification,47,0,,,
Image classification,An Analysis of Long-Term Rainfall Trends and Variability in the Uttarakhand Himalaya Using Google Earth Engine,"Abhishek Banerjee, Ruishan Chen, Michael E. Meadows, R.B. Singh, Suraj Mal, Dhritiraj Sengupta","This paper analyses the spatio-temporal trends and variability in annual, seasonal, and monthly rainfall with corresponding rainy days in Bhilangana river basin, Uttarakhand Himalaya, based on stations and two gridded products. Station-based monthly rainfall and rainy days data were obtained from the India Meteorological Department (IMD) for the period from 1983 to 2008 and applied, along with two daily rainfall gridded products to establish temporal changes and spatial associations in the study area. Due to the lack of more recent ground station rainfall measurements for the basin, gridded data were then used to establish monthly rainfall spatio-temporal trends for the period 2009 to 2018. The study shows all surface observatories in the catchment experienced an annual decreasing trend in rainfall over the 1983 to 2008 period, averaging 15.75 mm per decade. Analysis of at the monthly and seasonal trend showed reduced rainfall for August and during monsoon season as a whole (10.13 and 11.38 mm per decade, respectively); maximum changes were observed in both monsoon and winter months. Gridded rainfall data were obtained from the Climate Hazard Infrared Group Precipitation Station (CHIRPS) and Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks-Climate Data Record (PERSIANN-CDR). By combining the big data analytical potential of Google Earth Engine (GEE), we compare spatial patterns and temporal trends in observational and modelled precipitation and demonstrate that remote sensing products can reliably be used in inaccessible areas where observational data are scarce and/or temporally incomplete. CHIRPS reanalysis data indicate that there are in fact three significantly distinct annual rainfall periods in the basin, viz. phase 1: 1983 to 1997 (relatively high annual rainfall); phase 2: 1998 to 2008 (drought); phase 3: 2009 to 2018 (return to relatively high annual rainfall again). By comparison, PERSIANN-CDR data show reduced annual and winter precipitation, but no significant changes during the monsoon and pre-monsoon seasons from 1983 to 2008. The major conclusions of this study are that rainfall modelled using CHIRPS corresponds well with the observational record in confirming the decreased annual and seasonal rainfall, averaging 10.9 and 7.9 mm per decade respectively between 1983 and 2008, although there is a trend (albeit not statistically significant) to higher rainfall after the marked dry period between 1998 and 2008. Long-term variability in rainfall in the Bhilangana river basin has had critical impacts on the environment arising from water scarcity in this mountainous region.
",CoastSat image classification,47,0,,,
Image classification,Satellite-derived shoreline detection at a high-energy meso-macrotidal beach,"Bruno Castelle, Gerd Masselink, Tim Scott, Christopher Stokes, Aikaterini Konstantinou, Vincent Marieu, Stéphane Bujan","Publicly available satellite imagery can now provide multi-decadal time series of shoreline data from local to global scale, enabling analysis of sandy beach shoreline variability across a spectrum of temporal scales. Such data can, however, be associated with large uncertainties, particularly for beaches experiencing a large tidal range (>2 m) and energetic incident waves. We use a decade of bi-monthly topographic surveys at the high-energy meso-macrotidal beach of Truc Vert, southwest France, and concurrent wave and water-level hindcast to investigate the uncertainties associated with satellite-derived time series of the shoreline position. We show that consideration of the water level and wave runup elevation is critical for accurately estimating waterline position and, in turn, shoreline position. At Truc Vert, including non-tidal water level residuals (e.g. wind-driven surge) and accounting for time- and elevation-varying beach slope for horizontal correction did not improve satellite-derived shoreline position. A new total water level threshold is proposed to maximize the number of usable images while minimizing errors. Accounting for wave runup and the new water level threshold at Truc Vert, the number of usable satellite images is doubled and shoreline position errors are at least halved compared to previous work at this site. Using the 1984–2019 reconstructed shoreline, we also show that the satellite-derived shoreline trends and interannual variability are in better agreement with field measurements. Although the approach proposed here needs to be tested on other sites in different tidal/wave forcing environments with different morphological and sediment characteristics, we anticipate that it will improve the temporal and spatial description of shoreline change on most surf tidal beaches where accurate continuous water level and wave hindcasts and/or observations are available.",CoastSat image classification,47,0,,,
Image classification,An efficient protocol for accurate and massive shoreline definition from mid-resolution satellite imagery,"E.Sánchez-García, J.M.Palomar-Vázquez, J.E.Pardo-Pascual, J.Almonacid-Caballer, C.Cabezas-Rabadán, L.Gómez-Pujolb","Satellite images may constitute a useful source of information for coastal monitoring as long as it is possible to manage them in an efficient way and to derive precise indicators of the state of the beaches. In the present work, SHOREX system is employed for managing and processing Landsat 8 and Sentinel 2 images to automatically define the instantaneous shoreline position at sub-pixel level. Between the years 2013 and 2017, 91 satellite-derived shorelines (SDS) were assessed by comparing with high-resolution shorelines obtained simultaneously through video-monitoring. The analysis allowed identifying the combination of parameters to perform the extraction algorithm with the highest accuracy. Furthermore, an efficient self-contained workflow is proposed, more robust and independent from inaccuracies in the approximate input line and from multiple morphological and oceanographic issues that may condition the radiometric response near the shore. An iterative procedure ensures firstly a suitable kernel of analysis representing the water-land interface to get, afterward, the definition of the sub-pixel shoreline with high accuracy (below 3 m RMSE).

",CoastSat image classification,47,0,,,
Image classification,Satellite optical imagery in Coastal Engineering,"Ian L.Turner, Mitchell D.Harleya, Rafael Almar, Erwin W.J.Bergsma","This Short Communication provides a Coastal Engineering perspective on present and emerging capabilities of satellite optical imagery, including real-world applications that can now be realistically implemented from the desktop. Significantly, at the vast majority of locations worldwide, satellite remote sensing is currently the only source of information to complement much more limited in-situ instrumentation for land and sea mapping, monitoring and measurement. Less well recognised is that publicly available, routinely sampled and now easily accessible optical imagery covering virtually every position along the world's coastlines already spans multiple decades. In the past five years the common obstacles of (1) limited access to high-performance computing and (2) specialist remote sensing technical expertise, have been largely removed. The emergence of several internet-accessible application programming interfaces (APIs) now enable applied users to access petabytes of satellite imagery along with the necessary tools and processing power to extract, manipulate and analyse information of practical interest. Following a brief overview and timeline of civilian Earth observations from space, satellite-derived shorelines (SDS) and satellite-derived bathymetry (SDB) are used to introduce and demonstrate some of the present real-world capabilities of satellite optical imagery most relevant to coastal professionals and researchers. These practical examples illustrate the use of satellite imagery to monitor and quantify both engineered and storm-induced coastline changes, as well as the emerging potential to obtain seamless topo/bathy surveys along coastal regions. Significantly, timescales of satellite-derived changes at the coast can range from decades to days, with spatial scales of interest extending from individual project sites up to unprecedented regional and global studies. While we foresee the uptake and routine use of satellite-derived information becoming quickly ubiquitous within the Coastal Engineering profession, on-ground observations are – and in our view will remain - fundamentally important. Compared to precision in-situ instrumentation, present intrinsic limitations of satellites are their relatively low rates of revisit and decimetre spatial accuracy. New satellite advances including ‘video from space’ and the potential to combine Earth observation with numerical and data-driven coastal models through assimilation and artificial intelligence are advances that we foresee will have future major impact in Coastal Engineering.

",CoastSat image classification,47,0,,,
Image classification,Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model,"Ashutosh Singla, Lin Yuan, Touradj Ebrahimi","Recent past has seen a lot of developments in the field of image-based dietary assessment. Food image classification and recognition are crucial steps for dietary assessment. In the last couple of years, advancements in the deep learning and convolutional neural networks proved to be a boon for the image classification and recognition tasks, specifically for food recognition because of the wide variety of food items. In this paper, we report experiments on food/non-food classification and food recognition using a GoogLeNet model based on deep convolutional neural network. The experiments were conducted on two image datasets created by our own, where the images were collected from existing image datasets, social media, and imaging devices such as smart phone and wearable cameras. Experimental results show a high accuracy of 99.2% on the food/non-food classification and 83.6% on the food category recognition.",FOOD-5K,48,1,FOOD-5K," Ashutosh Singla, Lin Yuan, Touradj Ebrahimi                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ","This is a dataset containing 2500 food and 2500 non-food images, for the task of food/non-food classification in our paper “Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model”. The whole dataset is divided in three parts: training, validation and evaluation. The naming convention is as follows:
{ClassID}_{ImageID}.jpg
ClassID: 0 or 1; 0 means non-food and 1 means food.
ImageID: ID of the image within the class.                                                                                                                                                                                                                                                                                                                                                                                                                                  Source: https://www.epfl.ch/labs/mmspg/downloads/food-image-datasets/"
Image classification,A Survey on Food Computing,"Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, Ramesh Jain","Food is very essential for human life and it is fundamental to the human experience. Food-related study may support multifarious applications and services, such as guiding the human behavior, improving the human health and understanding the culinary culture. With the rapid development of social networks, mobile networks, and Internet of Things (IoT), people commonly upload, share, and record food images, recipes, cooking videos, and food diaries, leading to large-scale food data. Large-scale food data offers rich knowledge about food and can help tackle many central issues of human society. Therefore, it is time to group several disparate issues related to food computing. Food computing acquires and analyzes heterogenous food data from disparate sources for perception, recognition, retrieval, recommendation, and monitoring of food. In food computing, computational approaches are applied to address food related issues in medicine, biology, gastronomy and agronomy. Both large-scale food data and recent breakthroughs in computer science are transforming the way we analyze food data. Therefore, vast amounts of work has been conducted in the food area, targeting different food-oriented tasks and applications. However, there are very few systematic reviews, which shape this area well and provide a comprehensive and in-depth summary of current efforts or detail open problems in this area. In this paper, we formalize food computing and present such a comprehensive overview of various emerging concepts, methods, and tasks. We summarize key challenges and future directions ahead for food computing. This is the first comprehensive survey that targets the study of computing technology for the food area and also offers a collection of research studies and technologies to benefit researchers and practitioners working in different food-related fields.
",FOOD-5K,48,0,,,
Image classification,Application of Deep Learning in Food: A Review,"Lei Zhou, Chu Zhang, Fei Liu, Zhengjun Qiu, Yong He","Deep learning has been proved to be an advanced technology for big data analysis with a large number of successful cases in image processing, speech recognition, object detection, and so on. Recently, it has also been introduced in food science and engineering. To our knowledge, this review is the first in the food domain. In this paper, we provided a brief introduction of deep learning and detailedly described the structure of some popular architectures of deep neural networks and the approaches for training a model. We surveyed dozens of articles that used deep learning as the data analysis tool to solve the problems and challenges in food domain, including food recognition, calories estimation, quality detection of fruits, vegetables, meat and aquatic products, food supply chain, and food contamination. The specific problems, the datasets, the preprocessing methods, the networks and frameworks used, the performance achieved, and the comparison with other popular solutions of each research were investigated. We also analyzed the potential of deep learning to be used as an advanced data mining tool in food sensory and consume researches. The result of our survey indicates that deep learning outperforms other methods such as manual feature extractors, conventional machine learning algorithms, and deep learning as a promising tool in food quality and safety inspection. The encouraging results in classification and regression problems achieved by deep learning will attract more research efforts to apply deep learning into the field of food in the future.",FOOD-5K,48,0,,,
Image classification,NutriNet: A Deep Learning Food and Drink Image Recognition System for Dietary Assessment,"Simon Mezgec 1,, Barbara Koroušić Seljak","Automatic food image recognition systems are alleviating the process of food-intake estimation and dietary assessment. However, due to the nature of food images, their recognition is a particularly challenging task, which is why traditional approaches in the field have achieved a low classification accuracy. Deep neural networks have outperformed such solutions, and we present a novel approach to the problem of food and drink image detection and recognition that uses a newly-defined deep convolutional neural network architecture, called NutriNet. This architecture was tuned on a recognition dataset containing 225,953 512 × 512 pixel images of 520 different food and drink items from a broad spectrum of food groups, on which we achieved a classification accuracy of 86 . 72 % , along with an accuracy of 94 . 47 % on a detection dataset containing 130 , 517 images. We also performed a real-world test on a dataset of self-acquired images, combined with images from Parkinson's disease patients, all taken using a smartphone camera, achieving a top-five accuracy of 55 % , which is an encouraging result for real-world images. Additionally, we tested NutriNet on the University of Milano-Bicocca 2016 (UNIMIB2016) food image dataset, on which we improved upon the provided baseline recognition result. An online training component was implemented to continually fine-tune the food and drink recognition model on new images. The model is being used in practice as part of a mobile app for the dietary assessment of Parkinson's disease patients.

",FOOD-5K,48,0,,,
Image classification,Detecting Adversarial Image Examples in Deep Neural Networks with Adaptive Noise Reduction,"Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi, Xiaofeng Wang","Recently, many studies have demonstrated deep neural network (DNN) classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed. However, existing defense techniques often require modifying the target model or depend on the prior knowledge of attacks. In this paper, we propose a straightforward method for detecting adversarial image examples, which can be directly deployed into unmodified off-the-shelf DNN models. We consider the perturbation to images as a kind of noise and introduce two classic image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. Consequently, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version, without referring to any prior knowledge of attacks. More than 20,000 adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiments show that our detection method can achieve a high overall F1 score of 96.39% and certainly raises the bar for defense-aware attacks.
",FOOD-5K,48,0,,,
Image classification,You Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis,Weiqing Min; Bing-Kun Bao; Shuhuan Mei; Yaohui Zhu; Yong Rui; Shuqiang Jiang,"Cuisine is a style of cooking and usually associated with a specific geographic region. Recipes from different cuisines shared on the web are an indicator of culinary cultures in different countries. Therefore, analysis of these recipes can lead to deep understanding of food from the cultural perspective. In this paper, we perform the first cross-region recipe analysis by jointly using the recipe ingredients, food images, and attributes such as the cuisine and course (e.g., main dish and dessert). For that solution, we propose a culinary culture analysis framework to discover the topics of ingredient bases and visualize them to enable various applications. We first propose a probabilistic topic model to discover cuisine-course specific topics. The manifold ranking method is then utilized to incorporate deep visual features to retrieve food images for topic visualization. At last, we applied the topic modeling and visualization method for three applications: 1) multimodal cuisine summarization with both recipe ingredients and images, 2) cuisine-course pattern analysis including topic-specific cuisine distribution and cuisine-specific course distribution of topics, and 3) cuisine recommendation for both cuisine-oriented and ingredient-oriented queries. Through these three applications, we can analyze the culinary cultures at both macro and micro levels. We conduct the experiment on a recipe database Yummly-66K with 66,615 recipes from 10 cuisines in Yummly. Qualitative and quantitative evaluation results have validated the effectiveness of topic modeling and visualization, and demonstrated the advantage of the framework in utilizing rich recipe information to analyze and interpret the culinary cultures from different regions.",FOOD-5K,48,0,,,
Image classification,"Grab, Pay, and Eat: Semantic Food Detection for Smart Restaurants","Eduardo Aguilar, Beatriz Remeseiro,  Marc Bolaños, Petia Radeva","The increase in awareness of people towards their nutritional habits has drawn considerable attention to the field of automatic food analysis. Focusing on self-service restaurants environment, automatic food analysis is not only useful for extracting nutritional information from foods selected by customers, it is also of high interest to speed up the service solving the bottleneck produced at the cashiers in times of high demand. In this paper, we address the problem of automatic food tray analysis in canteens and restaurants environment, which consists in predicting multiple foods placed on a tray image. We propose a new approach for food analysis based on convolutional neural networks, we name Semantic Food Detection, which integrates in the same framework food localization, recognition and segmentation. We demonstrate that our method improves the state of the art food detection by a considerable margin on the public dataset UNIMIB2016 achieving about 90% in terms of F-measure, and thus provides a significant technological advance towards the automatic billing in restaurant environments.",FOOD-5K,48,0,,,
Image classification,Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets,"Patrick McAllister, Huiru Zheng, Raymond Bond, Anne Moorhead","Obesity is increasing worldwide and can cause many chronic conditions such as type-2 diabetes, heart disease, sleep apnea, and some cancers. Monitoring dietary intake through food logging is a key method to maintain a healthy lifestyle to prevent and manage obesity. Computer vision methods have been applied to food logging to automate image classification for monitoring dietary intake. In this work we applied pretrained ResNet-152 and GoogleNet convolutional neural networks (CNNs), initially trained using ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset with MatConvNet package, to extract features from food image datasets; Food 5K, Food-11, RawFooT-DB, and Food-101. Deep features were extracted from CNNs and used to train machine learning classifiers including artificial neural network (ANN), support vector machine (SVM), Random Forest, and Naive Bayes. Results show that using ResNet-152 deep features with SVM with RBF kernel can accurately detect food items with 99.4% accuracy using Food-5K validation food image dataset and 98.8% with Food-5K evaluation dataset using ANN, SVM-RBF, and Random Forest classifiers. Trained with ResNet-152 features, ANN can achieve 91.34%, 99.28% when applied to Food-11 and RawFooT-DB food image datasets respectively and SVM with RBF kernel can achieve 64.98% with Food-101 image dataset. From this research it is clear that using deep CNN features can be used efficiently for diverse food item image classification. The work presented in this research shows that pretrained ResNet-152 features provide sufficient generalisation power when applied to a range of food image classification tasks.",FOOD-5K,48,0,,,
Image classification,Mobile Multi-Food Recognition Using Deep Learning,"Parisa Pouladzadeh, Shervin Shirmohammadi","In this article, we propose a mobile food recognition system that uses the picture of the food, taken by the user’s mobile device, to recognize multiple food items in the same meal, such as steak and potatoes on the same plate, to estimate the calorie and nutrition of the meal. To speed up and make the process more accurate, the user is asked to quickly identify the general area of the food by drawing a bounding circle on the food picture by touching the screen. The system then uses image processing and computational intelligence for food item recognition. The advantage of recognizing items, instead of the whole meal, is that the system can be trained with only single item food images. At the training stage, we first use region proposal algorithms to generate candidate regions and extract the convolutional neural network (CNN) features of all regions. Second, we perform region mining to select positive regions for each food category using maximum cover by our proposed submodular optimization method. At the testing stage, we first generate a set of candidate regions. For each region, a classification score is computed based on its extracted CNN features and predicted food names of the selected regions. Since fast response is one of the important parameters for the user who wants to eat the meal, certain heavy computational parts of the application are offloaded to the cloud. Hence, the processes of food recognition and calorie estimation are performed in cloud server. Our experiments, conducted with the FooDD dataset, show an average recall rate of 90.98%, precision rate of 93.05%, and accuracy of 94.11% compared to 50.8% to 88% accuracy of other existing food recognition systems.

",FOOD-5K,48,0,,,
Image classification,Automatic food detection in egocentric images using artificial intelligence technology,"Wenyan Jia, Yuecheng Li, Ruowei Qu, Thomas Baranowski, Lora E Burke, Hong Zhang, Yicheng Bai, Juliet M Mancino, Guizhi Xu, Zhi-Hong Mao,Mingui Sun","Objective: To develop an artificial intelligence (AI)-based algorithm which can automatically detect food items from images acquired by an egocentric wearable camera for dietary assessment.

Design: To study human diet and lifestyle, large sets of egocentric images were acquired using a wearable device, called eButton, from free-living individuals. Three thousand nine hundred images containing real-world activities, which formed eButton data set 1, were manually selected from thirty subjects. eButton data set 2 contained 29 515 images acquired from a research participant in a week-long unrestricted recording. They included both food- and non-food-related real-life activities, such as dining at both home and restaurants, cooking, shopping, gardening, housekeeping chores, taking classes, gym exercise, etc. All images in these data sets were classified as food/non-food images based on their tags generated by a convolutional neural network.

Results: A cross data-set test was conducted on eButton data set 1. The overall accuracy of food detection was 91·5 and 86·4 %, respectively, when one-half of data set 1 was used for training and the other half for testing. For eButton data set 2, 74·0 % sensitivity and 87·0 % specificity were obtained if both 'food' and 'drink' were considered as food images. Alternatively, if only 'food' items were considered, the sensitivity and specificity reached 85·0 and 85·8 %, respectively.

Conclusions: The AI technology can automatically detect foods from low-quality, wearable camera-acquired real-world egocentric images with reasonable accuracy, reducing both the burden of data processing and privacy concerns.",FOOD-5K,48,0,,,
Image classification,Food Recognition Using Fusion of Classifiers Based on CNNs,"Eduardo Aguilar, Marc Bolaños, Petia Radeva","With the arrival of convolutional neural networks, the complex problem of food recognition has experienced an important improvement in recent years. The best results have been obtained using methods based on very deep convolutional neural networks, which show that the deeper the model,the better the classification accuracy will be obtain. However, very deep neural networks may suffer from the overfitting problem. In this paper, we propose a combination of multiple classifiers based on different convolutional models that complement each other and thus, achieve an improvement in performance. The evaluation of our approach is done on two public datasets: Food-101 as a dataset with a wide variety of fine-grained dishes, and Food-11 as a dataset of high-level food categories, where our approach outperforms the independent CNN models.",FOOD-5K,48,0,,,
Image classification,Places205-vggnet models for scene recognition,"Limin Wang, Sheng Guo, Weilin Huang, Yu Qiao","VGGNets have turned out to be effective for object recognition in still images. However, it is unable to yield good performance by directly adapting the VGGNet models trained on the ImageNet dataset for scene recognition. This report describes our implementation of training the VGGNets on the large-scale Places205 dataset. Specifically, we train three VGGNet models, namely VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe toolbox with high computational efficiency. We verify the performance of trained Places205-VGGNet models on three datasets: MIT67, SUN397, and Places205. Our trained models achieve the state-of-the-art performance on these datasets and are made public available.",sun397,49,1,sun397,"Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, Antonio Torralba  ","The database contains 108,753 images of 397 categories, used in the Scene UNderstanding (SUN) benchmark. The number of images varies across categories, but there are at least 100 images per category. Several configs of the dataset are made available through TFDS: - A custom (random) partition of the whole dataset with 76,128 training images, 10,875 validation images and 21,750 test images. Images have been resized to have at most 120,000 pixels, and encoded as JPEG with quality of 72. - ""standard-part1-120k"", ""standard-part2-120k"", ..., ""standard-part10-120k"": Each of the 10 official train/test partitions with 50 images per class in each split. Images have been resized to have at most 120,000 pixels, and encoded as JPEG with quality of 72.                                                                                                  Source:https://www.tensorflow.org/datasets/catalog/sun397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
Image classification,Let there be color! Joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification,"Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa","We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network features a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification database to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Furthermore, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.",sun397,49,0, ,,
Image classification,Architectural Requirements and Scalability of the NAS Parallel Benchmarks,"F.C. Wong, R.P. Martin, R.H. Arpaci-Dusseau, D.E. Culler","We present a study of the architectural requirements and scalability of the NAS Parallel
Benchmarks. Through direct measurements and simulations, we identify the factors which
affect the scalability of benchmark codes on two relevant and distinct platforms; a cluster
of workstations and a ccNUMA SGI Origin 2000.
We find that the benefit of increased global cache size is pronounced in certain applications and often offsets the communication cost. By constructing the working set profile of
the benchmarks, we are able to visualize the improvement of computational efficiency
under constant-problem-size scaling.
We also find that, while the Origin MPI has better point-to-point performance, the cluster
MPI layer is more scalable with communication load. However, communication performance within the applications is often much lower than what would be achieved by microbenchmarks. We show that the communication protocols used by MPI runtime library are
influential to the communication performance in applications, and that the benchmark
codes have a wide spectrum of communication requirements.",sun397,49,0,,,
Image classification,Knowledge Guided Disambiguation for Large-Scale Scene Classification With Multi-Resolution CNNs,"Limin Wang, Sheng Guo, Weilin Huang, Yuanjun Xiong, Yu Qiao","Convolutional Neural Networks (CNNs) have made remarkable progress on scene recognition, partially due to these recent large-scale scene datasets, such as the Places and Places2. Scene categories are often defined by multi-level information, including local objects, global layout, and background environment, thus leading to large intra-class variations. In addition, with the increasing number of scene categories, label ambiguity has become another crucial issue in large-scale classification. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture that captures visual content and structure at multiple levels. The multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, which are complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity. (i) We exploit the knowledge from the confusion matrix computed on validation data to merge ambiguous classes into a super category. (ii) We utilize the knowledge of extra networks to produce a soft label for each image. Then the super categories or soft labels are employed to guide CNN training on the Places2. We conduct extensive experiments on three large-scale image datasets (ImageNet, Places, and Places2), demonstrating the effectiveness of our approach. Furthermore, our method takes part in two major scene recognition challenges, and achieves the second place at the Places2 challenge in ILSVRC 2015, and the first place at the LSUN challenge in CVPR 2016. Finally, we directly test the learned representations on other scene benchmarks, and obtain the new state-of-the-art results on the MIT Indoor67 (86.7\%) and SUN397 (72.0\%). We release the code and models at~\url{this https URL}.
",sun397,49,0,,,
Image classification,Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation,"Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan, Cheng-Lin Liu","Convolutional neural network (CNN) has achieved state-of-the-art performance in many different visual tasks. Learned from a large-scale training dataset, CNN features are much more discriminative and accurate than the hand-crafted features. Moreover, CNN features are also transferable among different domains. On the other hand, traditional dictionarybased features (such as BoW and SPM) contain much more local discriminative and structural information, which is implicitly embedded in the images. To further improve the performance, in this paper, we propose to combine CNN with dictionarybased models for scene recognition and visual domain adaptation. Specifically, based on the well-tuned CNN models (e.g., AlexNet and VGG Net), two dictionary-based representations are further constructed, namely mid-level local representation (MLR) and convolutional Fisher vector representation (CFV). In MLR, an efficient two-stage clustering method, i.e., weighted spatial and feature space spectral clustering on the parts of a single image followed by clustering all representative parts of all images, is used to generate a class-mixture or a classspecific part dictionary. After that, the part dictionary is used to operate with the multi-scale image inputs for generating midlevel representation. In CFV, a multi-scale and scale-proportional GMM training strategy is utilized to generate Fisher vectors based on the last convolutional layer of CNN. By integrating the complementary information of MLR, CFV and the CNN features of the fully connected layer, the state-of-the-art performance can be achieved on scene recognition and domain adaptation problems. An interested finding is that our proposed hybrid representation (from VGG net trained on ImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained on Place205) greatly.
",sun397,49,0,,,
Image classification,A General Framework for Edited Video and Raw Video Summarization,"Xuelong Li, Bin Zhao, Xiaoqiang Lu","In this paper, we build a general summarization
framework for both of edited video and raw video summarization. Overall, our work can be divided into three folds: 1) Four
models are designed to capture the properties of video summaries,
i.e., containing important people and objects (importance), representative to the video content (representativeness), no similar
key-shots (diversity) and smoothness of the storyline (storyness).
Specifically, these models are applicable to both edited videos
and raw videos. 2) A comprehensive score function is built with
the weighted combination of the aforementioned four models.
Note that the weights of the four models in the score function,
denoted as property-weight, are learned in a supervised manner.
Besides, the property-weights are learned for edited videos and
raw videos, respectively. 3) The training set is constructed with
both edited videos and raw videos in order to make up the lack
of training data. Particularly, each training video is equipped
with a pair of mixing-coefficients which can reduce the structure
mess in the training set caused by the rough mixture. We test our
framework on three datasets, including edited videos, short raw
videos and long raw videos. Experimental results have verified
the effectiveness of the proposed framework.",sun397,49,0,,,
Image classification,Open-set human activity recognition based on micro-Doppler signatures,"Yang Yang, Chunping Hou, Yue Lang, Dai Guan, Danyang Huang, Jinchen Xu","Open-set activity recognition remains as a challenging problem because of complex activity diversity. In previous works, extensive efforts have been paid to construct a negative set or set an optimal threshold for the target set. In this paper, a model based on Generative Adversarial Network (GAN), called ‘OpenGAN’ is proposed to address the open-set recognition without manual intervention during the training process. The generator produces fake target samples, which serve as an automatic negative set, and the discriminator is redesigned to output multiple categories together with an ‘unknown’ class. We evaluate the effectiveness of the proposed method on measured micro-Doppler radar dataset and the MOtion CAPture (MOCAP) database from Carnegie Mellon University (CMU). The comparison results with several state-of-the-art methods indicate that OpenGAN provides a promising open-set solution to human activity recognition even under the circumstance with few known classes. Ablation studies are also performed, and it is shown that the proposed architecture outperforms other variants and is robust on both datasets.

",sun397,49,0,,,
Image classification,Machine learning in geo- and environmental sciences: From small to large scale,"Pejman Tahmasebi, Serveh Kamrava, Tao Bai, Muhammad Sahimi","In recent years significant breakthroughs in exploring big data, recognition of complex patterns, and predicting intricate variables have been made. One efficient way of analyzing big data, recognizing complex patterns, and extracting trends is through machine-learning (ML) algorithms. The field of porous media, and more generally geoscience, have also witnessed much progress, and recent progress in developing various ML techniques have benefitted various problems in porous media and geoscience across disparate scales. Thus, it is becoming increasingly clear that it is imperative to adopt advanced ML methods for the problems in porous media and geoscience because they enable researchers to solve many difficult problems. At the same time, one can use the already existing extensive knowledge of porous media to endow ML algorithms and develop novel physics-guided methods. The goal of this review paper is to provide the first comprehensive review of the recently developed methods in the ML algorithms and describe their application to porous media and geoscience. Thus, we review the basic concept of the ML and describe more advanced methods, known as deep-learning algorithms. Then, the application of such methods to various problems in porous media and geoscience, such as hydrological modeling, fluid flow in porous media, and (sub)surface characterization, are reviewed. We also provide a discussion of future directions in this rapidly developing field.

",sun397,49,0,,,
Image classification,Locally Supervised Deep Hybrid Model for Scene Recognition,"Sheng Guo, Weilin Huang, Limin Wang, Yu Qiao","Convolutional neural networks (CNN) have recently achieved remarkable successes in various image classification and understanding tasks. The deep features obtained at the top fully-connected layer of the CNN (FC-features) exhibit rich global semantic information and are extremely effective in image classification. On the other hand, the convolutional features in the middle layers of the CNN also contain meaningful local information, but are not fully explored for image representation. In this paper, we propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and explores the convolutional features for scene recognition. Firstly, we notice that the convolutional features capture local objects and fine structures of scene images, which yield important cues for discriminating ambiguous scenes, whereas these features are significantly eliminated in the highly-compressed FC representation. Secondly, we propose a new Local Convolutional Supervision (LCS) layer to enhance the local structure of the image by directly propagating the label information to the convolutional layers. Thirdly, we propose an efficient Fisher Convolutional Vector (FCV) that successfully rescues the orderless mid-level semantic information (e.g. objects and textures) of scene image. The FCV encodes the large-sized convolutional maps into a fixed-length mid-level representation, and is demonstrated to be strongly complementary to the high-level FC-features. Finally, both the FCV and FC-features are collaboratively employed in the LSDHM representation, which achieves outstanding performance in our experiments. It obtains 83.75% and 67.56% accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397 datasets, advancing the stat-of-the-art substantially.
",sun397,49,0,,,
Image classification,"Analysis and Optimization of Loss Functions for Multiclass, Top-k, and Multilabel Classification","Maksim Lapin, Matthias Hein, Bernt Schiele","Top-k error is currently a popular performance measure on large scale image classification benchmarks such as ImageNet and Places. Despite its wide acceptance, our understanding of this metric is limited as most of the previous research is focused on its special case, the top-1 error. In this work, we explore two directions that shed more light on the top-k error. First, we provide an in-depth analysis of established and recently proposed single-label multiclass methods along with a detailed account of efficient optimization algorithms for them. Our results indicate that the softmax loss and the smooth multiclass SVM are surprisingly competitive in top-k error uniformly across all k, which can be explained by our analysis of multiclass top-k calibration. Further improvements for a specific k are possible with a number of proposed top-k loss functions. Second, we use the top-k methods to explore the transition from multiclass to multilabel learning. In particular, we find that it is possible to obtain effective multilabel classifiers on Pascal VOC using a single label per image for training, while the gap between multiclass and multilabel methods on MS COCO is more significant. Finally, our contribution of efficient algorithms for training with the considered top-k and multilabel loss functions is of independent interest.
",sun397,49,0,,,
Image classification,Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition,"Zhe Wang, Limin Wang, Yali Wang, Bowen Zhang, Yu Qiao","Traditional feature encoding scheme (e.g., Fisher
vector) with local descriptors (e.g., SIFT) and recent convolutional neural networks (CNNs) are two classes of successful
methods for image recognition. In this paper, we propose a hybrid
representation, which leverages the discriminative capacity of
CNNs and the simplicity of descriptor encoding schema for
image recognition, with a focus on scene recognition. To this end,
we make three main contributions from the following aspects.
First, we propose a patch-level and end-to-end architecture
to model the appearance of local patches, called PatchNet.
PatchNet is essentially a customized network trained in a weakly
supervised manner, which uses the image-level supervision to
guide the patch-level feature extraction. Second, we present
a hybrid visual representation, called VSAD, by utilizing the
robust feature representations of PatchNet to describe local
patches and exploiting the semantic probabilities of PatchNet
to aggregate these local patches into a global representation.
Third, based on the proposed VSAD representation, we propose a
new state-of-the-art scene recognition approach, which achieves
an excellent performance on two standard benchmarks: MIT
Indoor67 (86.2%) and SUN397 (73.0%).",sun397,49,0,,,
Image classification,Do ImageNet Classifiers Generalize to ImageNet?,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar","We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly ""harder"" images than those found in the original test sets.
",ImageNet Dataset,50,1,ImageNet Dataset,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Fei-Fei Li
","The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection. The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld. ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”. The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.

Total number of non-empty WordNet synsets: 21841 Total number of images: 14197122 Number of images with bounding box annotations: 1,034,908 Number of synsets with SIFT features: 1000 Number of images with SIFT features: 1.2 million"
Image classification,Understanding deep learning (still) requires rethinking generalization,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals","Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.
Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.
We interpret our experimental findings by comparison with traditional models.",ImageNet Dataset,50,0,,,
Image classification,Learning Transferable Visual Models From Natural Language Supervision,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever","State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.
",ImageNet Dataset,50,0,,,
Image classification,Self-Training With Noisy Student Improves ImageNet Classification,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le","We present a simple self-training method that achieves
88.4% top-1 accuracy on ImageNet, which is 2.0% better
than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves
ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces
ImageNet-C mean corruption error from 45.7 to 28.3, and
reduces ImageNet-P mean flip rate from 27.8 to 12.2.
To achieve this result, we first train an EfficientNet model
on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then
train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate
this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not
noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject
noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student
generalizes better than the teacher. ",ImageNet Dataset,50,0,,,
Image classification,Training data-efficient image transformers & distillation through attention,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou","Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.
In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.
More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",ImageNet Dataset,50,0,,,
Image classification,Randaugment: Practical Automated Data Augmentation With a Reduced Search Space,"Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le","Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.
",ImageNet Dataset,50,0,,,
Image classification,Designing Network Design Spaces,"Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollar","In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.
",ImageNet Dataset,50,0,,,
Image classification,Shortcut learning in deep neural networks,"Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,  Felix A. Wichmann","Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.
",ImageNet Dataset,50,0,,,
Image classification,On Lazy Training in Differentiable Programming,"Lénaïc Chizat, Edouard Oyallon, Francis Bach","In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this ""lazy training"" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that ""lazy training"" is behind the many successes of neural networks in difficult high dimensional tasks.
",ImageNet Dataset,50,0,,,
Image classification,Beyond Accuracy: Behavioral Testing of NLP models with CheckList,"Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh","Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.
",ImageNet Dataset,50,0,,,
Image classification,Natural Adversarial Examples,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song","We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.
",ImageNet Dataset,50,0,,,
Image classification,Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response,"Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi","During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.
",Datasets for Social Media Image Classification for Disaster Response,51,1,Datasets for Social Media Image Classification for Disaster Response,"Firoj Alam
","Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community."
Image classification,HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks,"Firoj Alam, Umair Qazi, Muhammad Imran, Ferda Ofli","Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with ~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available. this https URL
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,"Firoj Alam, Hassan Sajjad, Muhammad Imran, Ferda Ofli","The time-critical analysis of social media streams is important for humanitarian organizations to plan rapid response during disasters. The crisis informatics research community has developed several techniques and systems to process and classify big crisis-related data posted on social media. However, due to the dispersed nature of the datasets used in the literature, it is not possible to compare the results and measure the progress made towards better models for crisis informatics. In this work, we attempt to bridge this gap by combining various existing crisis-related datasets. We consolidate eight annotated data sources and provide 166.1k and 141.5k tweets for informativeness and humanitarian classification tasks, respectively. The consolidation results in a larger dataset that affords the ability to train more sophisticated models. To that end, we provide binary and multiclass classification results using CNN, FastText, and transformer based models to address informativeness and humanitarian tasks, respectively. We make the dataset and scripts available at https://crisisnlp.qcri.org/crisis_datasets_benchmarks.html.
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification,"Firoj Alam, Tanvirul Alam, Md. Arid Hasan, Abul Hasnat, Muhammad Imran, Ferda Ofli","Recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). While notable progress has been made using texts, research on exploiting the images remains relatively under-explored. To advance image-based approaches, we propose MEDIC (Available at: this https URL), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. This is the first dataset of its kind: social media images, disaster response, and multi-task learning research. An important property of this dataset is its high potential to facilitate research on multi-task learning, which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. Therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. We experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. Along with the dataset, we also release all relevant scripts (this https URL).
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,Robust Training of Social Media Image Classification Models for Rapid Disaster Response,"Firoj Alam, Tanvirul Alam, Muhammad Imran, Ferda Ofli","Images shared on social media help crisis managers gain situational awareness and assess incurred damages, among other response tasks. As the volume and velocity of such content are typically high, real-time image classification has become an urgent need for a faster disaster response. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of the damage. To develop robust real-time models, it is necessary to understand the capability of the publicly available pre-trained models for these tasks, which remains to be under-explored in the crisis informatics literature. In this study, we address such limitations by investigating ten different network architectures for four different tasks using the largest publicly available datasets for these tasks. We also explore various data augmentation strategies, semi-supervised techniques, and a multitask learning setup. In our extensive experiments, we achieve promising results.
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,"Incidents1M: a large-scale dataset of images with natural disasters, damage, and incidents","Ethan Weber, Dim P. Papadopoulos, Agata Lapedriza, Ferda Ofli, Muhammad Imran, Antonio Torralba","Natural disasters, such as floods, tornadoes, or wildfires, are increasingly pervasive as the Earth undergoes global warming. It is difficult to predict when and where an incident will occur, so timely emergency response is critical to saving the lives of those endangered by destructive events. Fortunately, technology can play a role in these situations. Social media posts can be used as a low-latency data source to understand the progression and aftermath of a disaster, yet parsing this data is tedious without automated methods. Prior work has mostly focused on text-based filtering, yet image and video-based filtering remains largely unexplored. In this work, we present the Incidents1M Dataset, a large-scale multi-label dataset which contains 977,088 images, with 43 incident and 49 place categories. We provide details of the dataset construction, statistics and potential biases; introduce and train a model for incident detection; and perform image-filtering experiments on millions of images on Flickr and Twitter. We also present some applications on incident analysis to encourage and enable future work in computer vision for humanitarian aid. Code, data, and models are available at this http URL.
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,A Deep Learning Approach of Recognizing Natural Disasters on Images using Convolutional Neural Network and Transfer Learning,"Daryl B. Valdez, Rey Anthony G. Godmali","Natural disasters are uncontrollable phenomena occurring yearly which cause extensive damage to lives, property and cause permanent damage to the environment. However by, using Deep Learning, real-time recognition of these disasters can help the victims and emergency response agencies during the onset of these destructive events. At present, there are still gaps in the literature regarding real-time natural disaster recognition. In this paper, we present a dataset for the joint classification of natural disasters and intensity. We also proposed a lightweight convolutional neural network with two classification heads for the two tasks. This study leveraged on transfer learning in training the network to recognize natural disasters, as well as detecting normal, no-disaster images. At the same time, it is also capable of recognizing disaster intensity. Under controlled conditions, the model showed promising results on the two classification tasks. Thus, the study proved that accurate recognition of natural disasters is possible using a lightweight model and transfer learning. We hope that this study would lead to development of monitoring or surveillance systems that can perform accurate, on-the-ground, and real-time recognition of natural disasters allowing for rapid emergency responses mitigating the loss of lives and damages to properties.

",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,"Extraction and analysis of natural disaster-related VGI from social media: review, opportunities and challenges","Yu Feng, Xiao Huang, Monika Sester","The idea of ‘citizen as sensors’ has gradually become a reality over the past decade. Today, Volunteered Geographic Information (VGI) from citizens is highly involved in acquiring information on natural disasters. In particular, the rapid development of deep learning techniques in computer vision and natural language processing in recent years has allowed more information related to natural disasters to be extracted from social media, such as the severity of building damage and flood water levels. Meanwhile, many recent studies have integrated information extracted from social media with that from other sources, such as remote sensing and sensor networks, to provide comprehensive and detailed information on natural disasters. Therefore, it is of great significance to review the existing work, given the rapid development of this field. In this review, we summarized eight common tasks and their solutions in social media content analysis for natural disasters. We also grouped and analyzed studies that make further use of this extracted information, either standalone or in combination with other sources. Based on the review, we identified and discussed challenges and opportunities.

",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,Critical Image Identification via Incident-Type Definition Using Smartphone Data during an Emergency: A Case Study of the 2020 Heavy Rainfall Event in Korea,"Yoonjo Choi, Namhun Kim, Seunghwan Hong, Junsu Bae, Ilsuk Park, Hong-Gyoo Sohn","In unpredictable disaster scenarios, it is important to recognize the situation promptly and take appropriate response actions. This study proposes a cloud computing-based data collection, processing, and analysis process that employs a crowd-sensing application. Clustering algorithms are used to define the major damage types, and hotspot analysis is applied to effectively filter critical data from crowdsourced data. To verify the utility of the proposed process, it is applied to Icheon-si and Anseong-si, both in Gyeonggi-do, which were affected by heavy rainfall in 2020. The results show that the types of incident at the damaged site were effectively detected, and images reflecting the damage situation could be classified using the application of the geospatial analysis technique. For 5 August 2020, which was close to the date of the event, the images were classified with a precision of 100% at a threshold of 0.4. For 24–25 August 2020, the image classification precision exceeded 95% at a threshold of 0.5, except for the mudslide mudflow in the Yul area. The location distribution of the classified images showed a distribution similar to that of damaged regions in unmanned aerial vehicle images.
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,Categorization of Disaster Related Tweets using Multimodal Approach,"Bidari, Sumit","Contents shared in form of text and images in multimedia during and after disasters can be used to analyze the information about the event. Report of affected people as missing or injured, infrastructure and utility damages, rescue and volunteering needed, not humanitarian or other relevant information can also be found with this analysis. It has been found that only few researches focuses on text as well as image modality for such analysis. Also no works has been done for mixture of dissimilar and similar category text-image pairs. In this paper, we aim to use both text as well as image of different category and fuse them using score fusion for joint representation of text and images. For text modality, we have used BERT model and for image modality we have used VGG16 modality and fused them using late fusion for multimodal analysis of disaster related tweet categorization.
",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,Identifying humanitarian information for emergency response by modeling the correlation and independence between text and images,"Xuehua Wu, Jin Mao, Hao Xie, GangLi","Information residing in multiple modalities (e.g., text, image) of social media posts can jointly provide more comprehensive and clearer insights into an ongoing emergency. To identify information valuable for humanitarian aid from noisy multimodal data, we first clarify the categories of humanitarian information, and define a multi-label multimodal humanitarian information identification task, which can adapt to the label inconsistency issue caused by modality independence while maintaining the correlation between modalities. We proposed a Multimodal Humanitarian Information Identification Model that simultaneously captures the Correlation and Independence between modalities (CIMHIM). A tailor-made dataset containing 4,383 annotated text-image pairs was built to evaluate the effectiveness of our model. The experimental results show that CIMHIM outperforms both unimodal and multimodal baseline methods by at least 0.019 in macro-F1 and 0.022 in accuracy. The combination of OCR text, object-level features, and the decision rule based on label correlations enhances the overall performance of CIMHIM. Additional experiments on a similar dataset (CrisisMMD) also demonstrate the robustness of CIMHIM. The task, model, and dataset proposed in this study contribute to the practice of leveraging multimodal social media resources to support effective emergency response.

",Datasets for Social Media Image Classification for Disaster Response,51,0,,,
Image classification,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta","The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.
",JFT-300M Dataset,52,1,JFT-300M Dataset,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta","JFT-300M is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images."
Image classification,Mask R-CNN,"Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick","We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL
",JFT-300M Dataset,52,0,,,
Image classification,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam;","Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{this https URL}.
",JFT-300M Dataset,52,0,,,
Image classification,Rethinking Atrous Convolution for Semantic Image Segmentation,"Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam","In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.
",JFT-300M Dataset,52,0,,,
Image classification,A survey on Image Data Augmentation for Deep Learning,"Connor Shorten, Taghi M. Khoshgoftaar","Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.

",JFT-300M Dataset,52,0,,,
Image classification,Large Scale GAN Training for High Fidelity Natural Image Synthesis,"Andrew Brock, Jeff Donahue, Karen Simonyan","Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.
",JFT-300M Dataset,52,0,,,
Image classification,Frustum PointNets for 3D Object Detection From RGB-D Data,"Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J. Guibas","In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.
",JFT-300M Dataset,52,0,,,
Image classification,Deep Learning for Generic Object Detection: A Survey,"Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen","Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.
",JFT-300M Dataset,52,0,,,
Image classification,The Open Images Dataset V4,"Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, Vittorio Ferrari","We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.
",JFT-300M Dataset,52,0,,,
Image classification,Training data-efficient image transformers & distillation through attention,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou","Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.
In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.
More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",JFT-300M Dataset,52,0,,,
Image classification,Exploring the Limits of Weakly Supervised Pretraining,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten","State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards ""small"". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.
",JFT-300M Dataset,52,0,,,
Image classification,MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,"VARUN CHANDOLA, RANGA RAJU VATSAVAI","Multispectral remote sensing images have been widely used for automated land use
and land cover classification tasks. Often thematic classification is done using single date image,
however in many instances a single date image is not informative enough to distinguish between
different land cover types. In this paper we show how one can use multiple images, collected at
different times of year (for example, during crop growing season), to learn a better classifier. We
propose two approaches, an ensemble of classifiers approach and a co-training based approach, and
show how both of these methods outperform a straightforward stacked vector approach often used
in multi-temporal image classification. Additionally, the co-training based method addresses the
challenge of limited labeled training data in supervised classification, as this classification scheme
utilizes a large number of unlabeled samples (which comes for free) in conjunction with a small
set of labeled training data.",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,1,MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,"VARUN CHANDOLA,  RANGA RAJU VATSAVAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ","Multispectral remote sensing images have been widely used for automated land use and land cover classification tasks. Often thematic classification is done using single date image, however in many instances a single date image is not informative enough to distinguish between different land cover types. In this paper we show how one can use multiple images, collected at different times of year (for example, during crop growing season), to learn a better classifier. We propose two approaches, an ensemble of classifiers approach and a co-training based approach, and show how both of these methods outperform a straightforward stacked vector approach often used in multi-temporal image classification. Additionally, the co-training based method addresses the challenge of limited labeled training data in supervised classification, as this classification scheme utilizes a large number of unlabeled samples (which comes for free) in conjunction with a small set of labeled training data. Source:https://data.nasa.gov/dataset/MULTI-TEMPORAL-REMOTE-SENSING-IMAGE-CLASSIFICATION/y8zc-3wup"""
Image classification,"Land Use and Land Cover Change Detection using Remote Sensing and Geographic Information System in Bodri Watershed, Central Java, Indonesia","Jonh Piter G. LUBIS, Nobukazu Nakagoshi","Remote sensing and GIS technologies are very useful for mapping LULC patterns and detailing the dynamics of a certain area. The utilization of these technologies has been applied to Bodri watershed in Central Java, Indonesia. The aims of this study are to map and detect LULC changes in Bodri watershed over the last two decades (1991-2001 and 2001-2009). Three data sets of Landsat image acquired on June 28, 1991; July 1, 2001 and June 21, 2009 were used for this analysis. Image classification for mapping LULC was performed by supervised classification through the maximum likelihood method. All of the visible and infrared bands (bands 1-5 and 7) were used for the analysis. LULC change processes were then detected through post classification comparison method. Analyzing LULC changes associated to its slope and elevation were performed by overlaying the classified image with slope and elevation maps which had been extracted from digital topographic map (1:25,000). Six LULC categories were classified as forest, tree plantation, dry farming field, paddy field, settlement and water. The results show that in the period of 1991-2001 there were increases in settlement (56.22%), dry farming field (41.77%) and water (32.41%), and decrease in forest (-31.85%), tree plantation (-15.19%) and paddy field (-12.23%). While, between 2001 and 2009 increase occurred in water (34.17%), tree plantation (12.63%) and settlement (7.47%), and decrease in paddy field (26.01%), forest (-12.33%) and dry farming field (-1.14%). The changes predominantly took place in the gentle slope (0-8%) and the low elevation (0-500 m) areas. These results show that urbanization and agricultural activities have occurred in these areas. The low land and gentle slope were more widely affected by and vulnerable to human and agriculture caused LULC changes than highland and steep slope areas. 
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,"Land Use and Land Cover Change Impact on Groundwater Recharge: The Case of Lake Haramaya Watershed, Ethiopia","Shimelis B. Gebere, Tena Alamirew, Broder J. Merkel, Assefa M. Melesse","Anthropogenic actions have been dramatically changing the land cover of the earth with a substantial impact on the soil, water, and atmosphere. Haramaya watershed is located in the eastern part of Ethiopia which encompasses the dry Lake Haramaya, primarily used for agricultural production with burgeoning population, dramatic changes in land use land cover have been observed over the past few decades. The land cover changes have impacted the water balance of the watershed by changing groundwater level. This study focuses on assessing the impact of land use land cover changes on groundwater recharge potential of the watershed. Future land use change was simulated using CLUE-S (Conversion of Land Use and its Effects at Small regional extent) land use change model. The result showed an increase in chat cultivation from 6276 ha in 2011 to 7282 and 7000 ha in 2028 under current conditions (scenario-1) and good watershed management (scenario-2), respectively. Chat (Catha edulis) also referred to as Khat, is a stimulant plant chewed as a tradition but labeled as drug by the World Health Organization (WHO). Cultivated land declined from 4975 ha in 2011 to 3999 and 4013 ha in 2028 under both scenarios 1 and 2, respectively. The simulated result of the WetSpass water balance model showed that the groundwater recharge in the watershed is strongly influenced by land use land cover change. The annual groundwater recharge in the year 2011 ranged from 0 to 90 mm. A land use land cover projection to 2028 with baseline and good management scenarios showed the range of recharge values decreased to 0–83 and 0–87 mm, respectively. At the same time, groundwater level will continue declining due to increased abstraction. Therefore, it is recommended that the concerned authorities should consider the impact of land use change on the water resources of the watershed in order to optimally utilize the available water resources and to find alternative water sources to fill the deficit resulting from groundwater table decline.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,"Spatial modelling of groundwater quality across a land use and land cover gradient in Limpopo Province, South Africa","Timothy Dube, Cletah Shoko, Mbulisi Sibanda, Moses M. Baloyi, Mmasabata Molekoa, Dimakatso Nkuna, Bentley Rafapa, Berel M. Rampheri","The spatial variability of groundwater quality in rural Mokopane District is investigated. This was achieved by evaluating the variability in groundwater physio-chemical parameters in relation to Land Use and Land Cover (LULC)using Kriging geo-statistical technique and key groundwater physio-chemical parameters namely; ammonia, lead, Total Dissolved Solids (TDS), nitrates and chloride. Firstly, we derived the LULC from 30-m Landsat 8-OLI satellite data, with an overall accuracy of 76.67%. Water quality parameters were modelled in geographic information systems using the kriging interpolation technique. We then compared measured water quality and the drinking standards recommended by the South African Water Quality Standards (SAWS) and the World Health Organization (WHO) guideline standards for drinking water. Results indicated that there were significant differences (p < 0.05) in the levels of physio-chemical contents between borehole water and standard values according to the SAWS and WHO. For example, TDS in Mosesetjane exceeded both SAWS and WHO drinking water levels. Nitrate concentrations were also high on the first and second quarter with concentration levels of 5.56–29 mg/l and 30.9 mg/l in Mosesetjane, respectively. However, lead and ammonia were found to be evenly distributed in all the four quarters, but exhibited some slight deviations from the standard levels (±4.7 mg/l) in selected quarters. Results also indicated significant differences in TDS, chloride and nitrate concentrations across different LULC types. Subsistence farming and built-up areas were the main LULC types that were found to have high concentrations of TDS, ammonia and nitrates, exceeding the recommended standards. However, lead and ammonia, did not differ significantly (p > 0.05) and these parameters were fairly distributed, with less spatial variability . The second quarter, extending between October and December, and the fourth quarter, extending from April to June, exhibited high concentrations of nitrates, chloride and TDS during the wet season. On the other hand, the first and third quarter of the dry period were associated with low concentrations. This study indicated that LULC types and seasonal variability have influence on specific groundwater quality parameters.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Classifying Multivariate Time Series by Learning Sequence-level Discriminative Patterns,"Guruprasad Nayak, Varun Mithal, Xiaowei Jia, Vipin Kumar","Time series classification algorithms designed to use local context do not work on landcover classification problems where the instances of the two classes may often exhibit similar feature values due to the large natural variations in other land covers across the year and unrelated phenomena that they undergo. In this paper, we propose to learn discriminative patterns from the entire length of the time series, and use them as predictive features to identify the class of interest. We propose a novel neural network algorithm to learn the key signature of the class of interest as a function of the feature values together with the discriminative pattern made from that signature through the entire time series in a joint framework. We demonstrate the utility of this technique on the landcover classification application of burned area mapping that is of considerable societal importance.
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Multimodal Deep Learning Based Crop Classification Using Multispectral and Multitemporal Satellite Imagery,"Krishna Karthik Gadiraju, Bharathkumar Ramachandra, Zexi Chen, Ranga Raju Vatsavai","The Food and Agriculture Organization (FAO) of the United Nations predicts that in order to meet the needs of the expected 3 billion population growth by 2050, food production has to increase by 60%. Therefore, monitoring and mapping crops accurately is essential for estimating food production during each crop growing season across the globe. Traditionally, multispectral remote sensing imagery has been widely used for mapping crops worldwide. However, single date imagery does not capture temporal characteristics (phenology) of growing crops, leading to imprecise crop maps and food estimates. On the other hand, purely temporal classification approaches also produce inaccurate crop maps as they do not account for spatial autocorrelations. In this paper, we present a multimodal deep learning solution that jointly exploits spatial-spectral and phenological properties to identify major crop types. Using a two stream architecture, spatial characteristics are captured via a spatial stream consisting of very high resolution images (single date, 1m, 3-spectral bands, USDA NAIP) with a CNN and the phenological characteristics via a temporal stream images (biweekly, 250m, MODIS NDVI) with an LSTM. Experimental results show that the proposed multimodal solution reduces prediction error by 60%.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Multisensor temporal approach for transplanted paddy fields mapping using fuzzy-based classifiers,"Anuvi Rawat, Anil Kumar, Priyadarshi Upadhyay, Shashi Kumar","Paddy transplantation rate mapping can provide important information regarding water demand for its transplantation. It is a time-tagged activity that cannot be accurately mapped using a single date image. Further, many times, single sensor data may not have enough temporal resolution required to map it. Furthermore, due to cloud cover, a single optical sensor is not enough to provide required temporal scenes for mapping the transplanted paddy fields. Therefore, to have a finer temporal resolution, a multisensor approach was applied in our research work by integrating an optical sensor data with a synthetic aperture radar data to map transplanted paddy fields at a frequency of 5- to 10-day intervals. In our research, the transplanted paddy fields were mapped as a specific class of interest using suitable fuzzy-based classifiers. The specific fuzzy-based classifiers explored here were modified possibilistic c-mean (MPCM) and noise clustering (NC). For evaluating the results, mean membership difference (MMD) and entropy values were computed within transplanted paddy fields and for noninterest classes. Moreover, an error matrix criterion was also used to validate the results. The computed MMD and entropy values were found to be lower within transplanted paddy fields in comparison to the other classes of noninterest, indicating accurate classification. NC algorithm outperformed the MPCM algorithm while observing MMD and entropy values for assessment. However, using the fuzzy error matrix criterion, the performance of both algorithms was identical with an average overall accuracy and kappa coefficient of 90.88% and 0.82 for MPCM and 90.66% and 0.81 for NC, respectively.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,A Comparative Study of 1D-Convolutional Neural Networks with Modified Possibilistic c-Mean Algorithm for Mapping Transplanted Paddy Fields Using Temporal Data,"Anuvi Rawat, Anil Kumar, Priyadarshi Upadhyay, Shashi Kumar","With increasing availability of satellite data of high temporal resolution, a more robust classifier is needed which can exploit the temporal information along with the spectral information of the remote sensing images. Specific fuzzy-based and learning-based algorithms are two broad categories and have the potential to perform well in spectral–temporal domain. In the present study, for mapping paddy fields as a specific class two classification algorithms, viz. fuzzy-based modified possibilistic c-mean (MPCM) algorithm and learning-based 1D-convolutional neural networks (CNN), were tested using Sentinel-2A/2B temporal data. The overall accuracy for learning-based 1D-CNN and fuzzy-based MPCM classifiers was found to be 96% and 93%, respectively. The F-measure values were found to be 0.95 and 0.92 for 1D-CNN- and MPCM-based classifier, respectively. Thus, it can be inferred from this study that the 1D-CNN classifier performed better than the traditional fuzzy-based classifier and can handle heterogeneity within class.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Importance of individual sample of training data in modified possibilistic c-means classifier for handling heterogeneity within a specific crop,"Mragank Singhal, Ashish Payal, Anil Kumar","In remote sensing images, pixel-based classifiers use means or variance-covariance statistical parameters generated from training sample data sets. These parameters do not represent in totality about variations within the class. This research work enlightens each training sample’s role in handling heterogeneity within the class instead of using statistical parameters (mean). Modified possibilistic c-means fuzzy algorithm, capable of single class mapping, has been experimented to handle heterogeneity within the class. The mapping of mustard, wheat, and grass classes has been conducted using multispectral temporal images of Sentinel-2A/2B of Banasthali, Rajasthan region. It has been observed that while using individual samples in place of statistical parameters in fuzzy-based classifiers, the individual class identified has been least affected due to heterogeneity within the class. Mean membership difference for favorable and non-favorable classes as well as F-score, kappa, and overall accuracy have been calculated. Through all these parameters, individual samples as mean outperformed other training approaches.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Crop Classification from Multi-Temporal and Multi-spectral Remote Sensing Images,"Fırat Kizilirmak, Erchan Aptoula","The number of satellites, equipped with various sensors, aiming to observe agricultural activities have been progressively increasing. Satellite technology advances have enabled the acquisition of multispectral images of a region with small temporal intervals. Consequently, changes over a region can be observed, yield forecast can be made and the type of crops can be determined. In this work, it is aimed to classify 13 different crops by processing the multi temporal and multispectral data consisting of surface reflectance values. To this end, a siamese recurrent neural network structure, that processes time series information with deep metric learning approaches and providing easier classification, is proposed. A convolutional neural network that processes the multi temporal and multispectral information like an image is proposed to reduce the effect of class imbalance problem. These models are then combined under an ensemble neural network structure in order to leverage both networks' strengths. The proposed method outperforms other studies on the literature on BreizhCrops dataset.
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Vegetable mapping using fuzzy classification of Dynamic Time Warping distances from time series of Sentinel-1A images,"Wisdom Simataa Moola, Wietske Bijker, Mariana Belgiu, Mengmeng Li","Vegetable production is important because of the food security, diet improvement and socio-economic value. Mapping the location and extent of vegetable fields is therefore important in agricultural policy, food security and farmer support. Dynamic Time Warping (DTW) is a common way to map crops from time series of satellite images. However, as all hard classifications, it does not show the spatial distribution of uncertainty in the classification. In fuzzy classification, where memberships to multiple classes are assigned to each pixel, differences in membership between the first and the runners-up class can be used to assess classification uncertainty at the pixel level. This research formulates a fuzzy classifier based upon Time-Weighted Dynamic Time Warping (TWDTW) distances to map vegetable types from time series of Sentinel-1A SAR images. For each pixel, the TWDTW distances to the classes was normalised by dividing them by the sum of all TWDTW distances to all the classes for that pixel. The normalized distances were then used to compute fuzzy memberships for each pixel to each class, using the Gaussian membership function. Based on these memberships, fuzzy measures such as Confusion Index (CI), Ambiguity Index (AI), fuzziness and fuzzy membership were calculated and different thresholds applied on each of the measures during subsequent defuzzification. The overall accuracy and kappa coefficient of the defuzzified output results were 0.86 and 0.83, respectively, which was an improvement with regard to the crisp Time-Weighted Dynamic Time Warping with SPRING strategy for subsequence searching (TWDTWS) algorithm with 0.73 and 0.68 for overall accuracy and kappa, respectively. This study concludes that this new approach improves classification accuracy in image classification by excluding pixels with high uncertainty, which is especially relevant when only a limited number of classes are sampled and mapped.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,53,0,,,
Image classification,Tune It or Don't Use It: Benchmarking Data-Efficient Image Classification,"Lorenzo Brigato, Björn Barz, Luca Iocchi, Joachim Denzler","Data-efficient image classification using deep neural networks in settings, where only small amounts of labeled data are available, has been an active research area in the recent past. However, an objective comparison between published methods is difficult, since existing works use different datasets for evaluation and often compare against untuned baselines with default hyper-parameters. We design a benchmark for data-efficient image classification consisting of six diverse datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types (RGB, grayscale, multispectral). Using this benchmark, we re-evaluate the standard cross-entropy baseline and eight methods for data-efficient deep learning published between 2017 and 2021 at renowned venues. For a fair and realistic comparison, we carefully tune the hyper-parameters of all methods on each dataset. Surprisingly, we find that tuning learning rate, weight decay, and batch size on a separate validation split results in a highly competitive baseline, which outperforms all but one specialized method and performs competitively to the remaining one.
",DEIC Benchmark Dataset,54,1,DEIC Benchmark Dataset,"Lorenzo Brigato, Björn Barz, Luca Iocchi, Joachim Denzler","DEIC is a benchmark for measuring the data efficiency of models in the context of image classification. It is composed of 6 datasets that contain a small number of training samples per class (i.e., 30 < x < 80). It covers multiple image domains (i.e., natural images, fine-grained recognition, medical images, remote sensing, handwriting recognition) and data types (i.e., RGB, grayscale, multi-spectral)."
Image classification,Parametric Scattering Networks,"Shanel Gauthier, Benjamin Thérien, Laurent Alsène-Racicot, Muawiz Chaudhary, Irina Rish, Eugene Belilovsky, Michael Eickenberg, Guy Wolf","The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate whether this standard wavelet filterbank construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform. Moreover, our empirical results suggest that traditional filterbank constructions may not always be necessary for scattering transforms to extract effective representations.
",DEIC Benchmark Dataset,54,0,,,
Image classification,Towards Benchmarking and Evaluating Deepfake Detection,"Chenhao Lin, Jingyi Deng, Pengbin Hu, Chao Shen, Qian Wang, Qi Li","Deepfake detection automatically recognizes the manipulated medias through the analysis of the difference between manipulated and non-altered videos. It is natural to ask which are the top performers among the existing deepfake detection approaches to identify promising research directions and provide practical guidance. Unfortunately, it's difficult to conduct a sound benchmarking comparison of existing detection approaches using the results in the literature because evaluation conditions are inconsistent across studies. Our objective is to establish a comprehensive and consistent benchmark, to develop a repeatable evaluation procedure, and to measure the performance of a range of detection approaches so that the results can be compared soundly. A challenging dataset consisting of the manipulated samples generated by more than 13 different methods has been collected, and 11 popular detection approaches (9 algorithms) from the existing literature have been implemented and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92 models have been trained and 644 experiments have been performed for the evaluation. The results along with the shared data and evaluation methodology constitute a benchmark for comparing deepfake detection approaches and measuring progress.
",DEIC Benchmark Dataset,54,0,,,
Image classification,Model Assumptions and Data Characteristics: Impacts on Domain Adaptation in Building Segmentation,"Philipe Dias, Yuxin Tian, Shawn Newsam,  Aristeidis Tsaris, Jacob Hinkle,  Dalton Lunga","Studies on domain adaptation (DA) for remote sensing (RS) imagery analysis lack consistency in selection and description of evaluation scenarios. Without properly characterizing datasets, model assumptions, and evaluation scenarios, it is difficult to objectively compare DA methods and reach conclusions about their suitability across different applications. With this motivation, this work seeks to empirically assess to which extent the interaction between data characteristics and model assumptions influences the effectiveness of DA methods. Using the widely explored task of building footprint segmentation as a case study, we perform a large-scale study across over 200 DA scenarios that include variations across view angles, areas observed, and sensors used for data acquisition. Rather than adopting different model architectures or optimization criteria, we contrast the performances of two DA methods based on adversarial learning that differ only in their assumptions about source and target domains. Informed by metadata and data characteristics unveiled using traditional computer vision (CV) techniques as well as pretrained deep models, we provide a detailed meta-analysis of experiments highlighting the importance of accurately considering data assumptions for DA in RS segmentation tasks. As demonstrated by a “cherry-picking” exercise, different claims regarding which model is best could be made by selecting different subsets of evaluation scenarios. While well-calibrated assumptions can be beneficial, mismatching assumptions can lead to negative biases in DA applications. This study intends to motivate the community toward more consistent evaluation protocols while providing recommendations and insights toward creating novel benchmark datasets, documenting data characteristics, application-specific knowledge, and model assumptions.
",DEIC Benchmark Dataset,54,0,,,
Image classification,A Strong Baseline for the VIPriors Data-Efficient Image Classification Challenge,"Björn Barz, Lorenzo Brigato, Luca Iocchi, Joachim Denzler","Learning from limited amounts of data is the hallmark of intelligence, requiring strong generalization and abstraction skills. In a machine learning context, data-efficient methods are of high practical importance since data collection and annotation are prohibitively expensive in many domains. Thus, coordinated efforts to foster progress in this area emerged recently, e.g., in the form of dedicated workshops and competitions. Besides a common benchmark, measuring progress requires strong baselines. We present such a strong baseline for data-efficient image classification on the VIPriors challenge dataset, which is a sub-sampled version of ImageNet-1k with 100 images per class. We do not use any methods tailored to data-efficient classification but only standard models and techniques as well as common competition tricks and thorough hyper-parameter tuning. Our baseline achieves 69.7% accuracy on the VIPriors image classification dataset and outperforms 50% of submissions to the VIPriors 2021 challenge.
",DEIC Benchmark Dataset,54,0,,,
Image classification,Fuzzy Deep Forest with Deep Contours Feature for Leaf Cultivar Classification,"Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang","Deep learning is a compelling technique for feature extraction due to its adaptive capacity of processing and providing deeper image information. However, for the task of leaf cultivar classification, the deep learning-based classifier model is unable to extract contour features of leaf images deeply due to the lack of large specialized datasets and expert knowledge annotations. Also, the scale/size of the current leaf cultivar dataset does not meet the needs of deep neural networks. In particular, the high model complexity of deep neural networks (DNN) implies that deep-learning-based neural networks seem to must require a large dataset to achieve good performance, but facing the fact that the leaf cultivar dataset often is small, even some classes in this kind of datasets contain less than ten images/examples. To overcome these problems and inspired by the resounding success of fuzzy logic, we propose a novel fuzzy ensemble model for leaf cultivar classification. To extract the contours of leaves, we first propose generative adversarial networks (GANs) based methods. Second, to improve the ability of feature representation, we present a data augmentation method to transform our contour features. Thirdly, to get the essential features of leaves, we design a novel generation of the fuzzy random forest. Finally, to achieve accurate classification, we design a novel deep learning strategy, namely fuzzy deep representation learning, integrating and cascading a lot of our fuzzy random forests. Experimental results show that our model outperforms other existing stateof- the-arts on three real-world datasets, and performs much better than the original deep forest and DNN-based algorithms particularly.
",DEIC Benchmark Dataset,54,0,,,
Image classification,Exploring the Optimality of Tight-Frame Scattering Networks,"Shanel Gauthier, Benjamin Thérien, Laurent Alsène-Racicot, Muawiz Sajjad Chaudhary, Irina Rish, Eugene Belilovsky, Michael Eickenberg, Guy Wolf","The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations, and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate if such a tight frame construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform.  Moreover, our empirical results suggest that tight-frames may not always be necessary for scattering transforms to extract effective representations.",DEIC Benchmark Dataset,54,0,,,
Image classification,Species-level image classification with convolutional neural network enables insect identification from habitus images,"Oskar L. P. Hansen, Jens-Christian Svenning, Kent Olsen, Steen Dupont, Beulah H. Garner, Alexandros Iosifidis,Benjamin W. Price, Toke T. Høye","Changes in insect biomass, abundance, and diversity are challenging to track at sufficient spatial, temporal, and taxonomic resolution. Camera traps can capture habitus images of ground-dwelling insects. However, currently sampling involves manually detecting and identifying specimens. Here, we test whether a convolutional neural network (CNN) can classify habitus images of ground beetles to species level, and estimate how correct classification relates to body size, number of species inside genera, and species identity.
We created an image database of 65,841 museum specimens comprising 361 carabid beetle species from the British Isles and fine-tuned the parameters of a pretrained CNN from a training dataset. By summing up class confidence values within genus, tribe, and subfamily and setting a confidence threshold, we trade-off between classification accuracy, precision, and recall and taxonomic resolution.
The CNN classified 51.9% of 19,164 test images correctly to species level and 74.9% to genus level. Average classification recall on species level was 50.7%. Applying a threshold of 0.5 increased the average classification recall to 74.6% at the expense of taxonomic resolution. Higher top value from the output layer and larger sized species were more often classified correctly, as were images of species in genera with few species.
Fine-tuning enabled us to classify images with a high mean recall for the whole test dataset to species or higher taxonomic levels, however, with high variability. This indicates that some species are more difficult to identify because of properties such as their body size or the number of related species.
Together, species-level image classification of arthropods from museum collections and ecological monitoring can substantially increase the amount of occurrence data that can feasibly be collected. These tools thus provide new opportunities in understanding and predicting ecological responses to environmental change.","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,1,"Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images ""","Oskar L. P. Hansen, Jens-Christian Svenning, Kent Olsen, Steen Dupont, Beulah H. Garner, Alexandros Iosifidis,Benjamin W. Price, Toke T. Høye","Image-crops of specimens from insect drawers In 2017 we scanned 208 insect drawers containing the collection of british carabids from the Natural History Museum London and extracted crops from the scanned images. This database contain 63.364 specimens that we used to train, validate and test a convolutional neural network. Each folder is named as the gbif id number. E.g. Carabus problematicus is 4470555: https://www.gbif.org/species/4470555"
Image classification,Deep learning and computer vision will transform entomology,"Toke T. Høye, Johanna Ärje, Kim Bjerge, Oskar L. P. Hansen, Alexandros Iosifidis, Florian Leese, Hjalte M. R. Mann, Kristian Meissner, Claus Melvad, Jenni Raitoharju","Most animal species on Earth are insects, and recent reports suggest that their abundance is in drastic decline. Although these reports come from a wide range of insect taxa and regions, the evidence to assess the extent of the phenomenon is sparse. Insect populations are challenging to study, and most monitoring methods are labor intensive and inefficient. Advances in computer vision and deep learning provide potential new solutions to this global challenge. Cameras and other sensors can effectively, continuously, and noninvasively perform entomological observations throughout diurnal and seasonal cycles. The physical appearance of specimens can also be captured by automated imaging in the laboratory. When trained on these data, deep learning models can provide estimates of insect abundance, biomass, and diversity. Further, deep learning models can quantify variation in phenotypic traits, behavior, and interactions. Here, we connect recent developments in deep learning and computer vision to the urgent demand for more cost-efficient monitoring of insects and other invertebrates. We present examples of sensor-based monitoring of insects. We show how deep learning tools can be applied to exceptionally large datasets to derive ecological information and discuss the challenges that lie ahead for the implementation of such solutions in entomology. We identify four focal areas, which will facilitate this transformation: 1) validation of image-based taxonomic identification; 2) generation of sufficient training data; 3) development of public, curated reference databases; and 4) solutions to integrate deep learning and molecular tools.
","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Connected Carabids: Network Interactions and Their Impact on Biocontrol by Carabid Beetles,"Stefanie E De Heij, Christian J Willenborg","Carabid beetles can greatly contribute to biocontrol in agroecosystems, reducing both insect pests and weed seeds. However, insect foraging and feeding behavior can be highly dependent on the interaction network and spatial structure of the environment, which can make their biocontrol contributions variable. In the present article, we explore how the interaction network of carabids can affect their behavior and how spatial vegetation structure and specific agronomy practices can, in turn, affect the strength of interactions in their network. We suggest that research on carabid biocontrol should move toward an approach in which the network of interactions among pests, carabids, and other organisms within its spatial structure is evaluated, with equal focus on direct and indirect interactions, and provide examples of tools to do so. Overall, we believe this approach will improve our knowledge of carabid networks, help to elucidate the underlying mechanisms of biocontrol, and lay the foundation for future biocontrol strategies.

","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Deep learning as a tool for ecology and evolution,"Marek L. Borowiec, Rebecca B. Dikow, Paul B. Frandsen, Alexander McKeeken, Gabriele Valentini, Alexander E. White","Deep learning is driving recent advances behind many everyday technologies, including speech and image recognition, natural language processing and autonomous driving. It is also gaining popularity in biology, where it has been used for automated species identification, environmental monitoring, ecological modelling, behavioural studies, DNA sequencing and population genetics and phylogenetics, among other applications. Deep learning relies on artificial neural networks for predictive modelling and excels at recognizing complex patterns.
In this review we synthesize 818 studies using deep learning in the context of ecology and evolution to give a discipline-wide perspective necessary to promote a rethinking of inference approaches in the field. We provide an introduction to machine learning and contrast it with mechanistic inference, followed by a gentle primer on deep learning. We review the applications of deep learning in ecology and evolution and discuss its limitations and efforts to overcome them. We also provide a practical primer for biologists interested in including deep learning in their toolkit and identify its possible future applications.
We find that deep learning is being rapidly adopted in ecology and evolution, with 589 studies (64%) published since the beginning of 2019. Most use convolutional neural networks (496 studies) and supervised learning for image identification but also for tasks using molecular data, sounds, environmental data or video as input. More sophisticated uses of deep learning in biology are also beginning to appear.
Operating within the machine learning paradigm, deep learning can be viewed as an alternative to mechanistic modelling. It has desirable properties of good performance and scaling with increasing complexity, while posing unique challenges such as sensitivity to bias in input data. We expect that rapid adoption of deep learning in ecology and evolution will continue, especially in automation of biodiversity monitoring and discovery and inference from genetic data. Increased use of unsupervised learning for discovery and visualization of clusters and gaps, simplification of multi-step analysis pipelines, and integration of machine learning into graduate and postgraduate training are all likely in the near future.","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Identification of Indian butterflies using Deep Convolutional Neural Network,Hari Theivaprakasham,"The conventional butterfly identification method is based on their different morphological characters namely wing-venation, color, shape, patterns and through the dissection studies and molecular techniques which are tedious, expensive and highly time-consuming. To overcome the above aforesaid challenges, a new butterfly identification system using butterfly images has been designed to instantly identify the butterfly with high accuracy. In this study, we construct a new butterfly dataset with 34,024 butterfly images belonging to 315 species from India. We propose and prove the effectiveness of new data augmentation techniques on our dataset. To identify butterflies using photographic images, we built eleven new Deep Convolutional Neural Network (DCNN) butterfly classifier models using eleven pre-trained architectures namely ResNet-18, ResNet-34, ResNet-50, ResNet-121, ResNet-152, Alex-Net, DenseNet-121, DenseNet-161, VGG-16, VGG-19 and SqueezeNet-v1.1. The different model's classification results were compared and the proposed technique achieved a maximum top-1 accuracy(94.44%), top-3 accuracy(98.46%) and top-5 accuracy(99.09%) using ResNet-152 model, followed by DenseNet-161 model achieved the top-1 accuracy(94.31%), top-3 accuracy (98.07%) and top-5 accuracy (98.66%). The results suggest that models can be assertively used to identify butterflies in India.

","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Pretrained Convolutional Neural Networks Perform Well in a Challenging Test Case: Identification of Plant Bugs (Hemiptera: Miridae) Using a Small Number of Training Images,"Alexander Knyshov, Samantha Hoang, Christiane Weirauch","Automated insect identification systems have been explored for more than two decades but have only recently started to take advantage of powerful and versatile convolutional neural networks (CNNs). While typical CNN applications still require large training image datasets with hundreds of images per taxon, pretrained CNNs recently have been shown to be highly accurate, while being trained on much smaller datasets. We here evaluate the performance of CNN-based machine learning approaches in identifying three curated species-level dorsal habitus datasets for Miridae, the plant bugs. Miridae are of economic importance, but species-level identifications are challenging and typically rely on information other than dorsal habitus (e.g., host plants, locality, genitalic structures). Each dataset contained 2–6 species and 126–246 images in total, with a mean of only 32 images per species for the most difficult dataset. We find that closely related species of plant bugs can be identified with 80–90% accuracy based on their dorsal habitus alone. The pretrained CNN performed 10–20% better than a taxon expert who had access to the same dorsal habitus images. We find that feature extraction protocols (selection and combination of blocks of CNN layers) impact identification accuracy much more than the classifying mechanism (support vector machine and deep neural network classifiers). While our network has much lower accuracy on photographs of live insects (62%), overall results confirm that a pretrained CNN can be straightforwardly adapted to collection-based images for a new taxonomic group and successfully extract relevant features to classify insect species.

","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Deep learning approach to classify Tiger beetles of Sri Lanka,"D.L.Abeywardhana, C.D.Dangalle, Anupiya Nugaliyadde, Yashas Mallawarachchi","Deep learning has shown to achieve dramatic results in image classification tasks. However, deep learning models require large amounts of data to train. Most of the real-world datasets, generally insect classification data does not have large number of training dataset. These images have a large amount of noise and various differences. The paper proposes a novel architectural model which removes the background noise and classify the Tiger beetles. Here object location is identified using contours by converting the original coloured image to white on black background. Then the remaining background is eliminated using grabcut algorithm. Later the extracted images are classified using a modified SqueezeNet transfer learning model to identify the tiger beetle class up to genus level. Transfer learning models with fewer trainable parameters performed well than the total number of parameters in the original model. When evaluating results it was identified that by freezing uppermost layers of SqueezeNet model better accuracy can be gained while freezing lowermost layers will reduce the validation accuracy. The proposed model achieved more than 90% for the test set in 40 epochs using 701,481 trainable parameters by freezing the top 19 layers of the original model. Improving the pre-processing to localize insect has improved the accuracy.

","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,High Throughput Data Acquisition and Deep Learning for Insect Ecoinformatics,"Alexander Gerovichev, Achiad Sadeh, Vlad Winter, Avi Bar-Massada, Tamar Keasar, Chen Keasar","Ecology documents and interprets the abundance and distribution of organisms. Ecoinformatics addresses this challenge by analyzing databases of observational data. Ecoinformatics of insects has high scientific and applied importance, as insects are abundant, speciose, and involved in many ecosystem functions. They also crucially impact human well-being, and human activities dramatically affect insect demography and phenology. Hazards, such as pollinator declines, outbreaks of agricultural pests and the spread insect-borne diseases, raise an urgent need to develop ecoinformatics strategies for their study. Yet, insect databases are mostly focused on a small number of pest species, as data acquisition is labor-intensive and requires taxonomical expertise. Thus, despite decades of research, we have only a qualitative notion regarding fundamental questions of insect ecology, and only limited knowledge about the spatio-temporal distribution of insects. We describe a novel high throughput cost-effective approach for monitoring flying insects as an enabling step toward “big data” entomology. The proposed approach combines “high tech” deep learning with “low tech” sticky traps that sample flying insects in diverse locations. As a proof of concept we considered three recent insect invaders of Israel’s forest ecosystem: two hemipteran pests of eucalypts and a parasitoid wasp that attacks one of them. We developed software, based on deep learning, to identify the three species in images of sticky traps from Eucalyptus forests. These image processing tasks are quite difficult as the insects are small (<5 mm) and stick to the traps in random poses. The resulting deep learning model discriminated the three focal organisms from one another, as well as from other elements such as leaves and other insects, with high precision. We used the model to compare the abundances of these species among six sites, and validated the results by manually counting insects on the traps. Having demonstrated the power of the proposed approach, we started a more ambitious study that monitors these insects at larger spatial and temporal scales. We aim at building an ecoinformatics repository for trap images and generating data-driven models of the populations’ dynamics and morphological traits.","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Automated image-based taxon identification using deep learning and citizen-science contributions,Miroslav Valan,"The sixth mass extinction is well under way, with biodiversity disappearing at unprecedented rates in terms of species
richness and biomass. At the same time, given the currentpace, we would need the next two centuries to complete the
inventory of life on Earthand this is only one of the necessary steps toward monitoring and conservation of species.
Clearly, there is an urgent need to accelerate the inventory and the taxonomic researchrequired to identify and describe
the remaining species, a critical bottleneck. Arguably, leveraging recent technological innovations is our best chance to
speed up taxonomic research. Given that taxonomy has been and still is notably visual, and the recent break-throughs in
computer vision and machine learning, it seems that the time is ripe to exploreto what extent we can accelerate morphologybased taxonomy using these advances inartificial intelligence. Unfortunately, these so-called deep learning systems often
requiresubstantial computational resources, large volumes of labeled training data and sophisticated technical support,
which are rarely available to taxonomists. This thesis is devoted to addressing these challenges. In paper I and paper II,
we focus on developing an easy-to-use (’off-the-shelf’) solution to automated image-based taxon identification, which is
at the same time reliable, inexpensive, and generally applicable. This enables taxonomists to build their own automated
identification systems without prohibitive investments in imaging and computation. Our proposed solution utilizes a
technique called feature transfer, in which a pretrained convolutional neural network (CNN) is used to obtain image
representations (”deep features”) for a taxonomic task of interest. Then, these features are used to train a simpler system,
such as a linear support vector machine classifier. In paper I we optimized parameters for feature transfer on a range of
challenging taxonomic tasks, from the identification of insects to higher groups --- even when they are likely to belong
to subgroups that have not been seen previously --- to the identification of visually similar species that are difficult to
separate for human experts. In paper II, we applied the optimal approach from paper I to a new set of tasks, including a task
unsolvable by humans - separating specimens by sex from images of body parts that were not previously known to show
any sexual dimorphism. Papers I and II demonstrate that off-the-shelf solutions often provide impressive identification
performance while at the same time requiring minimal technical skills. In paper III, we show that phylogenetic information
describing evolutionary relationships among organisms can be used to improve the performance of AI systems for taxon
identification. Systems trained with phylogenetic information do as well as or better than standard systems in terms of
common identification performance metrics. At the same time, the errors they make are less wrong in a biological sense, and
thus more acceptable to humans. Finally, in paper IV we describe our experience from running a large-scale citizen science
project organized in summer 2018, the Swedish Ladybird Project, to collect images for training automated identification
systems for ladybird beetles. The project engaged more than 15,000 school children, who contributed over 5,000 images
and over 15,000 hours of effort. The project demonstrates the potential of targeted citizen science efforts in collecting the
required image sets for training automated taxonomic identification systems for new groups of organisms, while providing
many positive educational and societal side effects.","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Machine Learning in Ethnobotany,"Marc Böhlen, Wawan Sujarwo","We describe new opportunities created by bring A.I. to
the field of ethnobotany. In particular we describe a novel approach to
ethnobotany documentation that harnesses machine learning
opportunities, specifically for the documentation of traditional
ecological knowledge with mobile phones in emerging economies.
Using a case study on the island of Bali as a departure point, the project
maps out machine learning approaches to documentation and responds
to technology and capital gradients between research contexts in the
global north and south in an attempt to capture knowledge that might
otherwise not be represented","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,Real-time insect tracking and monitoring with computer vision and deep learning,"Kim Bjerge, Hjalte M. R. Mann, Toke Thomas Høye","Insects are declining in abundance and diversity, but their population trends remain uncertain as insects are difficult to monitor. Manual methods require substantial time investment in trapping and subsequent species identification. Camera trapping can alleviate some of the manual fieldwork, but the large quantities of image data are challenging to analyse. By embedding the image analyses into the recording process using computer vision techniques, it is possible to focus efforts on the most ecologically relevant image data. Here, we present an intelligent camera system, capable of detecting, tracking, and identifying individual insects in situ. We constructed the system from commercial off-the-shelf components and used deep learning open source software to perform species detection and classification. We present the Insect Classification and Tracking algorithm (ICT) that performs real-time classification and tracking at 0.33 frames per second. The system can upload summary data on the identity and movement track of insects to a server via the internet on a daily basis. We tested our system during the summer 2020 and detected 2994 insect tracks across 98 days. We achieved an average precision of 89% for correctly classified insect tracks of eight different species. This result was based on 504 manually verified tracks observed in videos during 10 days with varying insect activities. Using the track data, we could estimate the mean residence time for individual flower visiting insects within the field of view of the camera, and we were able to show a substantial variation in residence time among insect taxa. For honeybees, which were most abundant, residence time also varied through the season in relation to the plant species in bloom. Our proposed automated system showed promising results in non-destructive and real-time monitoring of insects and provides novel information about phenology, abundance, foraging behaviour, and movement ecology of flower visiting insects.

","Image data used for publication ""Species-level image classification with convolutional neural network enable insect identification from habitus images """,55,0,,,
Image classification,A Hybrid Geometric Spatial Image Representation for scene classification,"Nouman Ali, Bushra Zafar, Faisal Riaz, Saadat Hanif Dar, Naeem Iqbal Ratyal, Khalid Bashir Bajwa, Muhammad Kashif Iqbal, Muhammad Sajid","The recent development in the technology has increased the complexity of image contents and demand for image classification becomes more imperative. Digital images play a vital role in many applied domains such as remote sensing, scene analysis, medical care, textile industry and crime investigation. Feature extraction and image representation is considered as an important step in scene analysis as it affects the image classification performance. Automatic classification of images is an open research problem for image analysis and pattern recognition applications. The Bag-of-Features (BoF) model is commonly used to solve image classification, object recognition and other computer vision-based problems. In BoF model, the final feature vector representation of an image contains no information about the co-occurrence of features in the 2D image space. This is considered as a limitation, as the spatial arrangement among visual words in image space contains the information that is beneficial for image representation and learning of classification model. To deal with this, researchers have proposed different image representations. Among these, the division of image-space into different geometric sub-regions for the extraction of histogram for BoF model is considered as a notable contribution for the extraction of spatial clues. Keeping this in view, we aim to explore a Hybrid Geometric Spatial Image Representation (HGSIR) that is based on the combination of histograms computed over the rectangular, triangular and circular regions of the image. Five standard image datasets are used to evaluate the performance of the proposed research. The quantitative analysis demonstrates that the proposed research outperforms the state-of-art research in terms of classification accuracy.

",MSRC-v2 image dataset.,56,1,MSRC-v2 image dataset.,"Nouman Ali, Bushra Zafar                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ","At Microsoft Research in Cambridge we are developing new machine vision algorithms for automatic recognition and segmentation of many different object categories. We are interested in both the supervised and unsupervised scenarios.

Research data
Download labelled image databases for supervised learning in the “Downloads” link below. The data provided here may be used freely for research purposes but it cannot be used for commercial purposes.

Database of thousands of weakly labelled, high-res images.
Pixel-wise labelled image database v1 (240 images, 9 object classes)
Pixel-wise labelled image database v2 (591 images, 23 object classes)                                                                                                                                                                                                                                                                                                                                                          Source: https://figshare.com/articles/dataset/MSRC-v2_image_dataset/6075788"""
Image classification,Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments,"Farzad Niroui, Kaicheng Zhang,  Zendai Kashino, Goldie Nejat","Rescue robots can be used in urban search and rescue (USAR) applications to perform the important task of exploring unknown cluttered environments. Due to the unpredictable nature of these environments, deep learning techniques can be used to perform these tasks. In this letter, we present the first use of deep learning to address the robot exploration task in USAR applications. In particular, we uniquely combine the traditional approach of frontier-based exploration with deep reinforcement learning to allow a robot to autonomously explore unknown cluttered environments. Experiments conducted with a mobile robot in unknown cluttered environments of varying sizes and layouts showed that the proposed exploration approach can effectively determine appropriate frontier locations to navigate to, while being robust to different environment layouts and sizes. Furthermore, a comparison study with other frontier exploration approaches showed that our learning-based frontier exploration technique was able to explore more of an environment earlier on, allowing for potential identification of a larger number of victims at the beginning of the time-critical exploration task.
",MSRC-v2 image dataset.,56,0,,,
Image classification,Content-Based Image Retrieval and Feature Extraction: A Comprehensive Review,"Afshan Latif, Aqsa Rasheed, Umer Sajid, Jameel Ahmed, Nouman Ali, Naeem Iqbal Ratyal, Bushra Zafar, Saadat Hanif Dar, Muhammad Sajid, Tehmina Khalil","Multimedia content analysis is applied in different real-world computer vision applications, and digital images constitute a major part of multimedia data. In last few years, the complexity of multimedia contents, especially the images, has grown exponentially, and on daily basis, more than millions of images are uploaded at different archives such as Twitter, Facebook, and Instagram. To search for a relevant image from an archive is a challenging research problem for computer vision research community. Most of the search engines retrieve images on the basis of traditional text-based approaches that rely on captions and metadata. In the last two decades, extensive research is reported for content-based image retrieval (CBIR), image classification, and analysis. In CBIR and image classification-based models, high-level image visuals are represented in the form of feature vectors that consists of numerical values. The research shows that there is a significant gap between image feature representation and human visual understanding. Due to this reason, the research presented in this area is focused to reduce the semantic gap between the image feature representation and human visual understanding. In this paper, we aim to present a comprehensive review of the recent development in the area of CBIR and image representation. We analyzed the main aspects of various image retrieval and image representation models from low-level feature extraction to recent semantic deep-learning approaches. The important concepts and major research studies based on CBIR and image representation are discussed in detail, and future research directions are concluded to inspire further research in this area.

",MSRC-v2 image dataset.,56,0,,,
Image classification,Facial emotion recognition using convolutional neural networks (FERC),Ninad Mehendale,"Facial expression for emotion detection has always been an easy task for humans, but achieving the same task with a computer algorithm is quite challenging. With the recent advancement in computer vision and machine learning, it is possible to detect emotions from images. In this paper, we propose a novel technique called facial emotion recognition using convolutional neural networks (FERC). The FERC is based on two-part convolutional neural network (CNN): The first-part removes the background from the picture, and the second part concentrates on the facial feature vector extraction. In FERC model, expressional vector (EV) is used to find the five different types of regular facial expression. Supervisory data were obtained from the stored database of 10,000 images (154 persons). It was possible to correctly highlight the emotion with 96% accuracy, using a EV of length 24 values. The two-level CNN works in series, and the last layer of perceptron adjusts the weights and exponent values with each iteration. FERC differs from generally followed strategies with single-level CNN, hence improving the accuracy. Furthermore, a novel background removal procedure applied, before the generation of EV, avoids dealing with multiple problems that may occur (for example distance from the camera). FERC was extensively tested with more than 750K images using extended Cohn–Kanade expression, Caltech faces, CMU and NIST datasets. We expect the FERC emotion detection to be useful in many applications such as predictive learning of students, lie detectors, etc.

",MSRC-v2 image dataset.,56,0,,,
Image classification,A Novel Discriminating and Relative Global Spatial Image Representation with Applications in CBIR,"Bushra Zafar, Rehan Ashraf, Nouman Ali, Muhammad Kashif Iqbal, Muhammad Sajid, Saadat Hanif Dar, Naeem Iqbal Ratyal","The requirement for effective image search, which motivates the use of Content-Based Image Retrieval (CBIR) and the search of similar multimedia contents on the basis of user query, remains an open research problem for computer vision applications. The application domains for Bag of Visual Words (BoVW) based image representations are object recognition, image classification and content-based image analysis. Interest point detectors are quantized in the feature space and the final histogram or image signature do not retain any detail about co-occurrences of features in the 2D image space. This spatial information is crucial, as it adversely affects the performance of an image classification-based model. The most notable contribution in this context is Spatial Pyramid Matching (SPM), which captures the absolute spatial distribution of visual words. However, SPM is sensitive to image transformations such as rotation, flipping and translation. When images are not well-aligned, SPM may lose its discriminative power. This paper introduces a novel approach to encoding the relative spatial information for histogram-based representation of the BoVW model. This is established by computing the global geometric relationship between pairs of identical visual words with respect to the centroid of an image. The proposed research is evaluated by using five different datasets. Comprehensive experiments demonstrate the robustness of the proposed image representation as compared to the state-of-the-art methods in terms of precision and recall values.
",MSRC-v2 image dataset.,56,0,,,
Image classification,Weakly Supervised Fruit Counting for Yield Estimation Using Spatial Consistency,"Enrico Bellocchio, Thomas A. Ciarfuglia, Gabriele Costante, Paolo Valigi","Fruit counting is a fundamental component for yield estimation applications. Most of the existing approaches address this problem by relying on fruit models (i.e., by using object detectors) or by explicitly learning to count. Despite the impressive results achieved by these approaches, all of them need strong supervision information during the training phase. In agricultural applications, manual labeling may require a huge effort or, in some cases, it could be impossible to acquire fine-grained ground truth labels. In this letter, we tackle this problem by proposing a weakly supervised framework that learns to count fruits without the need for task-specific supervision labels. In particular, we devise a novel convolutional neural network architecture that requires only a simple image level binary classifier to detect whether the image contains instances of the fruits or not and combines this information with image spatial consistency constraints. The result is an architecture that learns to count without task-specific labels (e.g., object bounding boxes or the multiplicity of fruit instances in the image). The experiments on three different varieties of fruits (i.e., olives, almonds, and apples) show that our approach reaches performances that are comparable with SotA approaches based on the supervised paradigm.
",MSRC-v2 image dataset.,56,0,,,
Image classification,Modeling global geometric spatial information for rotation invariant classification of satellite images,"Nouman Ali, Bushra Zafar, Muhammad Kashif Iqbal, Muhammad Sajid, Muhammad Yamin Younis, Saadat Hanif Dar, Muhammad Tariq Mahmood, Ik Hyun Lee","The classification of high-resolution satellite images is an open research problem for computer vision research community. In last few decades, the Bag of Visual Word (BoVW) model has been used for the classification of satellite images. In BoVW model, an orderless histogram of visual words without any spatial information is used as image signature. The performance of BoVW model suffers due to this orderless nature and addition of spatial clues are reported beneficial for scene and geographical classification of images. Most of the image representations that can compute image spatial information as are not invariant to rotations. A rotation invariant image representation is considered as one of the main requirement for satellite image classification. This paper presents a novel approach that computes the spatial clues for the histograms of BoVW model that is robust to the image rotations. The spatial clues are calculated by computing the histograms of orthogonal vectors. This is achieved by calculating the magnitude of orthogonal vectors between Pairs of Identical Visual Words (PIVW) relative to the geometric center of an image. The comparative analysis is performed with recently proposed research to obtain the best spatial feature representation for the satellite imagery. We evaluated the proposed research for image classification using three standard image benchmarks of remote sensing. The results and comparisons conducted to evaluate this research show that the proposed approach performs better in terms of classification accuracy for a variety of datasets based on satellite images.

",MSRC-v2 image dataset.,56,0,,,
Image classification,Visual complexity modelling based on image features fusion of multiple kernels,"Carlos Fernandez-Lozano​, Adrian Carballal​​, Penousal Machado, Antonino Santos, Juan Romero","Humans’ perception of visual complexity is often regarded as one of the key principles of aesthetic order, and is intimately related to the physiological, neurological and, possibly, psychological characteristics of the human mind. For these reasons, creating accurate computational models of visual complexity is a demanding task. Building upon on previous work in the field (Forsythe et al., 2011; Machado et al., 2015) we explore the use of Machine Learning techniques to create computational models of visual complexity. For that purpose, we use a dataset composed of 800 visual stimuli divided into five categories, describing each stimulus by 329 features based on edge detection, compression error and Zipf’s law. In an initial stage, a comparative analysis of representative state-of-the-art Machine Learning approaches is performed. Subsequently, we conduct an exhaustive outlier analysis. We analyze the impact of removing the extreme outliers, concluding that Feature Selection Multiple Kernel Learning obtains the best results, yielding an average correlation to humans’ perception of complexity of 0.71 with only twenty-two features. These results outperform the current state-of-the-art, showing the potential of this technique for regression.",MSRC-v2 image dataset.,56,0,,,
Image classification,HDF: Hybrid Deep Features for Scene Image Representation,"Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu","Nowadays it is prevalent to take features extracted from pre-trained deep learning models as image representations which have achieved promising classification performance. Existing methods usually consider either object-based features or scene-based features only. However, both types of features are important for complex images like scene images, as they can complement each other. In this paper, we propose a novel type of features -- hybrid deep features, for scene images. Specifically, we exploit both object-based and scene-based features at two levels: part image level (i.e., parts of an image) and whole image level (i.e., a whole image), which produces a total number of four types of deep features. Regarding the part image level, we also propose two new slicing techniques to extract part based features. Finally, we aggregate these four types of deep features via the concatenation operator. We demonstrate the effectiveness of our hybrid deep features on three commonly used scene datasets (MIT-67, Scene-15, and Event-8), in terms of the scene image classification task. Extensive comparisons show that our introduced features can produce state-of-the-art classification accuracies which are more consistent and stable than the results of existing features across all datasets.
",MSRC-v2 image dataset.,56,0,,,
Image classification,A new axiomatic methodology for the image similarity,Alessia Amelio,"In this paper, a new theoretical framework is introduced which explores the notion of image similarity in two axiomatic conditions. The first one resembles to the concept of frequency in matching image patches. The second one formalizes the high-level strategy for computing the image similarity. A realization of this theoretical framework is a new image similarity measure based on approximate matching of patches shared between the images. The approximate match is defined in terms of normalized 2D Hamming distance, and verified within a given neighbourhood from the position of the patches in the images. The similarity measure is computed as the average area of these shared patches. The proposed approach is tested on well-known benchmark datasets. The obtained results show that the proposed method overcomes, in terms of retrieval precision and computational complexity, other competing measures adopted in image retrieval.
",MSRC-v2 image dataset.,56,0,,,
Image classification,"Sparse feature selection: Relevance, redundancy and locality structure preserving guided by pairwise constraints","Zahir Noorie, Fatemeh Afsari","Selection of features as a pre-processing stage is an essential issue in many machine learning tasks (such as classification) to reduce data dimensionality as there are many irrelevant and redundant features that can mislead the learning process. Graph-based sparse feature selection is developed to overcome this issue. In this paper, a novel graph-based sparse feature selection method is proposed that take into account both issues: relevancy and redundancy analysis. An empirical loss function joining with -norm regularization term is proposed to overcome the relevancy issue and the redundancy issue is overcome by introducing a regularization term that prefers uncorrelated features. Furthermore, the proposed learning procedure is guided by two different sets of supervision information as pairs of must-linked (positive) and cannot-linked (negative) constraint sets to select a discriminative feature subset. These guiding information besides the whole data points are encoded in the graph Laplacian matrix that preserves the locality structure of the original data. The graph Laplacian matrix is constructed by two different approaches. Our first approach tries to preserve the structure of the original data guided just by the positive data points (unique samples in the must-linked constraints), and our second approach applies a normalized adapted affinity matrix to embed the pairwise must-linked and cannot-linked constraints as well as the neighborhood relationships information, all together. The experimental results on a number of several datasets from the University of California-Irvine machine learning repository, in addition to several high dimensional gene expression datasets show the efficacy of the proposed methods in the classification tasks compared to several powerful feature selection methods.

",MSRC-v2 image dataset.,56,0,,,
Image classification,Automated Flower Classification over a Large Number of Classes,"Maria-Elena Nilsback, Andrew Zisserman","We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% for the combination of all features.
",Oxford 102 Flower Dataset,57,1,Oxford 102 Flower Dataset," Maria-Elena Nilsback, Andrew Zisserman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ","Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.
The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.                                                                                                                                                                                                                                                                                                                                                                                                          Source: https://paperswithcode.com/dataset/oxford-102-flower
"
Image classification,Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning,"Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, Michal Valko","We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",Oxford 102 Flower Dataset,57,0,,,
Image classification,Caltech-UCSD Birds 200,"Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona","The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.

",Oxford 102 Flower Dataset,57,0,,,
Image classification,Training data-efficient image transformers & distillation through attention,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou","Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.
In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.
More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",Oxford 102 Flower Dataset,57,0,,,
Image classification,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,"Mingxing Tan, Quoc Le","Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.
To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",Oxford 102 Flower Dataset,57,0,,,
Image classification,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,"Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson","Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.",Oxford 102 Flower Dataset,57,0,,,
Image classification,A Simple Framework for Contrastive Learning of Visual Representations,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton","This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.
",Oxford 102 Flower Dataset,57,0,,,
Image classification,Recent advances in convolutional neural networks,"Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xing xing Wang, Gang Wang Jianfei Cai, Tsuhan Chen","In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.
",Oxford 102 Flower Dataset,57,0,,,
Image classification,StackGAN: Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks,"Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N. Metaxas","Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.
",Oxford 102 Flower Dataset,57,0,,,
Image classification,3D Object Representations for Fine-Grained Categorization,"Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei","While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultra-wide baseline matching and 3D reconstruction.
",Oxford 102 Flower Dataset,57,0,,,
Image classification,Pruning Convolutional Neural Networks for Resource Efficient Inference,"Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz","We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.
",Oxford 102 Flower Dataset,57,0,,,
Image classification,Modified shape index for object-based random forest image classification of agricultural systems using airborne hyperspectral datasets,"Eric Ariel L. Salas, Sakthi Kumaran Subburayalu","This paper highlights the importance of optimized shape index for agricultural management system analysis that utilizes the contiguous bands of hyperspectral data to define the gradient of the spectral curve and improve image classification accuracy. Currently, a number of machine learning methods would resort to using averaged spectral information over wide bandwidths resulting in loss of crucial information available in those contiguous bands. The loss of information could mean a drop in the discriminative power when it comes to land cover classes with comparable spectral responses, as in the case of cultivated fields versus fallow lands. In this study, we proposed and tested three new optimized novel algorithms based on Moment Distance Index (MDI) that characterizes the whole shape of the spectral curve. The image classification tests conducted on two publicly available hyperspectral data sets (AVIRIS 1992 Indian Pine and HYDICE Washington DC Mall images) showed the robustness of the optimized algorithms in terms of classification accuracy. We achieved an overall accuracy of 98% and 99% for AVIRIS and HYDICE, respectively. The optimized indices were also time efficient as it avoided the process of band dimension reduction, such as those implemented by several well-known classifiers. Our results showed the potential of optimized shape indices, specifically the Moment Distance Ratio Right/Left (MDRRL), to discriminate between types of tillage (corn-min and corn-notill) and between grass/pasture and grass/trees, tree and grass under object-based random forest approach.

",AVIRIS and HYDICE datasets,58,1,AVIRIS and HYDICE datasets,"Eric Ariel L. Salas, Sakthi Kumaran Subburayalu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ","The ground-truth classes of the AVIRIS and HYDICE datasets and the training and test sets used for the classes.                                                                                                                                                            
Source:https://plos.figshare.com/articles/dataset/The_ground-truth_classes_of_the_AVIRIS_and_HYDICE_datasets_and_the_training_and_test_sets_used_for_the_classes_/7815050/1
"""
Image classification,Applications of Remote Sensing in Precision Agriculture: A Review,"Rajendra P. Sishodia, Ram L. Ray, Sudhir K. Singh","Agriculture provides for the most basic needs of humankind: food and fiber. The introduction of new farming techniques in the past century (e.g., during the Green Revolution) has helped agriculture keep pace with growing demands for food and other agricultural products. However, further increases in food demand, a growing population, and rising income levels are likely to put additional strain on natural resources. With growing recognition of the negative impacts of agriculture on the environment, new techniques and approaches should be able to meet future food demands while maintaining or reducing the environmental footprint of agriculture. Emerging technologies, such as geospatial technologies, Internet of Things (IoT), Big Data analysis, and artificial intelligence (AI), could be utilized to make informed management decisions aimed to increase crop production. Precision agriculture (PA) entails the application of a suite of such technologies to optimize agricultural inputs to increase agricultural production and reduce input losses. Use of remote sensing technologies for PA has increased rapidly during the past few decades. The unprecedented availability of high resolution (spatial, spectral and temporal) satellite images has promoted the use of remote sensing in many PA applications, including crop monitoring, irrigation management, nutrient application, disease and pest management, and yield prediction. In this paper, we provide an overview of remote sensing systems, techniques, and vegetation indices along with their recent (2015–2020) applications in PA. Remote-sensing-based PA technologies such as variable fertilizer rate application technology in Green Seeker and Crop Circle have already been incorporated in commercial agriculture. Use of unmanned aerial vehicles (UAVs) has increased tremendously during the last decade due to their cost-effectiveness and flexibility in obtaining the high-resolution (cm-scale) images needed for PA applications. At the same time, the availability of a large amount of satellite data has prompted researchers to explore advanced data storage and processing techniques such as cloud computing and machine learning. Given the complexity of image processing and the amount of technical knowledge and expertise needed, it is critical to explore and develop a simple yet reliable workflow for the real-time application of remote sensing in PA. Development of accurate yet easy to use, user-friendly systems is likely to result in broader adoption of remote sensing technologies in commercial and non-commercial PA applications. View Full-Text
",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Mapping crop types in fragmented arable landscapes using AVIRIS-NG imagery and limited field data,"Eric Ariel L. Salas, Sakthi Kumaran Subburayalu, Brian Slater, Kaiguang Zhao, Bimal Bhattacharya, Rojalin Tripathy, Ayan Das, Rahul Nigam, Rucha Dave, Parshva Parekh","The fragmented nature of arable landscapes and diverse cropping patterns often thwart the precise mapping of crop types. Recent advances in remote-sensing technologies and data mining approaches offer a viable solution to this mapping problem. We demonstrated the potential of using hyperspectral imaging and an ensemble classification approach that combines five machine-learning classifiers to map crop types in the Anand District of Gujarat, India. We derived a set of narrow/broad-band indices from the Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) imagery to represent spectral variations and identify target classes and their distribution patterns. The results showed that Maximum Entropy (MaxEnt) and Generalised Linear Model (GLM) had strong discriminatory image classification abilities with Area Under the Curve (AUC) values ranging between 0.75 and 0.93 for MaxEnt and between 0.73 and 0.92 for GLM. The ensemble model resulted in improved accuracy scores compared to individual models. We found the Photochemical Reflectance Index (PRI) and Moment Distance Ratio Right/Left (MDRRL) to be important predictors for target classes such as wheat, legumes, and eggplant. Results from the study revealed the potential of using one-class ensemble modelling approach and hyperspectral images with limited field dataset to map agricultural systems that are fragmented in nature.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Species Classification in a Tropical Alpine Ecosystem Using UAV-Borne RGB and Hyperspectral Imagery,"Carol X. Garzon-Lopez, Eloisa Lasso","Páramos host more than 3500 vascular plant species and are crucial water providers for millions of people in the northern Andes. Monitoring species distribution at large scales is an urgent conservation priority in the face of ongoing climatic changes and increasing anthropogenic pressure on this ecosystem. For the first time in this ecosystem, we explored the potential of unoccupied aerial vehicles (UAV)-borne red, green, and blue wavelengths (RGB) and hyperspectral imagery for páramo species classification by collecting both types of images in a 10-ha area, and ground vegetation cover data from 10 plots within this area. Five plots were used for calibration and the other five for validation. With the hyperspectral data, we tested our capacity to detect five representative páramo species with different growth forms using support vector machine (SVM) and random forest (RF) classifiers in combination with three feature selection methods and two class groups. Using RGB images, we could classify 21 species with an accuracy greater than 97%. From hyperspectral imaging, the highest accuracy (89%) was found using models built with RF or SVM classifiers combined with a binary grouping method and the sequential floating forward selection feature. Our results demonstrate that páramo species can be accurately mapped using both RGB and hyperspectral imagery.
",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Mapping Wetland Plant Communities Using Unmanned Aerial Vehicle Hyperspectral Imagery by Comparing Object/Pixel-Based Classifications Combining Multiple Machine-Learning Algorithms,"Baojia Du, Dehua Mao, Zongming Wang, Zhiqiang Qiu, Hengqi Yan, Kaidong Feng, Zhongbin Zhang","Understanding the spatial patterns of plant communities is important for sustainable wetland ecosystem management and biodiversity conservation. With the rapid development of unmanned aerial vehicle (UAV) technology, UAV-borne hyperspectral data with high spatial resolution have become ideal for accurate classification of wetland plant communities. In this article, four dominant plant communities (Phragmites australis, Typha orientalis, Suaeda glauca, and Scirpus triqueter) and two unvegetated cover types (water and bare land) in the Momoge Ramsar wetland site were classified. This was achieved using UAV hyperspectral images and three object- and pixel-based machine-learning classification algorithms [random forest (RF), convolutional neural network (CNN), and support vector machine (SVM)]. First, spectral derivative analysis, logarithmic analysis, and continuum removal analysis identified the wavelength at which the greatest difference in reflectance occurs. Second, dimensionality reduction of hyperspectral images was conducted using principal component analysis. Subsequently, an optimal feature combination for community mapping was formed based on data transformation (spectral features, vegetation indices, and principal components). Image objects were obtained by segmenting the optimum object feature subsets. Finally, distribution maps of communities were produced by using three machine-learning classification algorithms. Our results reveal that object-based image analysis outperforms pixel-based methods, with overall accuracies (OAs) of 80.29-87.75%; RF has the highest OA of 87.75% (Kappa = 0.864), followed consecutively by CNN (OA = 83.31%, Kappa = 0.829) and SVM (OA = 80.29%, Kappa = 0.813). Phragmites australis dominates the plant community (55.9%) at the study area, followed by Typha orientalis (16.2%), Suaeda glauca (16.2%), and Scirpus triqueter (4.6%). The results highlight the importance of spectral transformation features in red-edge regions. The mapping...
",AVIRIS and HYDICE datasets,58,0,,,
Image classification,"Above-ground biomass estimation models of mangrove forests based on remote sensing and field-surveyed data: Implications for C-PFES implementation in Quang Ninh Province, Vietnam","Hai-Hoa Nguyen, Thi Thu Hien Nguyen","The free charge and open-source of remote sensing imagery, including Landsat-8 and Sentinel-2, offer new opportunities for forest-based AGB mapping and monitoring, especially mangrove forests in the tropics. Modelling relationships between mangrove AGB estimation-based survey and remote sensing data (spectral bands and vegetation indices) have not been evaluated in Quang Ninh Province, Vietnam. In this study, we evaluated the capability of Landsat-8 and Sentinel-2 data for the retrieval and predictive mapping of mangrove AGB in Mong Cai Coast, Quang Ninh Province as a case study. We used 2019 Landsat-8 and Sentinel-2 to develop AGB estimation models through stepwise linear regression approaches in R statistics. We developed models each from spectral bands and vegetation indices derived from Landsat-8 and Sentinel-2 imagery. The results showed that the models based on spectral bands and vegetation indices derived from Sentinel-2 were more accurate in predicting the overall AGB of mangrove forests than those of Landsat-8 data. High correlation values between AGB and Sentinel-2-derived vegetation indices (Model 6.6, r; Model 6.7, r; Model 6.8, r.988) and Landsat-8-derived vegetation indices (Model 6.1, r; Model 6.2, r; Model 6.3, r.939); and between AGB and Sentinel-2-derived spectral bands (Model 5.5, r; Model 5.4, r.975); between AGB and Landsat-8 derived spectral bands (Model 5.2, r; Model 5.3, r; Model 5.1, r.855) were obtained. The developed AGB estimation models have high prediction accuracy, agreements of observed and predicted AGB values of 89.88% for Landsat-8 derived vegetation index (Model 6.2), 96.51% for Sentinel-2 derived vegetation index (Model 6.6). Overall, both Landsat-8 and Sentinel-2 provide satisfactory results in the retrieval and predictive mapping of mangrove AGB. Our study suggests that mangrove conservation under C-PFES schemes should be applied over Quang Ninh Coast based on AGB estimation models developed.",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Assessing the effectiveness of ground truth data to capture landscape variability from an agricultural region using Gaussian simulation and geostatistical techniques hyperspectral datasets,"Eric Ariel L.Salas, Sakthi Kumaran Subburayalu, Brian Slater, Rucha Dave, Parshva Parekh, Kaiguang Zhao, Bimal Bhattacharya","Predictive modeling with remotely sensed data requires an accurate representation of spatial variability by ground truth data. In this study, we assessed the reliability of the size and location of ground truth data in capturing the landscape spatial variability embedded in the Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) hyperspectral image in an agricultural region in Anand, India. We derived simulated spectral vegetation and soil indices using Gaussian simulation from AVIRIS-NG image for two point-location datasets, (1) ground truth points from adaptive sampling and (2) points from conditional Latin Hypercube Sampling (cLHS). We compared values of the simulated image indices against the actual image indices (measured) through the analysis of mean absolute errors. Modeling the variogram of the measured indices with the hyperspectral image in high spatial resolution (4m), is an effective way to characterize the spatial heterogeneity at the landscape level. We used geostatistical techniques to analyze the shapes of experimental variograms in order to assess whether or not the ground truth points, when compared against the cLHS-derived points, captured the spatial structures and variability of the studied agricultural area using measured indices. In addition, we explored the capability of the variogram by running tests in different point sample sizes. The ground truth and cLHS datasets were able to derive equivalent values for field spatial variability from image indices, according to our findings. Furthermore, this research presents a methodology for selecting spectral indices and determining the best sample size for efficiently replicating spatial patterns in hyperspectral images.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Correction: Modified shape index for objectbased random forest image classification of agricultural systems using airborne,"Eric Ariel L. Salas, Sakthi Kumaran Subburayalu","This paper highlights the importance of optimized shape index for agricultural management system analysis that utilizes the contiguous bands of hyperspectral data to define the gradient of the spectral curve and improve image classification accuracy. Currently, a number of machine learning methods would resort to using averaged spectral information over wide bandwidths resulting in loss of crucial information available in those contiguous bands. The loss of information could mean a drop in the discriminative power when it comes to land cover classes with comparable spectral responses, as in the case of cultivated fields versus fallow lands. In this study, we proposed and tested three new optimized novel algorithms based on Moment Distance Index (MDI) that characterizes the whole shape of the spectral curve. The image classification tests conducted on two publicly available hyperspectral data sets (AVIRIS 1992 Indian Pine and HYDICE Washington DC Mall images) showed the robustness of the optimized algorithms in terms of classification accuracy. We achieved an overall accuracy of 98% and 99% for AVIRIS and HYDICE, respectively. The optimized indices were also time efficient as it avoided the process of band dimension reduction, such as those implemented by several well-known classifiers. Our results showed the potential of optimized shape indices, specifically the Moment Distance Ratio Right/Left (MDRRL), to discriminate between types of tillage (corn-min and corn-notill) and between grass/pasture and grass/trees, tree and grass under object-based random forest approach.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Potential of mapping dissolved oxygen in the Little Miami River using Sentinel-2 images and machine learning algorithms,"Eric Ariel L. Salas, Sakthi Subburayalu Kumaran, Eric B. Partee, Leeoria P. Willis, Kayla Mitchell","Dissolved oxygen (DO) is one of the most significant indicators of water quality in the inland water systems. Maintaining DO monitoring systems is time and money consuming since in situ DO data with high spatial and temporal resolution is needed to design appropriate coastal management plans. In this study, we mapped the spatiotemporal changes of DO in the Little Miami River (LMR) using 10-m Sentinel-2 images. We trained two machine learning algorithms – Random Forest (RF) and Support Vector Machine (SVM) – to predict DO concentrations using spectral predictors derived from the satellite images. Moreover, we calculated several metrics, which include Root Mean Squared Error (RMSE), Amount of Variance Explained (AVE), Coefficient of Efficiency (COE), and Normalized Mean Bias (NMB) to assess the performance of the models and accuracy of the DO maps. Our results showed a good agreement between modeled and measured DO concentrations with minimal residual errors ranging between 0.201 mg/L and 0.241 mg/L. Furthermore, RF and SVM revealed to be the reliable and effective algorithms to estimate DO concentrations with bands 5 (B5, vegetation red edge) and 8 (B8, NIR) as important predictors. We found the DO concentrations in the LMR to be in good to excellent range (7.3 mg/L to 14.7 mg/L for August and 10.8 mg/L to 14.5 mg/L for October), based on the minimum criterion for warm water habitat in the Ohio Water Quality Standards (WQS). Our findings have provided spatial insight to the current status of DO and the success of the management steps taken to manage and prevent eutrophic problems in the LMR.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,Remote Sensing Satellite Image Classification Using Neural Network,"G.Karthik, M.Sangeetha, B.Karthik","Continuous monitoring of the geographical area of the Earth has become a necessity. Either to develop the infrastructure or to protect the resources of a region the right analysis of the geographical distribution has to be detected. Hence processing of satellite image of the region is the best solution. This paper proposes an approach for refining the detection and classification of various geographical regions like land, water, forest etc., in an area. Here the Convolutional Neural Network (CNN) classifier of deep learning technique is applied for processing the satellite image of the region under study using Matlab platform.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,"Remote Sensing Technology—A New Dimension in Detection, Quantification and Tracking of Abiotic and Biotic Stresses","Papan Chowhan, Arka Pratim Chakraborty","Plant stress results due to the lack of suitable and optimal conditions for ideal plant growth and development. Stress may be of any kind like abiotic or biotic that can cause many harmful effects to the plant. Remote sensing is a dynamic technique that records changes in electromagnetic radiation and assists in the quantification of different stresses. This technique has also been applied to monitor different abiotic and biotic stresses like nutrient, drought, salinity, pests and pathogen attack, etc. Numerous physiological, biochemical and structural crop characteristics can be measured through remote-sensing-based techniques and it is fast, cost-effective. Under both biotic and abiotic stresses, there are notable changes in photosynthetic ability and physical structure of the host plant at both tissue and canopy levels. Due to this, changes in light absorption pattern by the plant have been observed that in turn alter the reflectance spectrum. The study of the vegetative spectral reflectance helps us better understand the different physiological and chemical processes in plants due to the attack of pests and pathogens. Interestingly, remote sensing technology can be utilized to track the effect of various pathogens in different crops and for their better management. The present chapter aims to discuss the various applications of remote sensing in modern farming and their applications in the management of different abiotic and biotic stresses in crop plants.

",AVIRIS and HYDICE datasets,58,0,,,
Image classification,A citrus fruits and leaves dataset for detection and classification of citrus diseases through machine learning,"Hafiz Tayyab Rauf, Basharat Ali Saleem, M. Ikram Ullah Lali, Muhammad Attique Khan, Muhammad Sharif, Syed Ahmad Chan Bukhari",,Citrus Plant Dataset,59,1,Citrus Plant Dataset,Hafiz Tayyab Rauf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ,"(1) In agriculture, plant diseases are primarily responsible for the reduction in production which causes economic losses. In plants, citrus is used as a major source of nutrients like vitamin C throughout the world. However, ‘Citrus’ diseases badly effect the production and quality of citrus fruits.
(2) The computer vision and image processing techniques have been widely used for detection and classification of diseases in plants.
(3) The dataset contains an image gallery of healthy and unhealthy citrus fruits and leaves that could be usable for the researchers to prevent plants from diseases using advanced computer vision techniques. The disease targeted in the data sets are the Blackspot, Canker, Scab, Greening, and Melanose.
(4) The dataset contains 759 images of healthy and unhealthy images for both Citrus fruits and leaves collectively. Each image contains 256 * 25 dimensions with 72 dpi resolution.
(5) All images were acquired from the Sargodha region, a tropical area of Pakistan under the supervision of Dr. Basharat ALi Saleem, Endeavour Executive Fellow Curtin University · Horticulture Research Laboratory Postharvest Australia · Bentley
(6) All images were annotated manually by the domain expert Dr. Basharat ALi Saleem to represent their every class such as : For Citrus fruits (Black Spot, Canker, Greening, Scab, and healthy with total number of 150 images ), For Citrus Leaves (Black Spot, Canker, Greening, Melanose, and healthy with total number of 609 image)
(6) Further details can be found in the associated publications with the dataset.                                                                                                                                                                                                                                                                                                                                                                                        Source:https://data.mendeley.com/datasets/3f83gxmv57/2"""
Image classification,Performance of deep learning vs machine learning in plant leaf disease detection,"R.Sujatha, Jyotir Moy Chatterjee, NZ Jhanjhi, Sarfraz Nawaz Brohi","Plants are as vulnerable by diseases as animals. Citrus is a major plant grown mainly in the tropical areas of the world due to its richness in vitamin C and other important nutrients. The production of the citrus fruit has been widely affected by citrus diseases which ultimately degrades the fruit quality and causes financial loss to the growers. During the past decade, image processing and computer vision methods have been broadly adopted for the detection and classification of plant diseases. Early detection of diseases in citrus plants helps in preventing them to spread in the orchards which minimize the financial loss to the farmers. In this article, an image dataset citrus fruits, leaves, and stem is presented. The dataset holds citrus fruits and leaves images of healthy and infected plants with diseases such as Black spot, Canker, Scab, Greening, and Melanose. Most of the images were captured in December from the Orchards in Sargodha region of Pakistan when the fruit was about to ripen and maximum diseases were found on citrus plants. The dataset is hosted by the Department of Computer Science, University of Gujrat and acquired under the mutual cooperation of the University of Gujrat and the Citrus Research Center, Government of Punjab, Pakistan. The dataset would potentially be helpful to researchers who use machine learning and computer vision algorithms to develop computer applications to help farmers in early detection of plant diseases. The dataset is freely available at https://data.mendeley.com/datasets/3f83gxmv57/2.

",Citrus Plant Dataset,59,0,,,
Image classification,A Sustainable Deep Learning Framework for Object Recognition Using Multi-Layers Deep Features Fusion and Selection,"Muhammad Rashid, Muhammad Attique Khan, Majed Alhaisoni, Shui-Hua Wang, Syed Rameez Naqvi, Amjad Rehman, Tanzila Saba","With an overwhelming increase in the demand of autonomous systems, especially in the applications related to intelligent robotics and visual surveillance, come stringent accuracy requirements for complex object recognition. A system that maintains its performance against a change in the object’s nature is said to be sustainable and it has become a major area of research for the computer vision research community in the past few years. In this work, we present a sustainable deep learning architecture, which utilizes multi-layer deep features fusion and selection, for accurate object classification. The proposed approach comprises three steps: (1) By utilizing two deep learning architectures, Very Deep Convolutional Networks for Large-Scale Image Recognition and Inception V3, it extracts features based on transfer learning, (2) Fusion of all the extracted feature vectors is performed by means of a parallel maximum covariance approach, and (3) The best features are selected using Multi Logistic Regression controlled Entropy-Variances method. For verification of the robust selected features, the Ensemble Learning method named Subspace Discriminant Analysis is utilized as a fitness function. The experimental process is conducted using four publicly available datasets, including Caltech-101, Birds database, Butterflies database and CIFAR-100, and a ten-fold validation process which yields the best accuracies of 95.5%, 100%, 98%, and 68.80% for the datasets respectively. Based on the detailed statistical analysis and comparison with the existing methods, the proposed selection method gives significantly more accuracy. Moreover, the computational time of the proposed selection method is better for real-time implementation.
",Citrus Plant Dataset,59,0,,,
Image classification,Review of the State of the Art of Deep Learning for Plant Diseases: A Broad Analysis and Discussion,"Reem Ibrahim Hasan, Suhaila Mohd Yusuf, Laith Alzubaidi","Deep learning (DL) represents the golden era in the machine learning (ML) domain, and it has gradually become the leading approach in many fields. It is currently playing a vital role in the early detection and classification of plant diseases. The use of ML techniques in this field is viewed as having brought considerable improvement in cultivation productivity sectors, particularly with the recent emergence of DL, which seems to have increased accuracy levels. Recently, many DL architectures have been implemented accompanying visualisation techniques that are essential for determining symptoms and classifying plant diseases. This review investigates and analyses the most recent methods, developed over three years leading up to 2020, for training, augmentation, feature fusion and extraction, recognising and counting crops, and detecting plant diseases, including how these methods can be harnessed to feed deep classifiers and their effects on classifier accuracy.
",Citrus Plant Dataset,59,0,,,
Image classification,A Deep Neural Network based disease detection scheme for Citrus fruits,"Vinay Kukreja, Poonam Dhiman","One of the most significant factors is the quality evaluation of agricultural products in supporting their marketability and controlling waste management. To classify the fruits into healthy and defected class, deep learning algorithms have been implemented to perform citrus disease detection. This study aims to use the dense CNN algorithm to detect and provide an effective method for detecting the apparent defects of citrus fruit. Citrus fruit images are collected and put in two classes of good and damaged ones, to recognize and categorize the image dataset. Firstly, a dense CNN model was used without doing preprocessing and data augmentation on 150 images and achieved an accuracy of 67 percent but the proposed model has used data augmentation and pre-processing to enhance the CNN performance and have used 1200 images. Further, the proposed model is compared with the dense model where data augmentation and pre-processing techniques have not been used. The overall accuracy of the proposed model is 89.1%. The results show that techniques of data augmentation and preprocessing have delivered promising insights to estimate citrus fruit's damages.
",Citrus Plant Dataset,59,0,,,
Image classification,Recognizing apple leaf diseases using a novel parallel real-time processing framework based on MASK RCNN and transfer learning: An application for smart agriculture,"Zia ur Rehman, Muhammad Attique Khan, Fawad Ahmed, Robertas Damaševičius,Syed Rameez Naqvi, Wasif Nisar, Kashif Javed","Effective recognition of fruit leaf diseases has a substantial impact on agro-based economies. Several fruit diseases exist that badly impact the yield and quality of fruits. A naked-eye inspection of an infected region is a difficult and tedious process; therefore, it is required to have an automated system for accurate recognition of the disease. It is widely understood that low contrast images affect identification and classification accuracy. Here a parallel framework for real-time apple leaf disease identification and classification is proposed. Initially, a hybrid contrast stretching method to increase the visual impact of an image is proposed and then the MASK RCNN is configured to detect the infected regions. In parallel, the enhanced images are utilized for training a pre-trained CNN model for features extraction. The Kapur's entropy along MSVM (EaMSVM) approach-based selection method is developed to select strong features for the final classification. The Plant Village dataset is employed for the experimental process and achieve the best accuracy of 96.6% on the ensemble subspace discriminant analysis (ESDA) classifier. A comparison with the previous techniques illustrates the superiority of the proposed framework.

",Citrus Plant Dataset,59,0,,,
Image classification,Adversarial Attack and Defence through Adversarial Training and Feature Fusion for Diabetic Retinopathy Recognition,"Sheeba Lal, Saeed Ur Rehman, Jamal Hussain Shah, Talha Meraj, Hafiz Tayyab Rauf, Robertas Damaševičius, Mazin Abed Mohammed, Karrar Hameed Abdulkareem","Due to the rapid growth in artificial intelligence (AI) and deep learning (DL) approaches, the security and robustness of the deployed algorithms need to be guaranteed. The security susceptibility of the DL algorithms to adversarial examples has been widely acknowledged. The artificially created examples will lead to different instances negatively identified by the DL models that are humanly considered benign. Practical application in actual physical scenarios with adversarial threats shows their features. Thus, adversarial attacks and defense, including machine learning and its reliability, have drawn growing interest and, in recent years, has been a hot topic of research. We introduce a framework that provides a defensive model against the adversarial speckle-noise attack, the adversarial training, and a feature fusion strategy, which preserves the classification with correct labelling. We evaluate and analyze the adversarial attacks and defenses on the retinal fundus images for the Diabetic Retinopathy recognition problem, which is considered a state-of-the-art endeavor. Results obtained on the retinal fundus images, which are prone to adversarial attacks, are 99% accurate and prove that the proposed defensive model is robust.
",Citrus Plant Dataset,59,0,,,
Image classification,AI-driven deep CNN approach for multi-label pathology classification using chest X-Rays,"Saleh Albahli, Hafiz Tayyab Rauf​, Abdulelah Algosaibi, Valentina Emilia Balas​","Artificial intelligence (AI) has played a significant role in image analysis and feature extraction, applied to detect and diagnose a wide range of chest-related diseases. Although several researchers have used current state-of-the-art approaches and have produced impressive chest-related clinical outcomes, specific techniques may not contribute many advantages if one type of disease is detected without the rest being identified. Those who tried to identify multiple chest-related diseases were ineffective due to insufficient data and the available data not being balanced. This research provides a significant contribution to the healthcare industry and the research community by proposing a synthetic data augmentation in three deep Convolutional Neural Networks (CNNs) architectures for the detection of 14 chest-related diseases. The employed models are DenseNet121, InceptionResNetV2, and ResNet152V2; after training and validation, an average ROC-AUC score of 0.80 was obtained competitive as compared to the previous models that were trained for multi-class classification to detect anomalies in x-ray images. This research illustrates how the proposed model practices state-of-the-art deep neural networks to classify 14 chest-related diseases with better accuracy.

",Citrus Plant Dataset,59,0,,,
Image classification,Automatic identification of diseases in grains crops through computational approaches: A review,R.Manavalan,"Agricultural productivity significantly contributes to every country's economy. The grain plants such as wheat, rice, corn (maize), barley, oats, rye, millet, and sorghum are commonly grown across the world. The pest and various diseases in the grain plants severely affect major production and cause heavy losses in the global economy. Monitoring of health and early diagnosing of diseases in grains plants is a critical task for sustainable agriculture. The information on early diagnosis of several diseases can facilitate the control of diseases through proper selection of pest control techniques to improve the grains productivity. The manual identification of the disorders in grains plants can lead to inaccurate measurements of pesticides. While several papers on grain diseases identification through intelligent techniques have been published in recent years, there has been no clear attempt to study these papers systematically to describe various phases of diagnosis system such as image preprocessing, segmentation, feature extraction, features selection, and classification methods. In this context, a total of 109 peer-reviewed articles reporting to identify the diseases at the early stage to increase the production of the five most-produced grains in the world such as maize, rice, wheat, soybean, and barley are reviewed, ranging in publication date from 2001 to 2020. The article also presents a detailed taxonomy of grain plant leaf diseases. The study found that there are still many issues that’s need to be addressed in each phase of the automated disease detection system. The pros and cons of reviewed computational method is explored and future directions are highlighted. The survey outcomes reveal that the existing automated detection and classification methods for grain plants diseases is still infancy. Hence novel fully automated tools are necessary for the process of detection and classification diseases in grain plants.

",Citrus Plant Dataset,59,0,,,
Image classification,Deep Metric Learning Based Citrus Disease Classification With Sparse Data,"Sivasubramaniam Janarthan, Selvarajah Thuseethan, Sutharshan Rajasegarar, Qiang Lyu, Yongqiang Zheng, John Yearwood","Early recognition of citrus diseases is important for preventing crop losses and employing timely disease control measures in farms. Employing machine learning-based approaches, such as deep learning for accurate detection of multiple citrus diseases is challenging due to the limited availability of labeled diseased samples. Further, a lightweight architecture with low computational complexity is required to perform citrus disease classification on resource-constrained devices, such as mobile phones. This enables practical utility of the architecture to perform effective monitoring of diseases by farmers using their own mobile devices in the farms. Hence, we propose a lightweight, fast, and accurate deep metric learning-based architecture for citrus disease detection from sparse data. In particular, we propose a patch-based classification network that comprises an embedding module, a cluster prototype module, and a simple neural network classifier, to detect the citrus diseases accurately. Evaluation of our proposed approach using publicly available citrus fruits and leaves dataset reveals its efficiency in accurately detecting the various diseases from leaf images. Further, the generalization capability of our approach is demonstrated using another dataset, namely the tea leaves dataset. Comparison analysis of our approach with existing state-of-the-art algorithms demonstrate its superiority in terms of detection accuracy (95.04%), the number of parameters required for tuning (less than 2.3 M), and the time efficiency in detecting the citrus diseases (less than 10 ms) using the trained model. Moreover, the ability to learn with fewer resources and without compromising accuracy empowers the practical utility of the proposed scheme on resource-constrained devices, such as mobile phones.",Citrus Plant Dataset,59,0,,,
Image classification,A joint framework of feature reduction and robust feature selection for cucumber leaf diseases recognition,"Jaweria Kianat, Muhammad Attique Khan, Muhammad Sharifa,Tallha Akram, Amjad Rehman, Tanzila Saba","In machine learning (ML) domain, extracted features play a primary role in both segmentation and classification of salient/infected regions. Plants' diseases and pests are the main sources of colossal damage worldwide as they affect both the quality and quantity of crops. In China, cucumber is one of the main crops which is widely cultivated and has high economic benefits. Diseases like angular leaf spot, downy mildew, powdery mildew, etc., affect the cucumber crop and the only method being followed for its prevention is manual inspection. In this work, a hybrid framework based on feature fusion and selection techniques is proposed which classifies the cucumber diseases by using three core steps. Initialized with data augmentation, the contrast of image samples is enhanced in the first step, followed by feature extraction, fusion and selection in the second step. Finally, most discriminant features are classified using a set of classifiers. The extracted features in this work are initially reduced using the proposed probability distribution-based Entropy (PDbE) approach. After the serial-based fusion step, strong features are selected using the proposed Manhattan distance controlled entropy (MDcE) technique which has a capacity to select the features greater than a threshold. From the achieved accuracy (93.50 %) on the selected dataset having more than 900 image samples and six classes, it is very much evident that the presented method is comparable to several other existing techniques.

",Citrus Plant Dataset,59,0,,,
Image classification,RSI-CB: A Large Scale Remote Sensing Image Classification Benchmark via Crowdsource Data,"Haifeng Li, Xin Dou, Chao Tao, Zhixiang Hou, Jie Chen, Jian Peng, Min Deng, Ling Zhao","Image classification is a fundamental task in remote sensing image processing. In recent years, deep convolutional neural networks (DCNNs) have experienced significant breakthroughs in natural image recognition. The remote sensing field, however, is still lacking a large-scale benchmark similar to ImageNet. In this paper, we propose a remote sensing image classification benchmark (RSI-CB) based on massive, scalable, and diverse crowdsourced data. Using crowdsourced data, such as Open Street Map (OSM) data, ground objects in remote sensing images can be annotated effectively using points of interest, vector data from OSM, or other crowdsourced data. These annotated images can, then, be used in remote sensing image classification tasks. Based on this method, we construct a worldwide large-scale benchmark for remote sensing image classification. This benchmark has large-scale geographical distribution and large total image number. It contains six categories with 35 sub-classes of more than 24,000 images of size pixels. This classification system of ground objects is defined according to the national standard of land-use classification in China and is inspired by the hierarchy mechanism of ImageNet. Finally, we conduct numerous experiments to compare RSI-CB with the SAT-4, SAT-6, and UC-Merced data sets. The experiments show that RSI-CB is more suitable as a benchmark for remote sensing image classification tasks than other benchmarks in the big data era and has many potential applications.
","Satellite image Classification Dataset-RSI-CB256
",60,1,Satellite image Classification Dataset-RSI-CB256, Sohel Rana                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ,"The past years have witnessed great progress on remote sensing (RS) image interpretation and its wide applications. With RS images becoming more accessible than ever before, there is an increasing demand for the automatic interpretation of these images. In this context, the benchmark datasets serve as essential prerequisites for developing and testing intelligent interpretation algorithms. After reviewing existing benchmark datasets in the research community of RS image interpretation, this article discusses the problem of how to efficiently prepare a suitable benchmark dataset for RS image interpretation. Specifically, we first analyze the current challenges of developing intelligent algorithms for RS image interpretation with bibliometric investigations. We then present the general guidances on creating benchmark datasets in efficient manners. Following the presented guidances, we also provide an example on building RS image dataset, i.e., Million-AID, a new large-scale benchmark dataset containing a million instances for RS image scene classification. Several challenges and perspectives in RS image annotation are finally discussed to facilitate the research in benchmark dataset construction. We do hope this paper will provide the RS community an overall perspective on constructing large-scale and practical image datasets for further research, especially data-driven ones.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Source:https://www.kaggle.com/sohelranaccselab/satellite-data"
Image classification,"Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities","Gong Cheng, Xingxing Xie, Junwei Han, Lei Guo, Gui-Song Xia","Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this paper provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey (1) Autoencoder-based remote sensing image scene classification methods, (2) Convolutional Neural Network-based remote sensing image scene classification methods, and (3) Generative Adversarial Network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly-used benchmark data sets. Finally, we discuss the promising opportunities for further research.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Bigearthnet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding,"Gencer Sumbul, Marcela Charfuelan, Begüm Demir, Volker Markl","This paper presents the BigEarthNet that is a new large-scale multi-label Sentinel-2 benchmark archive. The BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. Unlike most of the existing archives, each image patch is annotated by multiple land-cover classes (i.e., multi-labels) that are provided from the CORINE Land Cover database of the year 2018 (CLC 2018). The BigEarthNet is significantly larger than the existing archives in remote sensing (RS) and thus is much more convenient to be used as a training source in the context of deep learning. This paper first addresses the limitations of the existing archives and then describes the properties of the BigEarthNet. Experimental results obtained in the framework of RS image scene classification problems show that a shallow Convolutional Neural Network (CNN) architecture trained on the BigEarthNet provides much higher accuracy compared to a state-of-the-art CNN model pre-trained on the ImageNet (which is a very popular large-scale benchmark archive in computer vision). The BigEarthNet opens up promising directions to advance operational RS applications and research in massive Sentinel-2 image archives.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,"A Survey on Deep Learning-Driven Remote Sensing Image Scene Understanding: Scene Classification, Scene Retrieval and Scene-Guided Object Detection","Yating Gu, Yantian Wang, Yansheng Li","As a fundamental and important task in remote sensing, remote sensing image scene understanding (RSISU) has attracted tremendous research interest in recent years. RSISU includes the following sub-tasks: remote sensing image scene classification, remote sensing image scene retrieval, and scene-driven remote sensing image object detection. Although these sub-tasks have different goals, they share some communal hints. Hence, this paper tries to discuss them as a whole. Similar to other domains (e.g., speech recognition and natural image recognition), deep learning has also become the state-of-the-art technique in RSISU. To facilitate the sustainable progress of RSISU, this paper presents a comprehensive review of deep-learning-based RSISU methods, and points out some future research directions and potential applications of RSISU.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Deep Learning-Based Classification Methods for Remote Sensing Images in Urban Built-Up Areas,"Wenmei Li, Haiyan Liu, Yu Wang, Zhuangzhuang Li, Yan Jia, Guan Gui","Urban areas have been focused recently on the remote sensing applications since their function closely relates to the distribution of built-up areas, where reflectivity or scattering characteristics are the same or similar. Traditional pixel-based methods cannot discriminate the types of urban built-up areas very well. This paper investigates a deep learning-based classification method for remote sensing images, particularly for high spatial resolution remote sensing (HSRRS) images with various changes and multi-scene classes. Specifically, to help develop the corresponding classification methods in urban built-up areas, we consider four deep neural networks (DNNs): 1) convolutional neural network (CNN); 2) capsule networks (CapsNet); 3) same model with a different training rounding based on CNN (SMDTR-CNN); and 4) same model with different training rounding based on CapsNet (SMDTR-CapsNet). The performances of the proposed methods are evaluated in terms of overall accuracy, kappa coefficient, precision, and confusion matrix. The results revealed that SMDTR-CNN obtained the best overall accuracy (95.0%) and kappa coefficient (0.944) while also improving the precision of parking lot and resident samples by 1% and 4%, respectively.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Error-Tolerant Deep Learning for Remote Sensing Image Scene Classification,"Yansheng Li, Yongjun Zhang, Zhihui Zhu","Due to its various application potentials, the remote sensing image scene classification (RSSC) has attracted a broad range of interests. While the deep convolutional neural network (CNN) has recently achieved tremendous success in RSSC, its superior performances highly depend on a large number of accurately labeled samples which require lots of time and manpower to generate for a large-scale remote sensing image scene dataset. In contrast, it is not only relatively easy to collect coarse and noisy labels but also inevitable to introduce label noise when collecting large-scale annotated data in the remote sensing scenario. Therefore, it is of great practical importance to robustly learn a superior CNN-based classification model from the remote sensing image scene dataset containing non-negligible or even significant error labels. To this end, this article proposes a new RSSC-oriented error-tolerant deep learning (RSSC-ETDL) approach to mitigate the adverse effect of incorrect labels of the remote sensing image scene dataset. In our proposed RSSC-ETDL method, learning multiview CNNs and correcting error labels are alternatively conducted in an iterative manner. It is noted that to make the alternative scheme work effectively, we propose a novel adaptive multifeature collaborative representation classifier (AMF-CRC) that benefits from adaptively combining multiple features of CNNs to correct the labels of uncertain samples. To quantitatively evaluate the performance of error-tolerant methods in the remote sensing domain, we construct remote sensing image scene datasets with: 1) simulated noisy labels by corrupting the open datasets with varying error rates and 2) real noisy labels by deploying the greedy annotation strategies that are practically used to accelerate the process of annotating remote sensing image scene datasets. Extensive experiments on these datasets demonstrate that our proposed RSSC-ETDL approach outperforms the state-of-the-art approaches.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Enhanced Fusion of Deep Neural Networks for Classification of Benchmark High-Resolution Image Data Sets,"Grant J. Scott, Kyle C. Hagan, Richard A. Marcum, James Alex Hurt, Derek T. Anderson, Curt H. Davis","Accurate land cover classification and detection of objects in high-resolution electro-optical remote sensing imagery (RSI) have long been a challenging task. Recently, important new benchmark data sets have been released which are suitable for land cover classification and object detection research. Here, we present state-of-the-art results for four benchmark data sets using a variety of deep convolutional neural networks (DCNN) and multiple network fusion techniques. We achieve 99.70%, 99.66%, 97.74%, and 97.30% classification accuracies on the PatternNet, RSI-CB256, aerial image, and RESISC-45 data sets, respectively, using the Choquet integral with a novel data-driven optimization method presented in this letter. The relative reduction in classification errors achieved by this data driven optimization is 25%-45% compared with the single best DCNN results.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Image retrieval from remote sensing big data: A survey,"Yansheng Li, Jiayi Ma, Yongjun Zhang","The blooming proliferation of aeronautics and astronautics platforms, together with the ever-increasing remote sensing imaging sensors on these platforms, has led to the formation of rapidly-growing earth observation data with the characteristics of large volume, large variety, large velocity, large veracity and large value, which raises awareness about the importance of large-scale image processing, fusion and mining. Unconsciously, we have entered an era of big earth data, also called remote sensing (RS) big data. Although RS big data provides great opportunities for a broad range of applications such as disaster rescue, global security, and so forth, it inevitably poses many additional processing challenges. As one of the most fundamental and important tasks in RS big data mining, image retrieval (i.e., image information mining) from RS big data has attracted continuous research interests in the last several decades. This paper mainly works for systematically reviewing the emerging achievements for image retrieval from RS big data. And then this paper further discusses the RS image retrieval based applications including fusion-oriented RS image processing, geo-localization and disaster rescue. To facilitate the quantitative evaluation of the RS image retrieval technique, this paper gives a list of publicly open datasets and evaluation metrics, and briefly recalls the mainstream methods on two representative benchmarks of RS image retrieval. Considering the latest advances from multiple domains including computer vision, machine learning and knowledge engineering, this paper points out some promising research directions towards RS big data mining. From this survey, engineers from industry may find skills to improve their RS image retrieval systems and researchers from academia may find ideas to conduct some innovative work.","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Multi-Spectral RGB-NIR Image Classification Using Double-Channel CNN,"Jionghui Jiang, Xi’an Feng, Fen Liu, Yingying Xu, Hui Huang","As 4-sensor line scan camera technology has matured, red (R), green (G), blue (B), and near-infrared (RGB-NIR) datasets have begun to appear in large numbers. The RGB-NIR data contain the rich color features of the RGB image and the sharp edge features of the NIR image. At present, in many studies, the RGB-NIR data are input directly into the processing algorithms for calculation of the 4D data; in these cases, redundant information is included, and the high correlation between the bands results in an inability to fully exploit the characteristics of the RGB-NIR data. In this paper, we propose a double-channel convolutional neural network (CNN) algorithm that takes into account the strong correlation between the R, G, and B bands in aerial images and the weaker correlation between the NIR band and the R, G, and B bands. First, the features of the RGB and NIR bands are calculated in two different CNN networks, and subsequently, feature fusion is performed in the fully connected layer. This is followed by the classification. By combining the two neural networks of RGB-CNN and NIR-CNN, the respective characteristics of the RGB-NIR data are fully exploited.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Scene Classification Using Hierarchical Wasserstein CNN,"Yishu Liu, Ching Y. Suen, Yingbin Liu, Liwang Ding","In multiclass classification, convolutional neural network (CNN) is generally coupled with the cross-entropy (CE) loss, which only penalizes the predicted probability corresponding to a ground truth class and ignores the interclass relationship. We argue that CNN can be improved by using a better loss function. On the other hand, the Wasserstein distance (WD) is a well-known metric used to measure the distance between two distributions. Directly solving the WD problem requires a prohibitively large amount of computation time, whereas the cheaper iterative algorithms have a variety of shortcomings such as computational instability and difficulty in selecting parameters. In this paper, we address these issues by giving an analytical solution to the WD problem-for the first time, we find that for two distributions in hierarchically organized data space, WD has a closed-form solution, which we call “hierarchical WD (HWD).” We use this theory to construct novel loss functions that overcome the shortcomings of CE loss. To this end, multi-CNN information fusion that provides the basis for building category hierarchies is carried out first. Then, the semantic relationship among classes is modeled as a binary tree. Then, CNN coupled with an HWD-based loss, i.e., hierarchical Wasserstein CNN (HW-CNN), is trained to learn deep features. In this way, prior knowledge about the interclass relationship is embedded into HW-CNN, and information from several CNNs provides guidance in the process of training individual HW-CNNs. We conducted extensive experiments over two publicly available remote sensing data sets and achieved a state-of-the-art performance in scene classification tasks.
","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Remote sensing image scene classification based on generative adversarial networks,"Suhui Xu, Xiaodong Mu, Dong Chai, Xiongmei Zhang","Scene classification of remote sensing images plays an important role in many remote sensing image applications. Training a good classifier needs a large number of training samples. The labeled samples are often scarce and difficult to obtain, and annotating a large number of samples is time-consuming. We propose a novel remote sensing image scene classification framework based on generative adversarial networks (GAN) in this paper. GAN can improve the generalization ability of machine learning network model. However, generating large-size images, especially high-resolution remote sensing images is difficult. To address this issue, the scaled exponential linear units (SELU) are applied into the GAN to generate high quality remote sensing images. Experiments carried out on two datasets show that our approach can obtain the state-of-the-art results compared with the classification results of the classic deep convolutional neural networks, especially when the number of training samples is small.

","Satellite image Classification Dataset-RSI-CB256
",60,0,,,
Image classification,Crack Detection of Structures using Deep Learning Framework,"Arunish Kumar, Amit Kumar, Avinash Kumar Jha, Ashutosh Trivedi","The transportation system has drawn attention due to the growing threats of cracks to the roads and bridges. To accurately detect cracks was made possible by the introduction of deep learning. In this paper, we propose the modified LeNet 5 model for detecting cracks in roads and bridges. We summarize our work in the following points: - 1. We worked on the three datasets, i.e., Automated Bridge Crack Detection Dataset, Concrete Crack Images for Classification Dataset, and Asphalt Crack Dataset. 2. We applied a modified version of LeNet-5 model on Automated Bridge Crack Detection Dataset, Concrete Crack Images for Classification Dataset, and Asphalt Crack Dataset. 3. Then, we analyzed our results and compared the same with Principal Component Analysis and without using Principal Component Analysis. We are also highlighting the region of crack and non-crack using green and red color, respectively. Our proposed model takes both the factors of time and accuracy into consideration while producing the output.
",Asphalt Crack Dataset,61,1,Asphalt Crack Dataset,"Jayanth Balaji A
","400 images of Asphalt cracks and Non-crack images that are captured using cameras, can be used for training crack detecting models."
Image classification,A Review of AI for Urban Planning: Towards Building Sustainable Smart Cities,"Avinash Kumar Jha, Awishkar Ghimire, Surendrabikram Thapa, Aryan Mani Jha, Ritu Raj","Urban planning, in short, deals with solving the problems of the modern society. The problems are complementary to the growing population in today's society. The problems in society range from mundane tasks like ensuring sanitization in society to more technical tasks like managing infrastructures. The concept of smart cities, lately, has been a topic of great interest for social scientists, engineers, and everyone who wants to integrate technologies into their daily lives. AI and IoT have become an important part of our lives. Data has become ubiquitous with such smart devices that are connected to the internet. Such data can be used to make intelligent systems for smart cities. AI and loT have promising effects on urban life. Artificial Intelligence is often considered to be the fourth industrial revolution because of its unprecedented potential to change everything. As AI progresses to improve day by day, it has blessed humans with everything from smart healthcare to secured smart cities. Everything has been changed by AI and IoT in smart cities. In this paper, the various use cases of AI and IoT for urban planning in order to build smart and sustainable cities are discussed The proposed research work has reviewed various papers that could potentially be used in the development of smart cities.
",Asphalt Crack Dataset,61,0,,,
Image classification,Detection of Cracks in Surfaces and Materials Using Convolutional Neural Network,"R. Venkatesh, K. Vignesh Saravanan, V. R. Aswin, S. Balaji, K. Amudhan, S. Rajakarunakaran","Crack is the separation of an objects or surface in two or more pieces. Identification of these cracks is crucial and plays a vital role in preventing the disaster that will happen due to that crack. The effective identification of these cracks on materials or parts may help the manufacturer to neglect those parts and to find the root cause for the formation of crack. For this, we have developed an image processing model using convolutional neural network. This model will constantly take picture by using the web camera, and these images were processed by the model and that model will predict whether the part or material has a crack on its surface or not. If the crack is detected by the model, those images will be saved, and respective actions will be taken. We have trained our model with 20,000 positive datasets (images with crack) and 20,000 negative datasets (image without any cracks). Then we have tested the model with some random images obtained from the internet. Analysing the results, we found that our model has attained maximum accuracy with an average confidence level of 93–97%. After this stage, we have developed the model to access the device’s webcam to capture live video feed and predict the images for cracks. If a crack is present in the image, the model will store the image.

",Asphalt Crack Dataset,61,0,,,
Image classification,Crack Detection in Civil Structures Using Deep Learning,"Bijimalla Shiva Vamshi Krishna, B.S. Rishiikeshwer, J. Sanjay Raju, N. Bharathi, C. Venkatasubramanian, G.R. Brindha","The safety monitoring process of the structure of any civil engineering work is the most significant task. The continuous monitoring for any abnormal state of the structure is predicted and severe damages can be prevented. It also depends on the other environmental parameters like load, nature of seasonal parameter, and soil type, not only in civil engineering, but also other industries, which makes efficient use of the technology. In mechanical engineering, internal parts have to be monitored and set alarm to give attention to prevent the major damage to the system. Flight internal engines, brake system in cars, etc., are monitored. The manual approach completely relies on the person's knowledge, experience, and skill‐set which obviously differs and always has the possibility of lacking objectivity in terms of quantitative analysis. The manual inspection can be replaced with automatic crack detection using ML and DL with computer vision. Currently, more powerful and fast image detection and recognition technologies are applied. The entire theme is all about providing the overview in brief and envisages the reader to analyze the crack detection using convolutional neural network (CNN) with real‐time data.
",Asphalt Crack Dataset,61,0,,,
Image classification,Machine Learning and IoT based solutions for detection of arrhythmia using ECG signals,"Vinod Kumar, Awishkar Ghimire, Hitesh Kumar Hoon","Cardiac arrhythmia is a life-threatening disease which causes severe health problems in patients. A timely diagnosis of arrhythmia diseases will be useful to save the lives. Internet of Things (IoT) assures to modernize the health-care sector through continuous, remote and noninvasive monitoring of cardiac arrhythmia diseases. An IoT platform for prediction of cardiovascular disease using an IoT-enabled ECG telemetry system acquires the ECG signal, processes the ECG signal and alerts physician for an emergency. It is helpful for the physician to analyze the heart disease as early and accurate. We are developing an IoT-enabled ECG monitoring system to analyze the ECG signal. The statistical features of raw ECG signal are calculated. The ECG signal is analyzed using Pan Tompkins QRS detection algorithm for obtaining the dynamic features of the ECG signal. The system is used to find the RR intervals from ECG signal to capture heart rate variability features. The statistical and dynamic features are then applied to the classification process to classify the cardiac arrhythmia disease. People can check their cardiac condition by the acquisition of ECG signal even in their home. The size of the system is small, and it requires less maintenance and operational cost. It is helpful for the physician to analyze the heart disease as easily and accurately.
",Asphalt Crack Dataset,61,0,,,
Image classification,RCNN-GAN: An Enhanced Deep Learning Approach Towards Detection of Road Cracks,"Shadrack Fred Mahenge, Stephen Wambura, Licheng Jiao","Automatic detection of road cracks is one of the significant aspects of road maintenance systems. However, it involves a lot of complexities to accurately identify the cracks because of various reasons such as heterogeneity of traffic conditions, dynamic environmental conditions and variabilities in multiple parameters. With the advancements in Deep Learning (DL) techniques, this research proposes a DL-based road crack detection model which combines two effective techniques namely RCNN and GAN. A novel RCNN-GAN deep architecture is proposed with reduced layers to improve the detection accuracy. The performance of the proposed approach was verified with respect to different performance metrics such as accuracy, precision, recall, and f1-score. The experimental results of the proposed approach were also compared to various state-of-the art deep learning algorithms. It can be inferred from results that the proposed approach achieves superior performance in terms of detection accuracy and other performance indicators.

",Asphalt Crack Dataset,61,0,,,
Image classification,A Study on Crack Detection in Asphalt Road Pavement Using Small Deep Learning,Bongjun Ji,"Cracks in asphalt pavement occur due to changes in weather or impact from vehicles, and if cracks are left unattended, the life of the pavement may be shortened, and various accidents may occur. Therefore, studies have been conducted to detect cracks through images in order to quickly detect cracks in the asphalt pavement automatically and perform maintenance activity. Recent studies adopt machine-learning models for detecting cracks in asphalt road pavement using a Convolutional Neural Network. However, their practical use is limited because they require high-performance computing power. Therefore, this paper proposes a framework for detecting cracks in asphalt road pavement by applying a small deep learning model applicable to mobile devices. The small deep learning model proposed through the case study was compared with general deep learning models, and although it was a model with relatively few parameters, it showed similar performance to general deep learning models. The developed model is expected to be embedded and used in mobile devices or IoT for crack detection in asphalt pavement.",Asphalt Crack Dataset,61,0,,,
Image classification,"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification","Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni","We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D / 3D neural networks and open-source / commercial AutoML tools. The data and code are publicly available at this https URL.","MedMNIST v2 Dataset
",62,1,"MedMNIST v2 Dataset
","Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni","MedMNIST v2 is a large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with the corresponding classification labels, so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various data scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision and machine learning.

Description and image from: MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification

Each subset keeps the same license as that of the source dataset. Please also cite the corresponding paper of source data if you use any subset of MedMNIST."
Image classification,"3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis","Jianhui Yu, Chaoyi Zhang, Heng Wang, Dingxin Zhang, Yang Song, Tiange Xiang, Dongnan Liu, Weidong Cai","General point clouds have been increasingly investigated for different tasks, and recently Transformer-based networks are proposed for point cloud analysis. However, there are barely related works for medical point clouds, which are important for disease detection and treatment. In this work, we propose an attention-based model specifically for medical point clouds, namely 3D medical point Transformer (3DMedPT), to examine the complex biological structures. By augmenting contextual information and summarizing local responses at query, our attention module can capture both local context and global content feature interactions. However, the insufficient training samples of medical data may lead to poor feature learning, so we apply position embeddings to learn accurate local geometry and Multi-Graph Reasoning (MGR) to examine global knowledge propagation over channel graphs to enrich feature representations. Experiments conducted on IntrA dataset proves the superiority of 3DMedPT, where we achieve the best classification and segmentation results. Furthermore, the promising generalization ability of our method is validated on general 3D point cloud benchmarks: ModelNet40 and ShapeNetPart. Code is released.","MedMNIST v2 Dataset
",62,0,,,
Image classification,"Quantum-classical convolutional neural networks in radiological image classification","Andrea Matic, Maureen Monnet, Jeanette Miriam Lorenz, Balthasar Schachtner, Thomas Messerer","Quantum machine learning is receiving significant attention currently, but its usefulness in comparison to classical machine learning techniques for practical applications remains unclear. However, there are indications that certain quantum machine learning algorithms might result in improved training capabilities with respect to their classical counterparts -- which might be particularly beneficial in situations with little training data available. Such situations naturally arise in medical classification tasks. Within this paper, different hybrid quantum-classical convolutional neural networks (QCCNN) with varying quantum circuit designs and encoding techniques are proposed. They are applied to two- and three-dimensional medical imaging data, e.g. featuring different, potentially malign, lesions in computed tomography scans. The performance of these QCCNNs is already similar to the one of their classical counterparts -- therefore encouraging further studies towards the direction of applying these algorithms within medical imaging tasks.","MedMNIST v2 Dataset
",62,0,,,
Image classification,"Learning Discriminative Representation via Metric Learning for Imbalanced Medical Image Classification
","Chenghua Zeng, Huijuan Lu, Kanghao Chen, Ruixuan Wang, Wei-Shi Zheng","Chenghua Zeng, Huijuan Lu, Kanghao Chen, Ruixuan Wang, Wei-Shi Zheng",MedMNIST v2 Dataset,62,0,,,
Image classification,"MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A
MULTI-VIEW APPROACH","VARUN CHANDOLA,  RANGA RAJU VATSAVAI "," Multispectral remote sensing images have been widely used for automated land use
and land cover classification tasks. Often thematic classification is done using single date image,
however in many instances a single date image is not informative enough to distinguish between
different land cover types. In this paper we show how one can use multiple images, collected at
different times of year (for example, during crop growing season), to learn a better classifier. We
propose two approaches, an ensemble of classifiers approach and a co-training based approach, and
show how both of these methods outperform a straightforward stacked vector approach often used
in multi-temporal image classification. Additionally, the co-training based method addresses the
challenge of limited labeled training data in supervised classification, as this classification scheme
utilizes a large number of unlabeled samples (which comes for free) in conjunction with a small
set of labeled training data.
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,63,1,MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,"VARUN CHANDOLA,  RANGA RAJU VATSAVAI  ","Multispectral remote sensing images have been widely used for automated land use
and land cover classification tasks. Often thematic classification is done using single date image,
however in many instances a single date image is not informative enough to distinguish between
different land cover types. In this paper we show how one can use multiple images, collected at
different times of year (for example, during crop growing season), to learn a better classifier. We
propose two approaches, an ensemble of classifiers approach and a co-training based approach, and
show how both of these methods outperform a straightforward stacked vector approach often used
in multi-temporal image classification. Additionally, the co-training based method addresses the
challenge of limited labeled training data in supervised classification, as this classification scheme
utilizes a large number of unlabeled samples (which comes for free) in conjunction with a small
set of labeled training data.
Source:https://data.nasa.gov/dataset/MULTI-TEMPORAL-REMOTE-SENSING-IMAGE-CLASSIFICATION/y8zc-3wup"""
Image classification,"Land Use and Land Cover Change
Detection using Remote Sensing and
Geographic Information System in
Bodri Watersh...","Nobukazu Nakagoshi, Jonh Piter G. LUBIS","Remote sensing and GIS technologies are very useful for mapping LULC patterns and detailing the dynamics of a certain
area. The utilization of these technologies has been applied to Bodri watershed in Central Java, Indonesia. The aims of this
study are to map and detect LULC changes in Bodri watershed over the last two decades (1991-2001 and 2001-2009). Three
data sets of Landsat image acquired on June 28, 1991; July 1, 2001 and June 21, 2009 were used for this analysis. Image
classification for mapping LULC was performed by supervised classification through the maximum likelihood method. All of
the visible and infrared bands (bands 1-5 and 7) were used for the analysis. LULC change processes were then detected through
post classification comparison method. Analyzing LULC changes associated to its slope and elevation were performed by
overlaying the classified image with slope and elevation maps which had been extracted from digital topographic map
(1:25,000). Six LULC categories were classified as forest, tree plantation, dry farming field, paddy field, settlement and water.
The results show that in the period of 1991-2001 there were increases in settlement (56.22%), dry farming field (41.77%) and
water (32.41%), and decrease in forest (-31.85%), tree plantation (-15.19%) and paddy field (-12.23%). While, between 2001
and 2009 increase occurred in water (34.17%), tree plantation (12.63%) and settlement (7.47%), and decrease in paddy field (-
26.01%), forest (-12.33%) and dry farming field (-1.14%). The changes predominantly took place in the gentle slope (0-8%)
and the low elevation (0-500 m) areas. These results show that urbanization and agricultural activities have occurred in these
areas. The low land and gentle slope were more widely affected by and vulnerable to human and agriculture caused LULC
changes than highland and steep slope areas.",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,63,0,,,
Image classification,"Land Use and Land Cover Change Impact on Groundwater Recharge: The Case of Lake Haramaya Watershed, Ethiopia","Shimelis B. Gebere, Tena Alamirew, Broder J. Merkel,  Assefa M. Melesse ","Anthropogenic actions have been dramatically changing the land cover of the earth with a substantial impact on the soil, water, and atmosphere. Haramaya watershed is located in the eastern part of Ethiopia which encompasses the dry Lake Haramaya, primarily used for agricultural production with burgeoning population, dramatic changes in land use land cover have been observed over the past few decades. The land cover changes have impacted the water balance of the watershed by changing groundwater level. This study focuses on assessing the impact of land use land cover changes on groundwater recharge potential of the watershed. Future land use change was simulated using CLUE-S (Conversion of Land Use and its Effects at Small regional extent) land use change model. The result showed an increase in chat cultivation from 6276 ha in 2011 to 7282 and 7000 ha in 2028 under current conditions (scenario-1) and good watershed management (scenario-2), respectively. Chat (Catha edulis) also referred to as Khat, is a stimulant plant chewed as a tradition but labeled as drug by the World Health Organization (WHO). Cultivated land declined from 4975 ha in 2011 to 3999 and 4013 ha in 2028 under both scenarios 1 and 2, respectively. The simulated result of the WetSpass water balance model showed that the groundwater recharge in the watershed is strongly influenced by land use land cover change. The annual groundwater recharge in the year 2011 ranged from 0 to 90 mm. A land use land cover projection to 2028 with baseline and good management scenarios showed the range of recharge values decreased to 0–83 and 0–87 mm, respectively. At the same time, groundwater level will continue declining due to increased abstraction. Therefore, it is recommended that the concerned authorities should consider the impact of land use change on the water resources of the watershed in order to optimally utilize the available water resources and to find alternative water sources to fill the deficit resulting from groundwater table decline.

",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,63,0,,,
Image classification,"Spatial modelling of groundwater quality across a land use and land cover gradient in Limpopo Province, South Africa","Timothy Dube, Cletah Shoko, Mbulisi Siband, Moses M.Baloyi, Mmasabata Moleko, Dimakatso Nkun, Bentley Rafap, Berel M.Rampheri","The spatial variability of groundwater quality in rural Mokopane District is investigated. This was achieved by evaluating the variability in groundwater physio-chemical parameters in relation to Land Use and Land Cover (LULC)using Kriging geo-statistical technique and key groundwater physio-chemical parameters namely; ammonia, lead, Total Dissolved Solids (TDS), nitrates and chloride. Firstly, we derived the LULC from 30-m Landsat 8-OLI satellite data, with an overall accuracy of 76.67%. Water quality parameters were modelled in geographic information systems using the kriging interpolation technique. We then compared measured water quality and the drinking standards recommended by the South African Water Quality Standards (SAWS) and the World Health Organization (WHO) guideline standards for drinking water. Results indicated that there were significant differences (p < 0.05) in the levels of physio-chemical contents between borehole water and standard values according to the SAWS and WHO. For example, TDS in Mosesetjane exceeded both SAWS and WHO drinking water levels. Nitrate concentrations were also high on the first and second quarter with concentration levels of 5.56–29 mg/l and 30.9 mg/l in Mosesetjane, respectively. However, lead and ammonia were found to be evenly distributed in all the four quarters, but exhibited some slight deviations from the standard levels (±4.7 mg/l) in selected quarters. Results also indicated significant differences in TDS, chloride and nitrate concentrations across different LULC types. Subsistence farming and built-up areas were the main LULC types that were found to have high concentrations of TDS, ammonia and nitrates, exceeding the recommended standards. However, lead and ammonia, did not differ significantly (p > 0.05) and these parameters were fairly distributed, with less spatial variability . The second quarter, extending between October and December, and the fourth quarter, extending from April to June, exhibited high concentrations of nitrates, chloride and TDS during the wet season. On the other hand, the first and third quarter of the dry period were associated with low concentrations. This study indicated that LULC types and seasonal variability have influence on specific groundwater quality parameters.
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,63,0,,,
Image classification,"Classifying Multivariate Time Series by Learning Sequence-level Discriminative Patterns","Guruprasad Nayak, Varun Mithal, Xiaowei Jia,  Vipin Kumar","Time series classification algorithms designed to use local context do not work on landcover classification problems where the instances of the two classes may often exhibit similar feature values due to the large natural variations in other land covers across the year and unrelated phenomena that they undergo. In this paper, we propose to learn discriminative patterns from the entire length of the time series, and use them as predictive features to identify the class of interest. We propose a novel neural network algorithm to learn the key signature of the class of interest as a function of the feature values together with the discriminative pattern made from that signature through the entire time series in a joint framework. We demonstrate the utility of this technique on the landcover classification application of burned area mapping that is of considerable societal importance.
",MULTI-TEMPORAL REMOTE SENSING IMAGE CLASSIFICATION - A MULTI-VIEW APPROACH,63,0,,,
Image classification,"Insights into few shot learning approaches for image scene classification
","Mohamed Soudy​, Yasmine Afify, Nagwa Badr ","Image understanding and scene classification are keystone tasks in computer vision. The development of technologies and profusion of existing datasets open a wide room for improvement in the image classification and recognition research area. Notwithstanding the optimal performance of exiting machine learning models in image understanding and scene classification, there are still obstacles to overcome. All models are data-dependent that can only classify samples close to the training set. Moreover, these models require large data for training and learning. The first problem is solved by few-shot learning, which achieves optimal performance in object detection and classification but with a lack of eligible attention in the scene classification task. Motivated by these findings, in this paper, we introduce two models for few-shot learning in scene classification. In order to trace the behavior of those models, we also introduce two datasets (MiniSun; MiniPlaces) for image scene classification. Experimental results show that the proposed models outperform the benchmark approaches in respect of classification accuracy.",MiniSun,64,1,MiniSun,Mohamed Soudy,"The Minisun dataset contains 100 classes randomly chosen from Sun397 with 100 images of size 84×84 pixels per class. It is split into 64 base classes, 16 validation classes, and 20 novel classes."
Image classification,"GenericConv: A Generic Model for Image Scene Classification Using Few-Shot Learning
","Mohamed Soudy,Yasmine M. Afify, Nagwa Badr  ","Scene classification is one of the most complex tasks in computer-vision. The accuracy of scene classification is dependent on other subtasks such as object detection and object classification. Accurate results may be accomplished by employing object detection in scene classification since prior information about objects in the image will lead to an easier interpretation of the image content. Machine and transfer learning are widely employed in scene classification achieving optimal performance. Despite the promising performance of existing models in scene classification, there are still major issues. First, the training phase for the models necessitates a large amount of data, which is a difficult and time-consuming task. Furthermore, most models are reliant on data previously seen in the training set, resulting in ineffective models that can only identify samples that are similar to the training set. As a result, few-shot learning has been introduced. Although few attempts have been reported applying few-shot learning to scene classification, they resulted in perfect accuracy. Motivated by these findings, in this paper we implement a novel few-shot learning model—GenericConv—for scene classification that has been evaluated using benchmarked datasets: MiniSun, MiniPlaces, and MIT-Indoor 67 datasets. The experimental results show that the proposed model GenericConv outperforms the other benchmark models on the three datasets, achieving accuracies of 52.16 ± 0.015, 35.86 ± 0.014, and 37.26 ± 0.014 for five-shots on MiniSun, MiniPlaces, and MIT-Indoor 67 datasets, respectively. View Full-Text

",MiniSun,64,0, ,,
Image classification,"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response","Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi","During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.","Datasets for Social Media Image Classification for Disaster Response",65,1,"Datasets for Social Media Image Classification for Disaster Response
",Firoj Alam,"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.                                                                                                                                                                                                                                        Source:https://data.mendeley.com/datasets/6kmy3pp3th"
Image classification,"CrisisBench: Benchmarking Crisis-related Social Media Datasets
for Humanitarian Information Processing","Firoj Alam, Hassan Sajjad, Muhammad Imran, Ferda Ofli","Time-critical analysis of social media streams is important
for humanitarian organizations for planing rapid response
during disasters. The crisis informatics research community
has developed several techniques and systems for processing and classifying big crisis-related data posted on social
media. However, due to the dispersed nature of the datasets
used in the literature (e.g., for training models), it is not possible to compare the results and measure the progress made
towards building better models for crisis informatics tasks.
In this work, we attempt to bridge this gap by combining
various existing crisis-related datasets. We consolidate eight
human-annotated datasets and provide 166.1k and 141.5k
tweets for informativeness and humanitarian classification
tasks, respectively. We believe that the consolidated dataset
will help train more sophisticated models. Moreover, we provide benchmarks for both binary and multiclass classification tasks using several deep learning architecrures including,
CNN, fastText, and transformers. We make the dataset and
scripts available at https://crisisnlp.qcri.org/
crisis_datasets_benchmarks.html.","Datasets for Social Media Image Classification for Disaster Response",65,0,,,
Image classification,"MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification","Firoj Alam, Tanvirul Alam, Md. Arid Hasan, Abul Hasnat, Muhammad Imran, Ferda Ofli","Recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). While notable progress has been made using texts, research on exploiting the images remains relatively under-explored. To advance image-based approaches, we propose MEDIC (Available at: this https URL), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. This is the first dataset of its kind: social media images, disaster response, and multi-task learning research. An important property of this dataset is its high potential to facilitate research on multi-task learning, which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. Therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. We experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. Along with the dataset, we also release all relevant scripts (this https URL).","Datasets for Social Media Image Classification for Disaster Response",65,0,,,
Image classification,"Robust Training of Social Media Image Classification Models for Rapid Disaster Response","Firoj Alam, Tanvirul Alam, Muhammad Imran, Ferda Ofli","Images shared on social media help crisis managers gain situational awareness and assess incurred damages, among other response tasks. As the volume and velocity of such content are typically high, real-time image classification has become an urgent need for a faster disaster response. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of the damage. To develop robust real-time models, it is necessary to understand the capability of the publicly available pre-trained models for these tasks, which remains to be under-explored in the crisis informatics literature. In this study, we address such limitations by investigating ten different network architectures for four different tasks using the largest publicly available datasets for these tasks. We also explore various data augmentation strategies, semi-supervised techniques, and a multitask learning setup. In our extensive experiments, we achieve promising results.","Datasets for Social Media Image Classification for Disaster Response
",65,0,,,
Image classification,"Tune It or Don't Use It: Benchmarking Data-Efficient Image Classification
"," Lorenzo Brigato, Björn Barz, Luca Iocchi, Joachim Denzler","Data-efficient image classification using deep neural networks in settings, where only small amounts of labeled data are available, has been an active research area in the recent past. However, an objective comparison between published methods is difficult, since existing works use different datasets for evaluation and often compare against untuned baselines with default hyper-parameters. We design a benchmark for data-efficient image classification consisting of six diverse datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types (RGB, grayscale, multispectral). Using this benchmark, we re-evaluate the standard cross-entropy baseline and eight methods for data-efficient deep learning published between 2017 and 2021 at renowned venues. For a fair and realistic comparison, we carefully tune the hyper-parameters of all methods on each dataset. Surprisingly, we find that tuning learning rate, weight decay, and batch size on a separate validation split results in a highly competitive baseline, which outperforms all but one specialized method and performs competitively to the remaining one.
",ImageNet 50 samples per class Dataset,66,1,ImageNet 50 samples per class Dataset," Lorenzo Brigato, Björn Barz, Luca Iocchi, Joachim Denzler","This ImageNet version contains only 50 training images per class while the original testing set remains unchanged. It is one of the datasets comprising the data-efficient image classification (DEIC) benchmark. It was proposed to challenge the generalization capabilities of modern image classifiers
"
Image classification,"Parametric Scattering Networks","Shanel Gauthier, Benjamin Thérien, Laurent Alsène-Racicot, Muawiz Chaudhary, Irina Rish, Eugene Belilovsky, Michael Eickenberg, Guy Wolf","The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate whether this standard wavelet filterbank construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform. Moreover, our empirical results suggest that traditional filterbank constructions may not always be necessary for scattering transforms to extract effective representations.
",ImageNet 50 samples per class Dataset,66,0,,,
Image classification,"Towards Benchmarking and Evaluating Deepfake Detection","Chenhao Lin, Jingyi Deng, Pengbin Hu, Chao Shen, Qian Wang, Qi Li","Deepfake detection automatically recognizes the manipulated medias through the analysis of the difference between manipulated and non-altered videos. It is natural to ask which are the top performers among the existing deepfake detection approaches to identify promising research directions and provide practical guidance. Unfortunately, it's difficult to conduct a sound benchmarking comparison of existing detection approaches using the results in the literature because evaluation conditions are inconsistent across studies. Our objective is to establish a comprehensive and consistent benchmark, to develop a repeatable evaluation procedure, and to measure the performance of a range of detection approaches so that the results can be compared soundly. A challenging dataset consisting of the manipulated samples generated by more than 13 different methods has been collected, and 11 popular detection approaches (9 algorithms) from the existing literature have been implemented and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92 models have been trained and 644 experiments have been performed for the evaluation. The results along with the shared data and evaluation methodology constitute a benchmark for comparing deepfake detection approaches and measuring progress.
",ImageNet 50 samples per class Dataset,66,0,,,
Image classification,"A Strong Baseline for the VIPriors Data-Efficient Image Classification Challenge","Björn Barz, Lorenzo Brigato, Luca Iocchi, Joachim Denzler  
","Learning from limited amounts of data is the hallmark of intelligence, requiring strong generalization and abstraction skills. In a machine learning context, data-efficient methods are of high practical importance since data collection and annotation are prohibitively expensive in many domains. Thus, coordinated efforts to foster progress in this area emerged recently, e.g., in the form of dedicated workshops and competitions. Besides a common benchmark, measuring progress requires strong baselines. We present such a strong baseline for data-efficient image classification on the VIPriors challenge dataset, which is a sub-sampled version of ImageNet-1k with 100 images per class. We do not use any methods tailored to data-efficient classification but only standard models and techniques as well as common competition tricks and thorough hyper-parameter tuning. Our baseline achieves 69.7% accuracy on the VIPriors image classification dataset and outperforms 50% of submissions to the VIPriors 2021 challenge.",ImageNet 50 samples per class Dataset,66,0,,,
Image classification,"Fuzzy Deep Forest with Deep Contours Feature for Leaf Cultivar Classification","Wenbo Zheng, Lan Yan, Chao Gou ,Fei-Yue Wang","Deep learning is a compelling technique for feature extraction due to its adaptive capacity of processing and providing deeper image information. However, for the task of leaf cultivar classification, the deep learning-based classifier model is unable to extract contour features of leaf images deeply due to the lack of large specialized datasets and expert knowledge annotations. Also, the scale/size of the current leaf cultivar dataset does not meet the needs of deep neural networks. In particular, the high model complexity of deep neural networks (DNN) implies that deep-learning-based neural networks seem to must require a large dataset to achieve good performance, but facing the fact that the leaf cultivar dataset often is small, even some classes in this kind of datasets contain less than ten images/examples. To overcome these problems and inspired by the resounding success of fuzzy logic, we propose a novel fuzzy ensemble model for leaf cultivar classification. To extract the contours of leaves, we first propose generative adversarial networks (GANs) based methods. Second, to improve the ability of feature representation, we present a data augmentation method to transform our contour features. Thirdly, to get the essential features of leaves, we design a novel generation of the fuzzy random forest. Finally, to achieve accurate classification, we design a novel deep learning strategy, namely fuzzy deep representation learning, integrating and cascading a lot of our fuzzy random forests. Experimental results show that our model outperforms other existing stateof- the-arts on three real-world datasets, and performs much better than the original deep forest and DNN-based algorithms particularly.",ImageNet 50 samples per class Dataset,66,0,,,
Image classification,"Exploring the Optimality of Tight-Frame Scattering Networks 
","Shanel Gauthier, Benjamin Thérien, Laurent Alsène-Racicot, Muawiz Sajjad Chaudhary, Irina Rish, Eugene Belilovsky, Michael Eickenberg, Guy Wolf","The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations, and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate if such a tight frame construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform.  Moreover, our empirical results suggest that tight-frames may not always be necessary for scattering transforms to extract effective representations.
",ImageNet 50 samples per class Dataset,66,0,,,
Image classification,"Classification of Butterfly Species Using the Convolutional Neural Network Method
","Micheal Micheal","Butterflies are one of the insects of the ordo Lepideptora. Butterflies have many species with different wing patterns. This study aims to classify butterfly species using the Convolutional Neural Network (CNN) method with the VGG-16 architecture and LeNet with the Adam, Adagrad, and SGD optimizers. The dataset contains 5455 images and is divided into 4955 train data, 250 test data, and 250 valid data. Then the dataset is augmented on 8000 train data for each class and 800 test data for each class. In this study, the highest accuracy level for each architecture was obtained using the Adam optimizer so that the accuracy rate using VGG-16 was 93% and by using LeNet was 67%.",Butterfly Image Classification 75 species,67,1,Butterfly Image Classification 75 species,Gerry,"Train, Test. Validation data set for 75 butterfly species. All images are 224 X 224 X 3 in jpg format . Train set consists of 9285 images partitioned into 75 sub directories one for each species. Test set consists of 375 images partitioned into 75 sub directories with 5 test images per species. Valid set consists of 375 images partitioned into 750 sub directories with 5 validation images per species. 3 CSV files are included. The butterflies.csv file consists of 3 columns with 10,035 rows, one row for each image in the dataset. The columns are filepaths, labels and data set. The filepaths column contains the relative path to the image. The labels column contains the species label associated with the image file. The data set column specifies which dataset (train, test or valid) the associated image belongs to. The class_dict.csv file is provided to enable use of the trained model. It consists of 6 columns, class_index, class, SCIENTIFIC NAME, height, width, and scale by and 75 rows one row for each species. class_index is the integer index of the class. class is the species name which is the common name for the butterfly species. SCIENTIFIC NAME is the official scientific name of the species. Height and width are the values used by the model when trained. scale by is the value to scale the image pixels by. For EfficientNet model the scale is 1 since these models expect pixels in the range 0 to 255. The CLASS NAMES.csv file has 2 columns by 75 rows. The COMMON NAME column contains the common name of the butterfly and the SCIENTIFIC NAME column contains the associated scientific name of the species. Finally the 6 images file contains 6 butterfly images. This file is used to demonstrate use of the prediction function within the kernel."
Image classification,"Fruits Classification and Grading Using VGG-16 Approach","Nishtha Parashar, Aman Mishra, Yatin Mishra ","Due to availability of human resources in India, nearly all the horticulture work is done manually by workers which involves the possibility of human error, and we get to have some rotten or not so fresh fruits. To avoid this possibility of human error and mistakes it is required to have an automated system in the field of horticulture in India. Numerous research papers have been proposed to address these external issues, however they all have drawbacks and limitations, such as low accuracy due to training on a smaller number of datasets and use of weak algorithms in some cases. In this paper, we present a CNN based approach that separates fresh and rotten fruits of different kinds. To reinforce our experimental results, we trained on a dataset of 14,000 images, resulting in an accuracy of 93.52%.
","Fresh and Rotten Fruits Dataset for Machine-Based Evaluation of Fruit Quality",68,1,"Fresh and Rotten Fruits Dataset for Machine-Based Evaluation of Fruit Quality","Nusrat Sultana","(1) Everyone is interested to get fresh and quality fruits. As fruits are going to be rotten after the passing of time. Hence, fruit quality has substantial economic consequences. It is estimated that roughly one-third of the fruits are rotten causing huge financial loss. Furthermore, the sale of fruits will be impacted because consumers believe that spoiled fruits are harmful to their health. Classification of fresh and rotten fruits is usually carried out by people, which is ineffective for fruit farmers, sellers as well as fruit processing industries. (2) In the recent era, computer vision techniques are very promising in performing such types of classification and detection tasks. (3) With a view to developing computer vision-based algorithms, an extensive fruit dataset is presented containing sixteen types of fruit classes, namely fresh apple, rotten apple, fresh banana, rotten banana, fresh orange, rotten orange, fresh grape, rotten grape, fresh guava, rotten guava, fresh jujube, rotten jujube, fresh pomegranate, rotten pomegranate, fresh strawberry and rotten strawberry. Fresh and rotten classifications are done with the help of a domain expert from an agricultural institute. (4) A total of 3200 images of fresh and rotten fruits are collected from different fruit shops and real fields. Then from these original images, a total of 12335 augmented images are produced by using rotation, flipping, zooming, and shearing techniques to increase the data number.

"
Image classification,"IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification

","Yuki Shimahara, Ko Sugawara, Kei H. Kojo, Hiroki Kawai, Yuya Yoshida, Seiichiro Hasezawa, Natsumaro Kutsuna ","Automated quantitative image analysis is essential for all fields of life science research. Although several software programs and algorithms have been developed for bioimage processing, an advanced knowledge of image processing techniques and high-performance computing resources are required to use them. Hence, we developed a cloud-based image analysis platform called IMACEL, which comprises morphological analysis and machine learning-based image classification. The unique click-based user interface of IMACEL’s morphological analysis platform enables researchers with limited resources to evaluate particles rapidly and quantitatively without prior knowledge of image processing. Because all the image processing and machine learning algorithms are performed on high-performance virtual machines, users can access the same analytical environment from anywhere. A validation study of the morphological analysis and image classification of IMACEL was performed. The results indicate that this platform is an accessible and potentially powerful tool for the quantitative evaluation of bioimages that will lower the barriers to life science research.
","IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification",69,1,"IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification","Yuki Shimahara, Ko Sugawara, Kei H. Kojo, Hiroki Kawai, Yuya Yoshida, Seiichiro Hasezawa, Natsumaro Kutsuna ","Automated quantitative image analysis is essential for all fields of life science research. Although several software programs and algorithms have been developed for bioimage processing, an advanced knowledge of image processing techniques and high-performance computing resources are required to use them. Hence, we developed a cloud-based image analysis platform called IMACEL, which comprises morphological analysis and machine learning-based image classification. The unique click-based user interface of IMACEL’s morphological analysis platform enables researchers with limited resources to evaluate particles rapidly and quantitatively without prior knowledge of image processing. Because all the image processing and machine learning algorithms are performed on high-performance virtual machines, users can access the same analytical environment from anywhere. A validation study of the morphological analysis and image classification of IMACEL was performed. The results indicate that this platform is an accessible and potentially powerful tool for the quantitative evaluation of bioimages that will lower the barriers to life science research.
"
Image classification,"A two-stage method for automated detection of ring-like endosomes in fluorescent microscopy images
","Dongyun Lin, Zhiping Lin, Jiuwen Cao, Ramraj Velmurugan, E. Sally Ward, Raimund J. Ober","Endosomes are subcellular organelles which serve as important transport compartments in eukaryotic cells. Fluorescence microscopy is a widely applied technology to study endosomes at the subcellular level. In general, a microscopy image can contain a large number of organelles and endosomes in particular. Detecting and annotating endosomes in fluorescence microscopy images is a critical part in the study of subcellular trafficking processes. Such annotation is usually performed by human inspection, which is time-consuming and prone to inaccuracy if carried out by inexperienced analysts. This paper proposes a two-stage method for automated detection of ring-like endosomes. The method consists of a localization stage cascaded by an identification stage. Given a test microscopy image, the localization stage generates a voting-map by locally comparing the query endosome patches and the test image based on a bag-of-words model. Using the voting-map, a number of candidate patches of endosomes are determined. Subsequently, in the identification stage, a support vector machine (SVM) is trained using the endosome patches and the background pattern patches. Each of the candidate patches is classified by the SVM to rule out those patches of endosome-like background patterns. The performance of the proposed method is evaluated with real microscopy images of human myeloid endothelial cells. It is shown that the proposed method significantly outperforms several state-of-the-art competing methods using multiple performance metrics.
","IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification
",69,0," 
",,
Image classification,"Effectiveness of Create ML in microscopy image classifications: a simple and inexpensive deep learning pipeline for non-data scientists","Kiyotaka Nagaki, Tomoyuki Furuta, Naoki Yamaji, Daichi Kuniyoshi, Megumi Ishihara, Yuji Kishima, Minoru Murata, Atsushi Hoshino,  Hirotomo Takatsuka ","Observing chromosomes is a time-consuming and labor-intensive process, and chromosomes have been analyzed manually for many years. In the last decade, automated acquisition systems for microscopic images have advanced dramatically due to advances in their controlling computer systems, and nowadays, it is possible to automatically acquire sets of tiling-images consisting of large number, more than 1000, of images from large areas of specimens. However, there has been no simple and inexpensive system to efficiently select images containing mitotic cells among these images. In this paper, a classification system of chromosomal images by deep learning artificial intelligence (AI) that can be easily handled by non-data scientists was applied. With this system, models suitable for our own samples could be easily built on a Macintosh computer with Create ML. As examples, models constructed by learning using chromosome images derived from various plant species were able to classify images containing mitotic cells among samples from plant species not used for learning in addition to samples from the species used. The system also worked for cells in tissue sections and tetrads. Since this system is inexpensive and can be easily trained via deep learning using scientists’ own samples, it can be used not only for chromosomal image analysis but also for analysis of other biology-related images.
","IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification
",69,0,,,
Image classification,Bio-Image Based Prediction of Protein Subcellular Localization Using Adaptive Threshold,Hiba Khurshid,"Over the past few decades, with the increase in microscopic imaging a very rapid
progress has been made in predicting Protein subcellular localization. Knowledge
of Protein subcellular localization is very important in understanding the function
of protein. During the drug discovery, it can significantly improve the target
identification process. Protein subcellular localization is also very important in
disease discovery. It has been prove that abnormal protein subcellular localization
causes diseases and can even involve cancer. Many researcher has come up with
different model to predict Protein Subcellular Localization. With the advancement
in deep learning models different architecture of CNN has been vastly used for
classification of protein. But CNN comes with the computational cost and other
drawbacks. In order to predict protein subcellular localization with high accuracy
this study propose a methodology that uses Otsu’s adaptive thresholding technique
which calculate threshold value for each image by using image histogram. With
this threshold value this methodology generates three binary images and for each
binary image it extracts 9 feature vectors by counting the number of white pixel
in neighboring pixel. In training phase Multi Label Random Forest classifier is
applied on the extracted features to predict Protein subcellular localization. In
order to evaluate this proposed technique this study used recently publish data
HPA (version 18) and outperformed the state-of-the-art technique (macro f1 –score
0.59) by achieving macro f1-score of 0.63. This study also evaluated this technique
against the fixed threshold and achieve macro f1-score of 0.44 which proves that
adaptive threshold achieve more accuracy then fixed threshold technique.","IMACEL: A cloud-based bioimage analysis platform for morphological analysis and image classification
",69,0,,,
Image classification,"Fast and Accurate Deep Learning Architecture on Vehicle Type
Recognition","Narong Boonsirisumpun, Olarik Surinta","Vehicle Type Recognition has a significant problem that happens
when people need to search for vehicle data from a video surveillance
system at a time when a license plate does not appear in the image.
This paper proposes to solve this problem with a deep learning
technique called Convolutional Neural Network (CNN), which is one
of the latest advanced machine learning techniques. In the
experiments, researchers collected two datasets of Vehicle Type
Image Data (VTID I & II), which contained 1,310 and 4,356 images,
respectively. The first experiment was performed with 5 CNN
architectures (MobileNets, VGG16, VGG19, Inception V3, and
Inception V4), and the second experiment with another 5 CNNs
(MobileNetV2, ResNet50, Inception ResNet V2, Darknet-19, and
Darknet-53) including several data augmentation methods. The
results showed that MobileNets, when combine with the brightness
augmented method, significantly outperformed other CNN
architectures, producing the highest accuracy rate at 95.46%. It was
also the fastest model when compared to other CNN networks.","Vehicle Type Image Dataset (Version 2): VTID2",70,1,"Vehicle Type Image Dataset (Version 2): VTID2",olarik surinta,"After creating VTID, the researchers decided to extend the collection process to create another larger dataset to add further diversity to the dataset in order to avoid data overfitting. Finally, the new dataset, called ""Vehicle Type Image Dataset 2 (VTID2)"", consisted of 4,356 image samples that could be separated into five vehicle type classes as follows: 1,230 sedans, 1,240 pick-ups, 680 SUVs, 606 hatchbacks, and 600 other vehicle images.

"
Question answering,"Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge
","Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Peter Clark ","We present the ARC-DA dataset, a direct-answer (""open response"", ""freeform"") version of the ARC (AI2 Reasoning Challenge) multiple-choice dataset. While ARC has been influential in the community, its multiple-choice format is unrepresentative of real-world questions, and multiple choice formats can be particularly susceptible to artifacts. The ARC-DA dataset addresses these concerns by converting questions to direct-answer format using a combination of crowdsourcing and expert review. The resulting dataset contains 2985 questions with a total of 8436 valid answers (questions typically have more than one valid answer). ARC-DA is one of the first DA datasets of natural questions that often require reasoning, and where appropriate question decompositions are not evident from the questions themselves. We describe the conversion approach taken, appropriate evaluation metrics, and several strong models. Although high, the best scores (81% GENIE, 61.4% F1, 63.2% ROUGE-L) still leave considerable room for improvement. In addition, the dataset provides a natural setting for new research on explanation, as many questions require reasoning to construct answers. We hope the dataset spurs further advances in complex question-answering by the community. 
","arc-da
",71,1,"arc-da
","Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Peter Clark ","A dataset of 2,985 grade-school level, direct-answer (""open response"", ""free form"") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018.
"
Question answering,"WebGPT: Browser-assisted question-answering with human feedback
","Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman","We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.
","arc-da
",71,0,,,
Question answering,"TruthfulQA: Measuring How Models Mimic Human Falsehoods","Stephanie Lin, Jacob Hilton, Owain Evans","We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","arc-da
",71,0,,,
Question answering,"GooAQ: Open Question Answering with Diverse Answer Types
","Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, Chris Callison-Burch
","While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types.
","arc-da
",71,0,,,
Question answering,"General-Purpose Question-Answering with Macaw","Oyvind Tafjord, Peter Clark","Despite the successes of pretrained language models, there are still few high-quality, general-purpose QA systems that are freely available. In response, we present Macaw, a versatile, generative question-answering (QA) system that we are making available to the community. Macaw is built on UnifiedQA, itself built on T5, and exhibits strong performance, zero-shot, on a wide variety of topics, including outperforming GPT-3 by over 10% (absolute) on Challenge300, a suite of 300 challenge questions, despite being an order of magnitude smaller (11 billion vs. 175 billion parameters). In addition, Macaw allows different permutations (""angles"") of its inputs and outputs to be used, for example Macaw can take a question and produce an answer; or take an answer and produce a question; or take an answer and question, and produce multiple-choice options. We describe the system, and illustrate a variety of question types where it produces surprisingly good answers, well outside the training setup. We also identify question classes where it still appears to struggle, offering insights into the limitations of pretrained language models. Macaw is freely available, and we hope that it proves useful to the community. Macaw is available at this https URL","arc-da
",71,0,,,
Question answering,"End-to-End Mongolian Text-to-Speech System","Jingdong Li, Hui Zhang, Rui Liu, Xueliang Zhang, Feilong Bao","Speech synthesis, or text-to-speech (TTS), generates a speech waveform of the given text. To build a satisfactory TTS system, a large natural speech corpus is requested. In the traditional approach, the corpus should be accompanied with precise annotations. However, the annotation is difficult and costly. Recently, end-to-end speech synthesis methods are proposed, which eliminated the requirement of annotation. The end-to-end methods make the development of TTS system less costly and easier. We used the state-of-the-art end-to-end Tacotron model in the Mongolian TTS task. With much more unannotated speech data (about 17 hours), the new system beats the old best Mongolian TTS system, which is trained on a small amount of annotated data (about 5 hours), with a big margin. The new mean opinion score (MOS) is 3.65 vs 2.08 which is the old one. The proposed system becomes the first Mongolian TTS system can be utilized in real applications.
",Mongolian daily question and answer corpus dataset,72,1,Mongolian daily question and answer corpus dataset,"te ri ge le hu, Siriguleng Wang
","This dataset is from the Chinese open question answering dataset. After preliminary screening and processing, it is translated into Mongolian and the corpus is manually corrected. Finally, a corpus of 50 thousand pairs of Mongolian question and answer is constructed
"
Question answering,WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss,"Rui Liu, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li","Tacotron-based text-to-speech (TTS) systems directly synthesize speech from text input. Such frameworks typically consist of a feature prediction network that maps character sequences to frequency-domain acoustic features, followed by a waveform reconstruction algorithm or a neural vocoder that generates the time-domain waveform from acoustic features. As the loss function is usually calculated only for frequency-domain acoustic features, that doesn't directly control the quality of the generated time-domain waveform. To address this problem, we propose a new training scheme for Tacotron-based TTS, referred to as WaveTTS, that has 2 loss functions: 1) time-domain loss, denoted as the waveform loss, that measures the distortion between the natural and generated waveform; and 2) frequency-domain loss, that measures the Mel-scale acoustic feature loss between the natural and generated acoustic features. WaveTTS ensures both the quality of the acoustic features and the resulting speech waveform. To our best knowledge, this is the first implementation of Tacotron with joint time-frequency domain loss. Experimental results show that the proposed framework outperforms the baselines and achieves high-quality synthesized speech.
",Mongolian daily question and answer corpus dataset,72,0,,,
Question answering,"Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for Mongolian Speech Synthesis
","Rui Liu, Berrak Sisman, Feilong Bao, Jichen Yang, Guanglai Gao, Haizhou Li","Prosodic phrasing is an important factor that affects naturalness and intelligibility in text-to-speech synthesis. Studies show that deep learning techniques improve prosodic phrasing when large text and speech corpus are available. However, for low-resource languages, such as Mongolian, prosodic phrasing remains a challenge for various reasons. First, the database suitable for system training is limited. Second, word composition knowledge that is prosody-informing has not been used in prosodic phrase modeling. To address these problems, in this article, we propose a feature augmentation method in conjunction with a self-attention neural classifier. We augment input text with morphological and phonological decompositions of words to enhance the text encoder. We study the use of self-attention classifier, that makes use of global context of a sentence, as a decoder for phrase break prediction. Both objective and subjective evaluations validate the effectiveness of the proposed phrase break prediction framework, that consistently improves voice quality in a Mongolian text-to-speech synthesis system.",Mongolian daily question and answer corpus dataset,72,0,,,
Question answering,Modeling Prosodic Phrasing With Multi-Task Learning in Tacotron-Based TTS,"Rui Liu, Berrak Sisman, Feilong Bao,  Guanglai Gao,  Haizhou Li","Tacotron-based end-to-end speech synthesis has shown remarkable voice quality. However, the rendering of prosody in the synthesized speech remains to be improved, especially for long sentences, where prosodic phrasing errors can occur frequently. In this letter, we extend the Tacotron-based speech synthesis framework to explicitly model the prosodic phrase breaks. We propose a multi-task learning scheme for Tacotron training, that optimizes the system to predict both Mel spectrum and phrase breaks. To our best knowledge, this is the first implementation of multi-task learning for Tacotron based TTS with a prosodic phrasing model. Experiments show that our proposed training scheme consistently improves the voice quality for both Chinese and Mongolian systems.
",Mongolian daily question and answer corpus dataset,72,0,,,
Question answering,"Question Answering on Scholarly Knowledge Graphs
","Mohamad Yaser Jaradeh, Markus Stocker, Sören Auer ","Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present JarvisQA, a BERT based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, JarvisQA is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.
",ORKG QA,73,1,ORKG QA, Mohamad Yaser Jaradeh,"A collection of tables collected from the open research knowledge graph (ORKG) infrastructure, with a set of questions about these tables.
"
Question answering,"Improving Access to Scientific Literature with Knowledge Graphs","Sören Auer, Allard Oelen, Muhammad Haris, Markus Stocker, Jennifer D’Souza, Kheir Eddine Farfar, Lars Vogt, Manuel Prinz, Vitalis Wiens and Mohamad Yaser Jaradeh","The transfer of knowledge has not changed fundamentally for many hundreds of years: It is usually document-based-formerly printed on paper as a classic essay and nowadays as PDF. With around 2.5 million new research contributions every year, researchers drown in a flood of pseudo-digitized PDF publications. As a result research is seriously weakened. In this article, we argue for representing scholarly contributions in a structured and semantic way as a knowledge graph. The advantage is that information represented in a knowledge graph is readable by machines and humans. As an example, we give an overview on the Open Research Knowledge Graph (ORKG), a service implementing this approach. For creating the knowledge graph representation, we rely on a mixture of manual (crowd/expert sourcing) and (semi-)automated techniques. Only with such a combination of human and machine intelligence, we can achieve the required quality of the representation to allow for novel exploration and assistance services for researchers. As a result, a scholarly knowledge graph such as the ORKG can be used to give a condensed overview on the state-of-the-art addressing a particular research quest, for example as a tabular comparison of contributions according to various characteristics of the approaches. Further possible intuitive access interfaces to such scholarly knowledge graphs include domain-specific (chart) visualizations or answering of natural language questions.

",ORKG QA,73,0,,,
Question answering,"Triple Classification for Scholarly Knowledge Graph Completion
","Mohamad Yaser Jaradeh, Kuldeep Singh, Markus Stocker, Sören Auer

 ",,ORKG QA ,73,0,,,
Question answering,"Organizing Scholarly Knowledge leveraging Crowdsourcing,Expert Curation and Automated Techniques","Allard Oelen, Mohamad Yaser Jaradeh, Markus Stocker, Sören Auer ","Research is a fundamental pillar of societal progress. Yet we use antique methods for representing and sharing
scholarly knowledge: scientific articles. Instead of representing research in static PDF articles, we work on a
dynamic knowledge graph, the Open Research Knowledge Graph (ORKG), where ideas, approaches and methods
are represented in machine-readable form. The core rationale of the Open Research Knowledge Graph is to facilitate the manual and automated curation of interlinked, rich semantic descriptions of research contributions. The
task of converting unstructured research papers into structured papers is a cumbersome process that requires domain experts. In order to solve difficulties with crowdsourcing, a more structured approach is needed to support
users in creating uniform structured paper descriptions. We propose a machine-in-the-loop approach to provide
the users guidance during the process of describing a paper. The transformation from unstructured research papers
to structured contributions is mainly performed via crowdsourcing. The ORKG infrastructure makes use of automated techniques to help users while adding new data or by extending the data. Another aspect of knowledge
curation within the ORKG is to extract structured data from unstructured text. The ORKG system wields the
power of structured data to provide yet another interface to explore and interact with scholarly knowledge using
question answering.",ORKG QA,73,0,,,
Question answering,Expert Curation and Automated Techniques,"A Both, A Gashkov, M Eltsova ","Question Answering refers to the task of providing a matching answer for a given user's natural-language question. We assume here that the received response from a Question Answering system is also given as a natural-language output. From this scenario the task is raised to validate if the received natural-language answer is valid w.r.t. the given question. In this paper, we will present our approach to compute the similarity of a question to the corresponding natural-language answer while using the features of the surface form of the question and the answer. Our metrics is the distinction between a matching answer and inappropriate answers to a given question. Hence, our research agenda is dedicated to improving the answer quality of Question Answering systems by using the natural-language representation of its input and output.

",VANiLLa dataset,74,1,VANiLLa dataset,"Debanjali Biswas, Mohnish Dubey, Md Rashad Al Hasan Rony, Jens Lehmann","VANiLLa is a dataset for Question Answering over Knowledge Graphs (KGQA) offering answers in natural language sentences. The answer sentences in this dataset are syntactically and semantically closer to the question than to the triple fact. The dataset consists of over 100k simple questions adapted from the CSQA and SimpleQuestionsWikidata datasets and generated using a semi-automatic framework.
"
Question answering,Improving the Question Answering Quality using Answer Candidate Filtering based on Natural-Language Features,"Aleksandr Gashkov, Aleksandr Perevalov, Maria Eltsova, Andreas Both","Software with natural-language user interfaces has an ever-increasing importance. However, the quality of the included Question Answering (QA) functionality is still not sufficient regarding the number of questions that are answered correctly.In our work, we address the research problem of how the QA quality of a given system can be improved just by evaluating the natural-language input (i.e., the user’s question) and output (i.e., the system’s answer).Our main contribution is an approach capable of identifying wrong answers provided by a QA system. Hence, filtering incorrect answers from a list of answer candidates is leading to a highly improved QA quality. In particular, our approach has shown its potential while removing in many cases the majority of incorrect answers, which increases the QA quality significantly in comparison to the non-filtered output of a system.
",VANiLLa dataset,74,0,,,
Question answering,Improving Question Answering Quality Through Language Feature-Based SPARQL Query Candidate Validation,"Aleksandr Gashkov, Aleksandr Perevalov, Maria Eltsova, Andreas Both ","Question Answering systems are on the rise and on their way to become one of the standard user interfaces. However, in conversational user interfaces, the information quantity needs to be kept low as users expect a limited number of precise answers (often it is 1) – similar to human-human communication. The acceptable number of answers in a result list is a key differentiator from search engines where showing more answers (10–100) to the user is widely accepted. Hence, the quality of Question Answering is crucial for the wide acceptance of such systems. The adaptation of natural-language user interfaces for satisfying the information needs of humans requires high-quality and not-redundant answers. However, providing compact and correct answers to the users’ questions is a challenging task. In this paper, we consider a certain class of Question Answering systems that work over Knowledge Graphs. We developed a system-agnostic approach for optimizing the ranked lists of SPARQL query candidates produced by the Knowledge Graph Question Answering system that are used to retrieve an answer to a given question. We call this a SPARQL query validation process. For the evaluation of our approach, we used two well-known Knowledge Graph Question Answering benchmarks. Our results show a significant improvement in the Question Answering quality. As the approach is system-agnostic, it can be applied to any Knowledge Graph Question Answering system that produces query candidates.",VANiLLa dataset,74,0,,,
Question answering,Deep Learning Based Text Classification: A Comprehensive Review,"Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao","Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.",SQuAD Dataset,75,1,SQuAD Dataset,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang
","The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.
"
Question answering,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen","Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",SQuAD Dataset,75,0,,,
Question answering,ABCDM: An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis,"Mohammad Ehsan Basiri, Shahla Nemati, Moloud Abdar, Erik Cambria, U. Rajendra Acharya","Sentiment analysis has been a hot research topic in natural language processing and data mining fields in the last decade. Recently, deep neural network (DNN) models are being applied to sentiment analysis tasks to obtain promising results. Among various neural architectures applied for sentiment analysis, long short-term memory (LSTM) models and its variants such as gated recurrent unit (GRU) have attracted increasing attention. Although these models are capable of processing sequences of arbitrary length, using them in the feature extraction layer of a DNN makes the feature space high dimensional. Another drawback of such models is that they consider different features equally important. To address these problems, we propose an Attention-based Bidirectional CNN-RNN Deep Model (ABCDM). By utilizing two independent bidirectional LSTM and GRU layers, ABCDM will extract both past and future contexts by considering temporal information flow in both directions. Also, the attention mechanism is applied on the outputs of bidirectional layers of ABCDM to put more or less emphasis on different words. To reduce the dimensionality of features and extract position-invariant local features, ABCDM utilizes convolution and pooling mechanisms. The effectiveness of ABCDM is evaluated on sentiment polarity detection which is the most common and essential task of sentiment analysis. Experiments were conducted on five review and three Twitter datasets. The results of comparing ABCDM with six recently proposed DNNs for sentiment analysis show that ABCDM achieves state-of-the-art results on both long review and short tweet polarity classification.",SQuAD Dataset,75,0,,,
Question answering,Cross-Cultural Polarity and Emotion Detection Using Sentiment Analysis and Deep Learning on COVID-19 Related Tweets,"Ali Shariq Imran, Sher Muhammad Daudpota, Zenun Kastrati, Rakhi Batra","How different cultures react and respond given a crisis is predominant in a society’s norms and political will to combat the situation. Often, the decisions made are necessitated by events, social pressure, or the need of the hour, which may not represent the nation’s will. While some are pleased with it, others might show resentment. Coronavirus (COVID-19) brought a mix of similar emotions from the nations towards the decisions taken by their respective governments. Social media was bombarded with posts containing both positive and negative sentiments on the COVID-19, pandemic, lockdown, and hashtags past couple of months. Despite geographically close, many neighboring countries reacted differently to one another. For instance, Denmark and Sweden, which share many similarities, stood poles apart on the decision taken by their respective governments. Yet, their nation’s support was mostly unanimous, unlike the South Asian neighboring countries where people showed a lot of anxiety and resentment. The purpose of this study is to analyze reaction of citizens from different cultures to the novel Coronavirus and people’s sentiment about subsequent actions taken by different countries. Deep long short-term memory (LSTM) models used for estimating the sentiment polarity and emotions from extracted tweets have been trained to achieve state-of-the-art accuracy on the sentiment140 dataset. The use of emoticons showed a unique and novel way of validating the supervised deep learning models on tweets extracted from Twitter.",SQuAD Dataset,75,0,,,
Question answering,Natural Language Processing Advancements By Deep Learning: A Survey,"Amirsina Torfi, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf, Edward A. Fox","Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.",SQuAD Dataset,75,0,,,
Question answering,A survey on empathetic dialogue systems,"Yukun Ma, Khanh Linh Nguyen, Frank Z.Xing, Erik Cambria","Dialogue systems have achieved growing success in many areas thanks to the rapid advances of machine learning techniques. In the quest for generating more human-like conversations, one of the major challenges is to learn to generate responses in a more empathetic manner. In this review article, we focus on the literature of empathetic dialogue systems, whose goal is to enhance the perception and expression of emotional states, personal preference, and knowledge. Accordingly, we identify three key features that underpin such systems: emotion-awareness, personality-awareness, and knowledge-accessibility. The main goal of this review is to serve as a comprehensive guide to research and development on empathetic dialogue systems and to suggest future directions in this domain.",SQuAD Dataset,75,0,,,
Question answering,Adversarial Training for Large Neural Language Models,"Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao","Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at this https URL.",SQuAD Dataset,75,0,,,
Question answering,A Survey on Text Classification: From Shallow to Deep Learning,"Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S. Yu, Lifang He","Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.",SQuAD Dataset,75,0,,,
Question answering,Very Deep Transformers for Neural Machine Translation,"Xiaodong Liu, Kevin Duh, Liyuan Liu, Jianfeng Gao","We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: this https URL.",SQuAD Dataset,75,0,,,
Question answering,"A Survey on Machine Reading Comprehension—Tasks, Evaluation Metrics and Benchmark Datasets","Changchang Zeng, Shaobo Li, Qin Li, Jie Hu, Jianjun Hu","Machine Reading Comprehension (MRC) is a challenging Natural Language Processing (NLP) research field with wide real-world applications. The great progress of this field in recent years is mainly due to the emergence of large-scale datasets and deep learning. At present, a lot of MRC models have already surpassed human performance on various benchmark datasets despite the obvious giant gap between existing MRC models and genuine human-level reading comprehension. This shows the need for improving existing datasets, evaluation metrics, and models to move current MRC models toward “real” understanding. To address the current lack of comprehensive survey of existing MRC tasks, evaluation metrics, and datasets, herein, (1) we analyze 57 MRC tasks and datasets and propose a more precise classification method of MRC tasks with 4 different attributes; (2) we summarized 9 evaluation metrics of MRC tasks, 7 attributes and 10 characteristics of MRC datasets; (3) We also discuss key open issues in MRC research and highlighted future research directions. In addition, we have collected, organized, and published our data on the companion website where MRC researchers could directly access each MRC dataset, papers, baseline projects, and the leaderboard. View Full-Text
",SQuAD Dataset,75,0,,,
Question answering,A benchmark study of machine learning models for online fake news detection,"Junaed Younus Khana, Md. Tawkat IslamKhondaker, Sadia Afroz, Gias Uddin, Anindya Iqbal","The proliferation of fake news and its propagation on social media has become a major concern due to its ability to create devastating impacts. Different machine learning approaches have been suggested to detect fake news. However, most of those focused on a specific type of news (such as political) which leads us to the question of dataset-bias of the models used. In this research, we conducted a benchmark study to assess the performance of different applicable machine learning approaches on three different datasets where we accumulated the largest and most diversified one. We explored a number of advanced pre-trained language models for fake news detection along with the traditional and deep learning ones and compared their performances from different aspects for the first time to the best of our knowledge. We find that BERT and similar pre-trained models perform the best for fake news detection, especially with very small dataset. Hence, these models are significantly better option for languages with limited electronic contents, i.e., training data. We also carried out several analysis based on the models’ performance, article’s topic, article’s length, and discussed different lessons learned from them. We believe that this benchmark study will help the research community to explore further and news sites/blogs to select the most appropriate fake news detection method.",SQuAD Dataset,75,0,,,
Question answering,Question Answering over Linked Data (QALD-4),"Christina Unger, Corina Forascu, Vanessa Lopez, Axel-Cyrille Ngonga Ngomo, Elena Cabrio, Philipp Cimiano, Sebastian Walter","With the increasing amount of semantic data available on the web there is a strong need for systems that allow common web users to access this body of knowledge. Especially question answering systems have received wide attention, as they allow users to express arbitrarily complex information needs in an easy and intuitive fashion (for an overview see [4]). The key challenge lies in translating the users' information needs into a form such that they can be evaluated using standard Semantic Web query processing and inferencing techniques. Over the past years, a range of approaches have been developed to address this challenge, showing signicant advances towards answering natural language questions with respect to large, heterogeneous sets of structured data. However, only few systems yet address the fact that the structured data available nowadays is distributed among a large collection of interconnected datasets, and that answers to questions can often only be provided if information from several sources are combined. In addition, a lot of information is still available only in textual form, both on the web and in the form of labels and abstracts in linked data sources. Therefore approaches are needed that can not only deal with the specific character of structured data but also with finding information in several sources, processing both structured and unstructured information, and combining such gathered information into one answer. The main objective of the open challenge on question answering over linked data (QALD) is to provide up-to-date, demanding benchmarks that establish a standard against which question answering systems over structured data can be evaluated and compared. QALD-4 is the fourth instalment of the QALD open challenge, comprising three tasks: multilingual question answering, biomedical question answering over interlinked data, and hybrid question answering.",Question Answering over Linked Data (QALD-4),76,1,Question Answering over Linked Data (QALD-4),Christina Unger,"This dataset comprises 250 natural language questions over DBpedia 3.9, annotated with SPARQL queries and answers. It was part of the QALD-4 open challenge: questions 1-200 constitute the training question set and questions 201-250 constitute the test question set.

"
Question answering,More Accurate Question Answering on Freebase,"Hannah Bast, Elmar Haussmann","Real-world factoid or list questions often have a simple structure, yet are hard to match to facts in a given knowledge base due to high representational and linguistic variability. For example, to answer ""who is the ceo of apple"" on Freebase requires a match to an abstract ""leadership"" entity with three relations ""role"", ""organization"" and ""person"", and two other entities ""apple inc"" and ""managing director"". Recent years have seen a surge of research activity on learning-based solutions for this method. We further advance the state of the art by adopting learning-to-rank methodology and by fully addressing the inherent entity recognition problem, which was neglected in recent works. We evaluate our system, called Aqqu, on two standard benchmarks, Free917 and WebQuestions, improving the previous best result for each benchmark considerably. These two benchmarks exhibit quite different challenges, and many of the existing approaches were evaluated (and work well) only for one of them. We also consider efficiency aspects and take care that all questions can be answered interactively (that is, within a second). Materials for full reproducibility are available on our website: http://ad.informatik.uni-freiburg.de/publications.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,MLQA: Evaluating Cross-lingual Extractive Question Answering,"Patrick Lewis, Barlas Oğuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk","Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Core techniques of question answering systems over knowledge bases: a survey,"Dennis Diefenbach, Vanessa Lopez, Kamal Singh, Pierre Maret ","The Semantic Web contains an enormous amount of information in the form of knowledge bases (KB). To make this information available, many question answering (QA) systems over KBs were created in the last years. Building a QA system over KBs is difficult because there are many different challenges to be solved. In order to address these challenges, QA systems generally combine techniques from natural language processing, information retrieval, machine learning and Semantic Web. The aim of this survey is to give an overview of the techniques used in current QA systems over KBs. We present the techniques used by the QA systems which were evaluated on a popular series of benchmarks: Question Answering over Linked Data. Techniques that solve the same task are first grouped together and then described. The advantages and disadvantages are discussed for each technique. This allows a direct comparison of similar techniques. Additionally, we point to techniques that are used over WebQuestions and SimpleQuestions, which are two other popular benchmarks for QA systems.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Survey on challenges of Question Answering in the Semantic Web,"Konrad Höffner, Sebastian Walter, Edgard Marx, Ricardo Usbeck, Jens Lehmann, Axel-Cyrille Ngonga Ngomo","Semantic Question Answering (SQA) removes two major access requirements to the Semantic Web: the mastery of a formal query language like SPARQL and knowledge of a specific vocabulary. Because of the complexity of natural language, SQA presents difficult challenges and many research opportunities. Instead of a shared effort, however, many essential components are redeveloped, which is an inefficient use of researcher’s time and resources. This survey analyzes 62 different SQA systems, which are systematically and manually selected using predefined inclusion and exclusion criteria, leading to 72 selected publications out of 1960 candidates. We identify common challenges, structure solutions, and provide recommendations for future systems. This work is based on publications from the end of 2010 to July 2015 and is also compared to older but similar surveys.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Automated Template Generation for Question Answering over Knowledge Graphs,"Abdalghani Abujabal, Mohamed Yahya, Mirek Riedewald, Gerhard Weikum","Templates are an important asset for question answering over knowledge graphs, simplifying the semantic parsing of input utterances and generating structured queries for interpretable answers. State-of-the-art methods rely on hand-crafted templates with limited coverage. This paper presents QUINT, a system that automatically learns utterance-query templates solely from user questions paired with their answers. Additionally, QUINT is able to harness language compositionality for answering complex questions without having any templates for the entire question. Experiments with different benchmarks demonstrate the high quality of QUINT.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Semantic Search on Text and Knowledge Bases,"Hannah Bast, Björn Buchhold, Elmar Haussmann","This article provides a comprehensive overview of the broad area of semantic search on text and knowledge bases. In a nutshell, semantic search is “search with meaning”. This “meaning” can refer to various parts of the search process: understanding the query (instead of just finding matches of its components in the data), understanding the data (instead of just searching it for such matches), or representing knowledge in a way suitable for meaningful retrieval. Semantic search is studied in a variety of different communities with a variety of different views of the problem. In this survey, we classify this work according to two dimensions: the type of data (text, knowledge bases, combinations of these) and the kind of search (keyword, structured, natural language). We consider all nine combinations. The focus is on fundamental techniques, concrete systems, and benchmarks. The survey also considers advanced issues: ranking, indexing, ontology matching and merging, and inference. It also provides a succinct overview of fundamental natural language processing techniques: POS-tagging, named-entity recognition and disambiguation, sentence parsing, and distributional semantics. The survey is as self-contained as possible, and should thus also serve as a good tutorial for newcomers to this fascinating and highly topical field.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,AskNow: A Framework for Natural Language Query Formalization in SPARQL,"Mohnish Dubey, Sourish Dasgupta, Ankit Sharma, Konrad Höffner, Jens Lehmann ","Natural Language Query Formalization involves semantically parsing queries in natural language and translating them into their corresponding formal representations. It is a key component for developing question-answering (QA) systems on RDF data. The chosen formal representation language in this case is often SPARQL. In this paper, we propose a framework, called AskNow, where users can pose queries in English to a target RDF knowledge base (e.g. DBpedia), which are first normalized into an intermediary canonical syntactic form, called Normalized Query Structure (NQS), and then translated into SPARQL queries. NQS facilitates the identification of the desire (or expected output information) and the user-provided input information, and establishing their mutual semantic relationship. At the same time, it is sufficiently adaptive to query paraphrasing. We have empirically evaluated the framework with respect to the syntactic robustness of NQS and semantic accuracy of the SPARQL translator on standard benchmark datasets.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,"Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks","Xiao Lin, Devi Parikh","Artificial agents today can answer factual questions. But they fall short on questions that require common sense reasoning. Perhaps this is because most existing common sense databases rely on text to learn and represent knowledge. But much of common sense knowledge is unwritten - partly because it tends to not be interesting enough to talk about, and partly because some common sense is unnatural to articulate in text. While unwritten, it is not unseen. In this paper we leverage semantic common sense knowledge learned from images - i.e. visual common sense - in two textual tasks: fill-in-the-blank and visual paraphrasing. We propose to ""imagine"" the scene behind the text, and leverage visual cues from the ""imagined"" scenes in addition to textual cues while answering these questions. We imagine the scenes as a visual abstraction. Our approach outperforms a strong text-only baseline on these tasks. Our proposed tasks can serve as benchmarks to quantitatively evaluate progress in solving tasks that go ""beyond recognition"". Our code and datasets are publicly available.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Towards a question answering system over the Semantic Web,"Dennis Diefenbach, Andreas Both, Kamal Singh, Pierre Maret","With the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of question answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provide a conceptional solution for multilingual, KB-agnostic question answering over the Semantic Web. The provided first approximation validates this concept.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,Question answering over knowledge graphs: question understanding via template decomposition,"Weiguo Zheng, Jeffrey Xu Yu, Lei Zou, Hong Cheng","The gap between unstructured natural language and structured data makes it challenging to build a system that supports using natural language to query large knowledge graphs. Many existing methods construct a structured query for the input question based on a syntactic parser. Once the input question is parsed incorrectly, a false structured query will be generated, which may result in false or incomplete answers. The problem gets worse especially for complex questions. In this paper, we propose a novel systematic method to understand natural language questions by using a large number of binary templates rather than semantic parsers. As sufficient templates are critical in the procedure, we present a low-cost approach that can build a huge number of templates automatically. To reduce the search space, we carefully devise an index to facilitate the online template decomposition. Moreover, we design effective strategies to perform the two-level disambiguations (i.e., entity-level ambiguity and structure-level ambiguity) by considering the query semantics. Extensive experiments over several benchmarks demonstrate that our proposed approach is effective as it significantly outperforms state-of-the-art methods in terms of both precision and recall.",Question Answering over Linked Data (QALD-4),76,0,,,
Question answering,VQA: Visual Question Answering,"Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh ","We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).",Visual Question Answering Dataset,77,1,Visual Question Answering Dataset,"Yash Goyal,Tejas Khot,Douglas Summers-Stay, Dhruv Batra, Devi Parikh
","Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset.

265,016 images (COCO and abstract scenes) At least 3 questions (5.4 questions on average) per image 10 ground truth answers per question 3 plausible (but likely incorrect) answers per question Automatic evaluation metric

The first version of the dataset was released in October 2015."
Question answering,Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization,"Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra","We propose a technique for producing 'visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, and needs no architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions.",Visual Question Answering Dataset,77,0,,,
Question answering,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang","Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively.",Visual Question Answering Dataset,77,0,,,
Question answering,Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,"Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Li Fei-Fei ","Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that “the person is riding a horse-drawn carriage.” In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.",Visual Question Answering Dataset,77,0,,,
Question answering,Cascade R-CNN: Delving Into High Quality Object Detection,"Zhaowei Cai, Nuno Vasconcelos","In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength.",Visual Question Answering Dataset,77,0,,,
Question answering,ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases,"Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ""ChestX-ray8"", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based ""reading chest X-rays"" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",Visual Question Answering Dataset,77,0,,,
Question answering,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,"Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes","Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",Visual Question Answering Dataset,77,0,,,
Question answering,Bidirectional Attention Flow for Machine Comprehension,"Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi","Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",Visual Question Answering Dataset,77,0,,,
Question answering,Salient Object Detection: A Benchmark,"Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li","We extensively compare, qualitatively and quantitatively, 41 state-of-the-art models (29 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over seven challenging data sets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted three years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for the state-of-the-art models, provide useful hints toward constructing more challenging large-scale data sets and better saliency models. Finally, we propose probable solutions for tackling several open problems, such as evaluation scores and data set bias, which also suggest future research directions in the rapidly growing field of salient object detection.",Visual Question Answering Dataset,77,0,,,
Question answering,Stacked Attention Networks for Image Question Answering,"Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola","This paper presents stacked attention networks (SANs)that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",Visual Question Answering Dataset,77,0,,,
Question answering,Objects as Points,"Xingyi Zhou, Dequan Wang, Philipp Krähenbühl","Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.",Visual Question Answering Dataset,77,0,,,
Question answering,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering","Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning","Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,1," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering","Yoshua Bengio, Zhilin Yang, Peng Qi, Saizheng Zhang, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning","HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems built based on Wikipedia.
"
Question answering,Longformer: The Long-Document Transformer,"Iz Beltagy, Matthew E. Peters, Arman Cohan","Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,SpanBERT: Improving Pre-training by Representing and Predicting Spans,"Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy","We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1"," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Natural Questions: A Benchmark for Question Answering Research,"Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov","We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Big Bird: Transformers for Longer Sequences,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed","Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,CTRL: A Conditional Transformer Language Model for Controllable Generation,"Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher","Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at this https URL."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Pre-trained models for natural language processing: A survey,"XiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao, Ning Dai & XuanJing Huang","Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,"Hu Xu, Bing Liu, Lei Shu, Philip S. Yu","Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.~We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective. The datasets and code are available at this https URL."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Self-supervised Learning: Generative or Contrastive,"Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, Jie Tang","Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analyses on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Adversarial NLI: A New Benchmark for Natural Language Understanding,"Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela","We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,"Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova","In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work."," HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",78,0,,,
Question answering,Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph,"Amrita Saha, Vardaan Pahuja, Mitesh Khapra,Karthik Sankaranarayanan, Sarath Chandar","While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.",Complex Sequential Question Answering dataset,79,1,Complex Sequential Question Answering dataset,"Amrita Saha, Vardaan Pahuja, Mitesh Khapra,Karthik Sankaranarayanan, Sarath Chandar","While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.
"
Question answering,CoQA: A Conversational Question Answering Challenge,"Siva Reddy, Danqi Chen, Christopher D. Manning","Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at this http URL.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Neural Approaches to Conversational AI,"Jianfeng Gao, Michel Galley, Lihong Li","The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,QuAC : Question Answering in Context,"Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer","We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at this http URL.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Survey on evaluation methods for dialogue systems,"Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, Mark Cieliebak ","In this paper, we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation, in and of itself, is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost- and time-intensive. Thus, much work has been put into finding methods which allow a reduction in involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented, conversational, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then present the evaluation methods regarding that class.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Interpretation of Natural Language Rules in Conversational Machine Reading,"Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel","Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer ""Can I...?"" or ""Do I have to...?"" questions such as ""I am working in Canada. Do I have to carry on paying UK National Insurance?"" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as ""How long have you been working abroad?"" when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,"Hsin-Yuan Huang, Eunsol Choi, Wen-tau Yih","Conversational machine comprehension requires the understanding of the conversation history, such as previous question/answer pairs, the document context, and the current question. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base,"Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin","We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Few-Shot NLG with Pre-Trained Language Model,"Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang","Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of \textit{few-shot natural language generation}. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at \url{this https URL}",Complex Sequential Question Answering dataset,79,0,,,
Question answering,"A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",Mark Yatskar,"We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers. We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third. Because of the datasets' structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA. Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining. To encourage cross-evaluation, we release code for conversion between datasets at this https URL .",Complex Sequential Question Answering dataset,79,0,,,
Question answering,Towards a Better Metric for Evaluating Question Generation Systems,"Preksha Nema, Mitesh M. Khapra","There has always been criticism for using n-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on n-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available at this https URL",Complex Sequential Question Answering dataset,79,0,,,
Question answering,"Deep learning based question answering system in Bengali
","Tasmiah Tahsin Mayeesha, Abdullah Md Sarwar, Rashedur M. Rahman","Recent advances in the field of natural language processing has improved state-of-the-art performances on many tasks including question answering for languages like English. Bengali language is ranked seventh and is spoken by about 300 million people all over the world. But due to lack of data and active research on QA similar progress has not been achieved for Bengali. Unlike English, there is no benchmark large scale QA dataset collected for Bengali, no pretrained language model that can be modified for Bengali question answering and no human baseline score for QA has been established either. In this work we use state-of-the-art transformer models to train QA system on a synthetic reading comprehension dataset translated from one of the most popular benchmark datasets in English called SQuAD 2.0. We collect a smaller human annotated QA dataset from Bengali Wikipedia with popular topics from Bangladeshi culture for evaluating our models. Finally, we compare our models with human children to set up a benchmark score using survey experiments.",Deep learning based question answering system in Bengali,80,1,Deep learning based question answering system in Bengali,"Tasmiah Tahsin Mayeesha, Abdullah Md Sarwar, Rashedur M. Rahman","This dataset contains the data for the paper ""Deep learning based question answering system in Bengali"". It is a translated version of SQuAD 2.0 dataset to bengali language. Preprocessing details can be found in the paper.
"
Question answering,"AraConv: Developing an Arabic Task-Oriented Dialogue System Using Multi-Lingual Transformer Model mT5","Ahlam Fuad, Maha Al-Yahya
","Task-oriented dialogue systems (DS) are designed to help users perform daily activities using natural language. Task-oriented DS for English language have demonstrated promising performance outcomes; however, developing such systems to support Arabic remains a challenge. This challenge is mainly due to the lack of Arabic dialogue datasets. This study introduces the first Arabic end-to-end generative model for task-oriented DS (AraConv), which uses the multi-lingual transformer model mT5 with different settings. We also present an Arabic dialogue dataset (Arabic-TOD) and used it to train and test the proposed AraConv model. The results obtained are reasonable compared to those reported in the studies of English and Chinese using the same mono-lingual settings. To avoid problems associated with a small training dataset and to improve the AraConv model’s results, we suggest joint-training, in which the model is jointly trained on Arabic dialogue data and data from one or two high-resource languages such as English and Chinese. The findings indicate the AraConv model performed better in the joint-training setting than in the mono-lingual setting. The results obtained from AraConv on the Arabic dialogue dataset provide a baseline for other researchers to build robust end-to-end Arabic task-oriented DS that can engage with complex scenarios. ",Deep learning based question answering system in Bengali,80,0,,,
Question answering,Survey: using BERT model for Arabic Question Answering System.,Abdullah Farhan Mahdi,"In this paper, we deal with the community question answer problem. Using Burt's algorithm, the Question Answer Task (QA) is a Natural Domain Language Processing (NLP). We present a survey on language representation learning for the purpose of consolidating a set of common lessons learned across a variety of recent efforts. Which enables machine reading comprehension and natural language inference tasks. BERT controls its simplicity of use also is a light refinement method without substantial task-specific modifications. We highlight important considerations when interpreting recent contributions and choosing which model to use. We will address the strengths and weaknesses of the algorithm and what are the challenges that faced researchers.",Deep learning based question answering system in Bengali,80,0,,,
Question answering,Reading Comprehension Based Question Answering System in Bangla Language with Transformer-Based Learning,"Tanjim Taharat Aurpa,Richita Khandakar Rifat, Md Shoaib Ahmed, Md Musfique Anwar, A. B. M. Shawkat Ali ","Question answering (QA) system in any language is an assortment of mechanisms for obtaining answers to user questions with various data compositions. Reading comprehension (RC) is one type of composition, and the popularity of this type is increasing day by day in Natural Language Processing (NLP) research area. Some works have been done in several languages, mainly in English. In the Bangla language, neither any dataset available for RC nor any work has been done in the past. In this research work, we develop a question-answering system from RC. For doing this, we construct a dataset containing 3636 reading comprehensions along with questions and answers. We apply a transformer-based deep neural network model to obtain convenient answers to questions based on reading comprehensions precisely and swiftly. We exploit some deep neural network architectures such as LSTM (Long Short-Term Memory), Bi-LSTM (Bidirectional LSTM) with attention, RNN (Recurrent Neural Network), ELECTRA, and BERT (Bidirectional Encoder Representations from Transformers) to our dataset for training. The transformer-based pre-training language architectures BERT and ELECTRA perform more prominently than others from those architectures. Finally, the trained model of BERT performs a satisfactory outcome with 87.78% of testing accuracy and 99% training accuracy, and ELECTRA provides training and testing accuracy of 82.5% and 93%, respectively.
",Deep learning based question answering system in Bengali,80,0,,,
Question answering,Learning to Map Natural Language to Executable Programs over Databases,Tao Yu,"Natural language is a fundamental form of information and communication and is becoming the next frontier in computer interfaces. As the amount of data available online has increased exponentially, so has the need for Natural Language Interfaces (NLIs, which is not used for natural language inference in this thesis) to connect the data and the user by easily using natural language, significantly promoting the possibility and efficiency of information access for many users besides data experts. All consumer-facing software will one day have a dialogue interface, and this is the next vital leap in the evolution of search engines. Such intelligent dialogue systems should understand the meaning of language grounded in various contexts and generate effective language responses in different forms for information requests and human-computer communication.

Developing these intelligent systems is challenging due to (1) limited benchmarks to drive advancements, (2) alignment mismatches between natural language and formal programs, (3) lack of trustworthiness and interpretability, (4) context dependencies in both human conversational interactions and the target programs, and (5) joint language understanding between dialog questions and NLI environments (e.g. databases and knowledge graphs). This dissertation presents several datasets, neural algorithms, and language models to address these challenges for developing deep learning technologies for conversational natural language interfaces (more specifically, NLIs to Databases or NLIDB).

First, to drive advancements towards neural-based conversational NLIs, we design and propose several complex and cross-domain NLI benchmarks, along with introducing several datasets. These datasets enable training large, deep learning models. The evaluation is done on unseen databases. (e.g., about course arrangement). Systems must generalize well to not only new SQL queries but also to unseen database schemas to perform well on these tasks. Furthermore, in real-world applications, users often access information in a multi-turn interaction with the system by asking a sequence of related questions. The users may explicitly refer to or omit previously mentioned entities and constraints and may introduce refinements, additions, or substitutions to what has already been said. Therefore, some of them require systems to model dialog dynamics and generate natural language explanations for user verification. The full dialogue interaction with the system’s responses is also important as this supports clarifying ambiguous questions, verifying returned results, and notifying users of unanswerable or unrelated questions. A robust dialogue-based NLI system that can engage with users by forming its responses has thus become an increasingly necessary component for the query process.

Moreover, this thesis presents the development of scalable algorithms designed to parse complex and sequential questions to formal programs (e.g., mapping questions to SQL queries that can execute against databases). We propose a novel neural model that utilizes type information from knowledge graphs to better understand rare entities and numbers in natural language questions. We also introduce a neural model based on syntax tree neural networks, which was the first methodology proposed for generating complex programs from language.

Finally, language modeling creates contextualized vector representations of words by training a model to predict the next word given context words, which are the basis of deep learning for NLP. Recently, pre-trained language models such as BERT and RoBERTa achieve tremendous success in many natural language processing tasks such as text understanding and reading comprehension. However, most language models are pre-trained only on free-text such as Wikipedia articles and Books. Given that language in semantic parsing is usually related to some formal representations such as logic forms and SQL queries and has to be grounded in structural environments (e.g., databases), we propose better language models for NLIs by enforcing such compositional interpolation in them. To show they could better jointly understand dialog questions and NLI environments (e.g. databases and knowledge graphs), we show that these language models achieve new state-of-the-art results for seven representative tasks on semantic parsing, dialogue state tracking, and question answering. Also, our proposed pre-training method is much more effective than other prior work.",CoQA Conversational Question Answering Dataset ,81,1,CoQA Conversational Question Answering Dataset ,Jérôme E. Blanch,"Now that I have your attention, please up-vote this dataset and read the following!!!
What is CoQA?
With extensive research & reading on Natural Language Processing, I realized the CoQA dataset is quite unknown according to GitHub. It is a pleasure for me to upload the dataset here on the Kaggle platform and also provide a useful kernel properly dealing with JSON files.

CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. CoQA is pronounced as coca .

CoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.

2019 Papers"
Question answering,"PQuAD: A Persian Question Answering Dataset","Kasra Darvishi, Newsha Shahbodagh, Zahra Abbasiantaeb, Saeedeh Momtazi","We present Persian Question Answering Dataset (PQuAD), a crowdsourced reading comprehension dataset on Persian Wikipedia articles. It includes 80,000 questions along with their answers, with 25% of the questions being adversarially unanswerable. We examine various properties of the dataset to show the diversity and the level of its difficulty as an MRC benchmark. By releasing this dataset, we aim to ease research on Persian reading comprehension and development of Persian question answering systems. Our experiments on different state-of-the-art pre-trained contextualized language models show 74.8% Exact Match (EM) and 87.6% F1-score that can be used as the baseline results for further research on Persian QA.","PersianQA Dataset
",82,1,"PersianQA Dataset
","Sajjad Ayoubi","PersianQA: a dataset for Persian Question Answering Persian Question Answering (PersianQA) Dataset is a reading comprehension dataset on Persian Wikipedia. The crowd-sourced the dataset consists of more than 9,000 entries. Each entry can be either an impossible-to-answer or a question with one or more answers spanning in the passage (the context) from which the questioner proposed the question. Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be utilized to create a system which ""knows that it doesn't know the answer"".

Moreover, the dataset has 900 test data available. On top of that, the very first models trained on the dataset, Transformers, are available online.

All the crowd workers of the dataset are native Persian speakers. Also, it worth mentioning that the contexts are collected from all categories of the Wiki (Historical, Religious, Geography, Science, etc)."
Question answering,"Question Answering (QA) has enticed the interest of NLP community in recent years. NLP enthusiasts are engineering new Models and
fine-tuning the existing ones that can give out answers for the posed questions. The deep neural network models are found to perform
exceptionally on QA tasks, but these models are also data intensive. For instance, BERT has outperformed many of its contemporary
contenders on SQuAD dataset. In this work, we attempt at solving the closed domain reading comprehension Question Answering task
on QRCD (Qur’anic Reading Comprehension Dataset) to extract an answer span from the provided passage, using BERT as a baseline
model. We improved the model’s output by applying regularization techniques like weight-decay and data augmentation. Using different
strategies we had 59% and 31% partial Reciprocal Ranking (pRR) on development and testing data splits respectively.","Esha Aftab, Muhammad Kamran Malik","eRock at Qur’an QA 2022: Contemporary Deep Neural Networks for Qur’an
based Reading Comprehension Question Answers","PersianQA Dataset
",82,0,,,
Question answering,The Catalan Language CLUB,"Carlos Rodriguez-Penagos, Carme Armentano-Oller, Marta Villegas, Maite Melero, Aitor Gonzalez, Ona de Gibert Bonet, Casimiro Carrino Pio
","The Catalan Language Understanding Benchmark (CLUB) encompasses various datasets representative of different NLU tasks that enable accurate evaluations of language models, following the General Language Understanding Evaluation (GLUE) example. It is part of AINA and PlanTL, two public funding initiatives to empower the Catalan language in the Artificial Intelligence era.",VilaQuAD: an extractive QA dataset from Catalan newswire,83,1,VilaQuAD: an extractive QA dataset from Catalan newswire,"Carlos Gerardo Rodriguez-Penagos, Carme Armentano-Oller
","If you use this resource in your work, please cite our latest paper: @inproceedings{armengol-estape-etal-2021-multilingual, title = ""Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan"", author = ""Armengol-Estap{\'e}, Jordi and Carrino, Casimiro Pio and Rodriguez-Penagos, Carlos and de Gibert Bonet, Ona and Armentano-Oller, Carme and Gonzalez-Agirre, Aitor and Melero, Maite and Villegas, Marta"", booktitle = ""Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"", month = aug, year = ""2021"", address = ""Online"", publisher = ""Association for Computational Linguistics"", url = ""https://aclanthology.org/2021.findings-acl.437"", doi = ""10.18653/v1/2021.findings-acl.437"", pages = ""4933--4946"", } Dataset de QA extractiu amb 6282 parells de pregunta-resposta desenvolupats a partir de paràgrafs del diari en línia Vilaweb (https://www.vilaweb.cat) usats sota llicència CC-BY-NC-ND 4.0. This dataset contains 2095 of Catalan language news articles along with 1 to 5 questions referring to each fragment (or context). VilaQuad articles are extracted from the daily Vilaweb (www.vilaweb.cat) and used under CC-by-nc-sa-nd (https://creativecommons.org/licenses/by-nc-nd/3.0/deed.ca) licence. This dataset can be used to build extractive-QA and Language Models. Funded by the Generalitat de Catalunya, Departament de Polítiques Digitals i Administració Pública (AINA), MT4ALL and Plan de Impulso de las Tecnologías del Lenguaje (Plan TL).
"
Question answering,"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","Gorka Urbizu, Inaki San Vicente , Xabier Saralegi,Rodrigo Agerri, Aitor Soroa","Natural Language Understanding (NLU) technology has improved significantly over the last few years and multitask benchmarks such as GLUE are key to evaluate this improvement in a robust and general way. These benchmarks take into account
a wide and diverse set of NLU tasks that require some form of language understanding, beyond the detection of superficial,
textual clues. However, they are costly to develop and language-dependent, and therefore they are only available for a small
number of languages. In this paper, we present BasqueGLUE, the first NLU benchmark for Basque, a less-resourced language,
which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of
GLUE and SuperGLUE. We also report the evaluation of two state-of-the-art language models for Basque on BasqueGLUE,
thus providing a strong baseline to compare upon. BasqueGLUE is freely available under an open license.",VilaQuAD: an extractive QA dataset from Catalan newswire,83,0,,,
Question answering,Multilingual Question Answering over Linked Data (QALD-3): Lab Overview,"Philipp Cimiano, Vanessa Lopez, Christina Unger, Elena Cabrio, Axel-Cyrille Ngonga Ngomo & Sebastian Walter ","The third edition of the open challenge on Question Answering over Linked Data (QALD-3) has been conducted as a half-day lab at CLEF 2013. Differently from previous editions of the challenge, has put a strong emphasis on multilinguality, offering two tasks: one on multilingual question answering and one on ontology lexicalization. While no submissions were received for the latter, the former attracted six teams who submitted their systems’ results on the provided datasets. This paper provides an overview of QALD-3, discussing the approaches proposed by the participating systems as well as the obtained results.
",Multilingual Question Answering over Linked Data: QALD-3 Dataset,84,1,Multilingual Question Answering over Linked Data: QALD-3 Dataset,Christina Unger,"This dataset comprises 200 natural language questions over DBpedia 3.8, annotated with SPARQL queries and answers. It was part of the QALD-3 open challenge: questions 1-100 constitute the training question set and questions 101-200 constitute the test question set.
"
Question answering,"Knowledge graph refinement: A survey of approaches and evaluation methods
",Heiko Paulheim,"In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google
coined the term “Knowledge Graph” in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO,
and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such
as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale
knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility
of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to
the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement
approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.",Multilingual Question Answering over Linked Data: QALD-3 Dataset,84,0,,,
Question answering,"Natural language question answering over RDF: a graph data driven approach
","Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey Xu Yu, Wenqiang He, Dongyan Zhao

","RDF question/answering (Q/A) allows users to ask questions in natural languages over a knowledge base represented by RDF. To answer a national language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over RDF repository (RDF Q/A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, RDF Q/A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. We compare our method with some state-of-the-art RDF Q/A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly.

",Multilingual Question Answering over Linked Data: QALD-3 Dataset,84,0,,,
Question answering,DrugEHRQA: A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,"Jayetri Bardhan, Anthony Colas, Kirk Roberts, Daisy Zhe Wang","This paper develops the first question answering dataset (DrugEHRQA) containing question-answer pairs from both structured tables and unstructured notes from a publicly available Electronic Health Record (EHR). EHRs contain patient records, stored in structured tables and unstructured clinical notes. The information in structured and unstructured EHRs is not strictly disjoint: information may be duplicated, contradictory, or provide additional context between these sources. Our dataset has medication-related queries, containing over 70,000 question-answer pairs. To provide a baseline model and help analyze the dataset, we have used a simple model (MultimodalEHRQA) which uses the predictions of a modality selection network to choose between EHR tables and clinical notes to answer the questions. This is used to direct the questions to the table-based or text-based state-of-the-art QA model. In order to address the problem arising from complex, nested queries, this is the first time Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers (RAT-SQL) has been used to test the structure of query templates in EHR data. Our goal is to provide a benchmark dataset for multi-modal QA systems, and to open up new avenues of research in improving question answering over EHR structured data by using context from unstructured clinical data.",A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,85,1,A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,"Jayetri Bardhan, Anthony Colas, Kirk Roberts, Daisy Zhe Wang","Electronic Health Records (EHR) contain patient records, stored in structured tables as well as unstructured clinical notes. The information in structured and unstructured EHR records is not strictly disjoint: information may be duplicated, contradictory, or provide additional context between these sources. This presents a rich opportunity to study question answering (QA) models that combine reasoning over both structured and unstructured data. This work presents the first question answering (QA) dataset (DrugEHRQA) containing question-answer pairs from both structured tables and unstructured notes from MIMIC-III, a publicly available Electronic Health Record (EHR). We are releasing a QA dataset over MIMIC-III tables through PhysioNet, containing 41,417 triplets of natural language questions, its corresponding SQL query and the answer retrieved from MIMIC-III tables. We also generated a QA dataset on the unstructured clinical notes of MIMIC-III which can be found in the n2c2 repository. Both these datasets are combined to generate a multimodal QA dataset (DrugEHRQA), which contains question-answers from both structured and unstructured data of MIMIC-III. The DrugEHRQA dataset has medication-related queries, containing over 70,000 question-answer pairs. Our goal is to provide a benchmark dataset for multi-modal QA systems, and to open up new avenues of research in improving question answering over EHR structured data by using context from unstructured clinical data."
Question answering,EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records,"Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon,Woncheol Shin, Seongjun Yang, Minjoon Seo, Jongyeup Kim, Edward Choi","We present a new text-to-SQL dataset for electronic health records (EHRs), where
the utterances are collected from 222 hospital staff—including physicians, nurses,
and insurance review and health records teams through a poll conducted at a
university hospital. Along with the utterances covering the actual needs of hospital
staff, we manually linked them to two open-source EHR databases, MIMIC-III
and eICU, to construct a healthcare QA dataset. Our dataset poses three unique
challenges: understanding the meaning and generating queries for the questions
that 1) reflect the actual needs of different professionals in the hospital and 2)
contain various types of time expressions, while 3) assessing the trustworthiness of
the QA model for system reliability. We believe that our dataset could serve as a
benchmark to develop semantic parsing models in the hospital workplace and take
a further step towards bridging the gap between academic and industrial settings in
the healthcare domain.",A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,85,0,,,
Question answering,"Sanitizing Synthetic Training Data Generation for Question Answering over Knowledge Graphs
","Trond Linjordet, Krisztian Balog ","Synthetic data generation is important to training and evaluating neural models for question answering over knowledge graphs. The quality of the data and the partitioning of the datasets into training, validation and test splits impact the performance of the models trained on this data. If the synthetic data generation depends on templates, as is the predominant approach for this task, there may be a leakage of information via a shared basis of templates across data splits if the partitioning is not performed hygienically. This paper investigates the extent of such information leakage across data splits, and the ability of trained models to generalize to test data when the leakage is controlled. We find that information leakage indeed occurs and that it affects performance. At the same time, the trained models do generalize to test data under the sanitized partitioning presented here. Importantly, these findings extend beyond the particular flavor of question answering task we studied and raise a series of difficult questions around template-based synthetic data generation that will necessitate additional research
","DBpedia Neural Question Answering (DBNQA) dataset
",86,1,"DBpedia Neural Question Answering (DBNQA) dataset
","Edgard Marx
","This resource contains a large and comprehensive dataset of 894,499 pairs of questions and SPARQL queries, materialized using the NSpMT.
"
Question answering,"Towards Formally Grounded Evaluation Measures for
Semantic Parsing-based Knowledge Graph Question Answering","Trond Linjordet, Krisztian Balog, Vinay Setty","Knowledge graph question answering (KGQA) is important to make
structured information accessible without formal query language
expertise on the part of the users. The semantic parsing (SP) avor
of this task maps a natural language question to a formal query
that is machine executable, such as SPARQL. The SP-KGQA task is
currently evaluated by adopting measures from other tasks, such
as information retrieval and machine translation. However, this
adoption typically occurs without fully considering the desired
behavior of SP-KGQA systems. To address this, we articulate taskspecic desiderata, then develop novel SP-KGQA measures based on
a probabilistic framework. We use the desiderata to formulate a set
of axioms for SP-KGQA measures and conduct an axiomatic analysis
that reveals insuciencies of established measures previously used
to report SP-KGQA performance. We also perform experimental
evaluations, using synthetic and state-of-the-art neural machine
translation approaches. The results highlight the importance of
grounded alternative SP-KGQA measures.","DBpedia Neural Question Answering (DBNQA) dataset
",86,0,,,
Question answering,How Train–Test Leakage Affects Zero-shot Retrieval,"Maik Fröbe, Christopher Akiki,  Martin Potthast,  Matthias Hagen ","Neural retrieval models are often trained on (subsets of) the
millions of queries of the MS MARCO / ORCAS datasets and then tested
on the 250 Robust04 queries or other TREC benchmarks with often only
50 queries. In such setups, many of the few test queries can be very
similar to queries from the huge training data—in fact, 69% of the Robust04 queries have near-duplicates in MS MARCO / ORCAS. We investigate the impact of this unintended train–test leakage by training neural
retrieval models on combinations of a fixed number of MS MARCO / ORCAS queries that are highly similar to the actual test queries and an increasing number of other queries. We find that leakage can improve
effectiveness and even change the ranking of systems. However, these
effects diminish as the amount of leakage among all training instances
decreases and thus becomes more realistic.","DBpedia Neural Question Answering (DBNQA) dataset
",86,0,,,
Question answering,"A survey on non-factoid question answering systems","Manvi Breja, Sanjay Kumar Jain","Question Answering System (QAS) aims at providing the most appropriate answer to the user’s question asked in any natural language. It emerged as a future of web search that crawls structured and unstructured collections of documents to find answers. A lot of research has already been done on answering factoid questions and achieved promising accuracy, whereas non-factoid question answering is a challenging task and explores newer avenues for fact-finding and analysis. The research on handling different modules of non-factoid question answering has drastically increased from 2012 which evokes the need for summarizing and providing a comprehensive analysis of the work that has already been done in this field. The survey identifies different challenges faced in each module, how they have been addressed by the researchers with their advantages or disadvantages and further improvements. The survey is written with an objective to help the researchers understand the field with how it has been undertaken by different approaches with their impact on performance. The paper finally concludes by investigating research gaps and research objectives that are promising to get an insight into future research directions.",A survey on non-factoid question answering systems,87,1,A survey on non-factoid question answering systems,"Manvi Breja, Sanjay Kumar Jain","Question Answering System (QAS) aims at providing the most appropriate answer to the user’s question asked in any natural language. It emerged as a future of web search that crawls structured and unstructured collections of documents to find answers. A lot of research has already been done on answering factoid questions and achieved promising accuracy, whereas non-factoid question answering is a challenging task and explores newer avenues for fact-finding and analysis. The research on handling different modules of non-factoid question answering has drastically increased from 2012 which evokes the need for summarizing and providing a comprehensive analysis of the work that has already been done in this field. The survey identifies different challenges faced in each module, how they have been addressed by the researchers with their advantages or disadvantages and further improvements. The survey is written with an objective to help the researchers understand the field with how it has been undertaken by different approaches with their impact on performance. The paper finally concludes by investigating research gaps and research objectives that are promising to get an insight into future research directions.
"
Question answering,"Where Was COVID-19 First Discovered? Designing a Question-Answering System for Pandemic Situations","Johannes Graf, Gino Lancho, Patrick Zschech, Kai Heinrich","The COVID-19 pandemic is accompanied by a massive ""infodemic"" that makes it hard to identify concise and credible information for COVID-19-related questions, like incubation time, infection rates, or the effectiveness of vaccines. As a novel solution, our paper is concerned with designing a question-answering system based on modern technologies from natural language processing to overcome information overload and misinformation in pandemic situations. To carry out our research, we followed a design science research approach and applied Ingwersen's cognitive model of information retrieval interaction to inform our design process from a socio-technical lens. On this basis, we derived prescriptive design knowledge in terms of design requirements and design principles, which we translated into the construction of a prototypical instantiation. Our implementation is based on the comprehensive CORD-19 dataset, and we demonstrate our artifact's usefulness by evaluating its answer quality based on a sample of COVID-19 questions labeled by biomedical experts.",,87,0,,,
Question answering,"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer","We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL",TriviaQA,88,1,TriviaQA,"Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer","TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.
"
Question answering,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",TriviaQA,88,0,,,
Question answering,"Language Models are Few-Shot Learners
","Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

 ","We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. ",TriviaQA,88,0,,,
Question answering,"End-to-end Biomedical Question Answering via Bio-AnswerFinder and Discriminative Language
Representation Models",Ibrahim Burak Ozyurt,"Generative Transformers based language representation models such as BERT and its biomedical domain adapted version BioBERT have been shown to be highly effective for biomedical question answering. Here, discriminative, sample-efficient biomedical language representation models based on ELECTRA language representation model architecture were introduced to enhance an end-to-end biomedical
question answering system, Bio-AnswerFinder, for the BioASQ challenge. The introduced language
representation models outperformed other language models including BioBERT in answer span classification, answer candidate re-ranking and yes/no answer classification tasks. The resulting end-to-end
system participated in BioASQ Synergy and both phases of Task 9B with promising results.","Training/evaluation data sets and databases for the operation of bio-answerfinder biomedical question answering system
",89,1,"Training/evaluation data sets and databases for the operation of bio-answerfinder biomedical question answering system
","Ibrahim Burak Ozyurt","A zip file containing training/evaluation data sets for the bio-answerfinder biomedical question answering system training and evaluation. The zip file also contains SQLite databases for named entity lookups, morphology, nominalizations, acronyms, PubMED trained GLoVe word/phrase embeddings and vocabulary with document frequencies and SciCrunch ontology data for named entities such as proteins, anatomical structures,

"
Question answering,"Overview of BioASQ 2021: The Ninth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering
","Anastasios Nentidis, Georgios Katsimpras, Eirini Vandorou, Anastasia Krithara, Luis Gasco, Martin Krallinger, Georgios Paliouras ","Advancing the state-of-the-art in large-scale biomedical semantic indexing and question answering is the main focus of the BioASQ challenge. BioASQ organizes respective tasks where different teams develop systems that are evaluated on the same benchmark datasets that represent the real information needs of experts in the biomedical domain. This paper presents an overview of the ninth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2021. In this year, a new question answering task, named Synergy, is introduced to support researchers studying the COVID-19 disease and measure the ability of the participating teams to discern information while the problem is still developing. In total, 42 teams with more than 170 systems were registered to participate in the four tasks of the challenge. The evaluation results, similarly to previous years, show a performance gain against the baselines which indicates the continuous improvement of the state-of-the-art in this field.

","Training/evaluation data sets and databases for the operation of bio-answerfinder biomedical question answering system
",89,0,,,
Question answering,"Qasar: Self-Supervised Learning Framework for Extractive Question Answering","Haytham Assem, Rajdeep Sarkar,  Sourav Dutta","Question Answering (QA) has become a foundational research area in Natural Language Understanding (NLU) with widespread applications in search, personal digital assistance, and conversational systems. Despite the success in open-domain question answering, existing extractive question answering models pre-trained using Wikipedia articles (e.g., SQuAD data) perform rather poorly in closed-domain and industrial scenarios. Further, a major limitation in adapting question answering systems to such contexts is the poor availability and the expensive annotation of domain-specific data. Thus, wide applicability of QA models are severely hampered in enterprise systems.In this paper, we aim to address the above challenges by introducing a novel QA framework, Qasar, using self-supervised learning for efficient domain adaptation. We show, for the first time, the advantage of fine-tuning pre-trained QA models for closed-domains by synthetically generated domain-specific questions and answers (from relevant documents) from large language models like T5. Further, we also propose a novel context retrieval component based on question-context semantic relatedness to further boost the accuracy of the Qasar QA framework. Experimental results show significant performance improvements on both open-and closed-domain QA datasets, while requiring no labelling efforts, which we believe will contribute to the ease of deployment of such systems in enterprise settings. The different modules of our framework (synthetic data generation, context retrieval, and question answering) can be fully reproduced by fine-tuning publicly available language models and QA models on SQuAD dataset as discussed in the paper.
","Training/evaluation data sets and databases for the operation of bio-answerfinder biomedical question answering system
",89,0,,,
Question answering,"Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs","Tarcísio Souza Costa, Simon Gottschalk, Elena Demidova

 ","Semantic Question Answering (QA) is a crucial technology to facilitate intuitive user access to semantic information stored in knowledge graphs. Whereas most of the existing QA systems and datasets focus on entity-centric questions, very little is known about these systems' performance in the context of events. As new event-centric knowledge graphs emerge, datasets for such questions gain importance. In this paper, we present the Event-QA dataset for answering event-centric questions over knowledge graphs. Event-QA contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.
",A Dataset for Event-Centric Question Answering over Knowledge Graphs,90,1,A Dataset for Event-Centric Question Answering over Knowledge Graphs,"Tarcísio Souza Costa, Simon Gottschalk, Elena Demidova
","Event-QA dataset contains 1000 semantic queries and the corresponding verbalisations for EventKG - a recently proposed event-centric knowledge graph containing over 970 thousand events.

"
Question answering,"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","Anna Rogers, Matt Gardner, Isabelle Augenstein","Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been also much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with over 80 new datasets appearing in the past two years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of ``reasoning types"" in question answering and propose a new taxonomy. We also discuss the implications of over-focusing on English, and survey the current monolingual resources for other languages and multilingual resources. The study is aimed at both practitioners looking for pointers to the wealth of existing data, and at researchers working on new resources.",A Dataset for Event-Centric Question Answering over Knowledge Graphs,90,0,,,
Question answering,"Complex Temporal Question Answering on Knowledge Graphs","Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, Gerhard Weikum

","Question answering over knowledge graphs (KG-QA) is a vital topic in IR. Questions with temporal intent are a special class of practical importance, but have not received much attention in research. This work presents EXAQT, the first end-to-end system for answering complex temporal questions that have multiple entities and predicates, and associated temporal conditions. EXAQT answers natural language questions over KGs in two stages, one geared towards high recall, the other towards precision at top ranks. The first step computes question-relevant compact subgraphs within the KG, and judiciously enhances them with pertinent temporal facts, using Group Steiner Trees and fine-tuned BERT models. The second step constructs relational graph convolutional networks (R-GCNs) from the first step's output, and enhances the R-GCNs with time-aware entity embeddings and attention over temporal relations. We evaluate EXAQT on TimeQuestions, a large dataset of 16k temporal questions we compiled from a variety of general purpose KG-QA benchmarks. Results show that EXAQT outperforms three state-of-the-art systems for answering complex questions over KGs, thereby justifying specialized treatment of temporal QA.

",A Dataset for Event-Centric Question Answering over Knowledge Graphs,90,0,,,
Question answering,"WorldKG: A World-Scale Geographic Knowledge Graph","Alishiba Dsouza, Nicolas Tempelmeier, Ran Yu, Simon Gottschalk, Elena Demidova

 ","OpenStreetMap is a rich source of openly available geographic information. However, the representation of geographic entities, e.g., buildings, mountains, and cities, within OpenStreetMap is highly heterogeneous, diverse, and incomplete. As a result, this rich data source is hardly usable for real-world applications. This paper presents WorldKG - a new geographic knowledge graph aiming to provide a comprehensive semantic representation of geographic entities in OpenStreetMap. We describe the WorldKG knowledge graph, including its ontology that builds the semantic dataset backbone, the extraction procedure of the ontology and geographic entities from OpenStreetMap, and the methods to enhance entity annotation. We perform statistical and qualitative dataset assessment, demonstrating the large scale and high precision of the semantic geographic information in WorldKG.
",A Dataset for Event-Centric Question Answering over Knowledge Graphs,90,0,,,
Question answering,"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge","Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant","When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",commonsenseqa,91,1,commonsenseqa,"Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant","The CommonsenseQA is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each. The dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):

a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”), the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”) for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually."
Question answering,"Language Models as Knowledge Bases?","Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel","Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as ""fill-in-the-blank"" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at this https URL.
",commonsenseqa,91,0,,,
Question answering,How Can We Know What Language Models Know? ,"Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig","Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.
",commonsenseqa,91,0,,,
Question answering,"Applying Semantic Parsing to Question Answering Over Linked Data: Addressing the Lexical Gap","Sherzod Hakimov, Christina Unger, Sebastian Walter, Philipp Cimiano ","Question answering over linked data has emerged in the past years as an important topic of research in order to provide natural language access to a growing body of linked open data on the Web. In this paper we focus on analyzing the lexical gap that arises as a challenge for any such question answering system. The lexical gap refers to the mismatch between the vocabulary used in a user question and the vocabulary used in the relevant dataset. We implement a semantic parsing approach and evaluate it on the QALD-4 benchmark, showing that the performance of such an approach suffers from training data sparseness. Its performance can, however, be substantially improved if the right lexical knowledge is available. To show this, we model a set of lexical entries by hand to quantify the number of entries that would be needed. Further, we analyze if a state-of-the-art tool for inducing ontology lexica from corpora can derive these lexical entries automatically. We conclude that further research and investments are needed to derive such lexical knowledge automatically or semi-automatically.",Applying Semantic Parsing to Question Answering over Linked Data: Addressing the Lexical Gap - Data,92,1,Applying Semantic Parsing to Question Answering over Linked Data: Addressing the Lexical Gap - Data,"Sherzod Hakimov, Christina Unger, Sebastian Walter, Philipp Cimiano ","Question Answering over Linked Data (QALD) has emerged in the last years as an important topic of research. QALD systems provide access to a growing body of linked open data on the Web for casual end users that are empowered to satisfy their information needs intuitively, using natural language. In this paper we focus on analyzing the infamous lexical gap that arises in any information or question answering system. The lexical gap refers to the problem that there can be a vocabulary mismatch between the vocabulary used in a user question or query and the vocabulary used in the data to describe the relevant information. We build on a semantic parsing approach to QALD and adapt the approach of Zettlemoyer and Collins (2005) to the task at hand. We evaluate our approach on the QALD-4 benchmark and show that performance of a semantic parsing approach can be substantially improved if the right lexical knowledge is available. For this, we model a set of lexical entries by hand to quantify the number of entries that would be needed. Further, we analyze if a state-of-the-art tool for inducing ontology lexical from corpora can derive these lexical entries automatically. We conclude that further research and investments are needed to derive the lexical knowledge automatically or semi-automatically to increase performance of QALD systems.
"
Question answering,"Core techniques of question answering systems over knowledge bases: a survey","Dennis Diefenbach, Vanessa Lopez, Kamal Singh, Pierre Maret ","The Semantic Web contains an enormous amount of information in the form of knowledge bases (KB). To make this information available, many question answering (QA) systems over KBs were created in the last years. Building a QA system over KBs is difficult because there are many different challenges to be solved. In order to address these challenges, QA systems generally combine techniques from natural language processing, information retrieval, machine learning and Semantic Web. The aim of this survey is to give an overview of the techniques used in current QA systems over KBs. We present the techniques used by the QA systems which were evaluated on a popular series of benchmarks: Question Answering over Linked Data. Techniques that solve the same task are first grouped together and then described. The advantages and disadvantages are discussed for each technique. This allows a direct comparison of similar techniques. Additionally, we point to techniques that are used over WebQuestions and SimpleQuestions, which are two other popular benchmarks for QA systems.
",Applying Semantic Parsing to Question Answering over Linked Data: Addressing the Lexical Gap - Data,92,0,,,
Question answering,"A question-entailment approach to question answering","Asma Ben Abacha, Dina Demner-Fushman ","One of the challenges in large-scale information retrieval (IR) is developing fine-grained and domain-specific methods to answer natural language questions. Despite the availability of numerous sources and datasets for answer retrieval, Question Answering (QA) remains a challenging problem due to the difficulty of the question understanding and answer extraction tasks. One of the promising tracks investigated in QA is mapping new questions to formerly answered questions that are “similar”.

Results
We propose a novel QA approach based on Recognizing Question Entailment (RQE) and we describe the QA system and resources that we built and evaluated on real medical questions. First, we compare logistic regression and deep learning methods for RQE using different kinds of datasets including textual inference, question similarity, and entailment in both the open and clinical domains. Second, we combine IR models with the best RQE method to select entailed questions and rank the retrieved answers. To study the end-to-end QA approach, we built the MedQuAD collection of 47,457 question-answer pairs from trusted medical sources which we introduce and share in the scope of this paper. Following the evaluation process used in TREC 2017 LiveQA, we find that our approach exceeds the best results of the medical task with a 29.8% increase over the best official score.
","MedQuAD Dataset",93,1,"MedQuAD Dataset
","Asma Ben Abacha, Dina Demner-Fushman ","MedQuAD includes 47,457 medical question-answer pairs created from 12 NIH websites (e.g. cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). The collection covers 37 question types (e.g. Treatment, Diagnosis, Side Effects) associated with diseases, drugs and other medical entities such as tests.
"
Question answering,"Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering","Asma Ben Abacha, Chaitanya Shivade, Dina Demner-Fushman","This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task. In this paper, we describe the tasks, the datasets, and the participants’ approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.","MedQuAD Dataset",93,0,,,
Question answering,"Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension","Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi","We introduce the task of Multi-Modal Machine Comprehension (M3C), which aims at answering multimodal questions given a context of text, diagrams and images. We present the Textbook Question Answering (TQA) dataset that includes 1,076 lessons and 26,260 multi-modal questions, taken from middle school science curricula. Our analysis shows that a significant portion of questions require complex parsing of the text and the diagrams and reasoning, indicating that our dataset is more complex compared to previous machine comprehension and visual question answering datasets. We extend state-of-the-art methods for textual machine comprehension and visual question answering to the TQA dataset. Our experiments show that these models do not perform well on TQA. The presented dataset opens new challenges for research in question answering and reasoning across multiple modalities.",Textbook Question Answering (TQA),94,1,Textbook Question Answering (TQA),"Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi","The TextbookQuestionAnswering (TQA) dataset is drawn from middle school science curricula. It consists of 1,076 lessons from Life Science, Earth Science and Physical Science textbooks. This includes 26,260 questions, including 12,567 that have an accompanying diagram.

The TQA dataset encourages work on the task of Multi-Modal Machine Comprehension (M3C) task. The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images. The dataset constructed to showcase this task has been built from a middle school science curriculum that pairs a given question to a limited span of knowledge needed to answer it."
Question answering,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge","Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord","We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",Textbook Question Answering (TQA),94,0,,,
Question answering,"WIKIQA: A Challenge Dataset for Open-Domain Question Answering","Yi Yang, Wen-tau Yih, Christopher Meek","We describe the WIKIQA dataset, a new
publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which
includes editor-generated questions and
candidate answer sentences selected by
matching content words in the question.
WIKIQA is constructed using a more natural process and is more than an order of
magnitude larger than the previous dataset.
In addition, the WIKIQA dataset also includes questions for which there are no
correct sentences, enabling researchers to
work on answer triggering, a critical component in any QA system. We compare
several systems on the task of answer sentence selection on both datasets and also
describe the performance of a system on
the problem of answer triggering using the
WIKIQA dataset","WikiQA Dataset",95,1,"WikiQA Dataset
","Yi Yang, Wen-tau Yih, Christopher Meek","The WikiQA corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions.
"
Question answering,"SQuAD: 100,000+ Questions for Machine Comprehension of Text
","Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang
","We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.
","WikiQA Dataset",95,0,,,
Question answering,"QBLink: A Dataset for Sequential Open-Domain Question Answering
","Ahmed Elgohary, Chen Zhao","We introduce QBLink, a new dataset of about 18,000 question sequences, each sequence consists of three naturally occurring human-authored questions (totaling around 56,000 unique questions). The sequences themselves are also naturally occurring (i.e., we do not artificially combine individually-authored questions to form sequences), which allows us to focus more on the important connections between questions that should be incorporated to improve the end-to-end question answering accuracy. QBLink is based on the bonus questions of Quiz Bowl tournaments. Unlike previous work that only uses the starter (or tossup) questions, bonus questions are not interruptable (players always hear the complete question) and have greater variability in difficulty. Bonus questions start with a lead-in text, which sets the stage for the rest of the question, followed by a sequence of related questions.","QBLink: A Dataset for Sequential Open-Domain Question Answering",96,1,"QBLink: A Dataset for Sequential Open-Domain Question Answering
","Ahmed Elgohary, Chen Zhao","We introduce QBLink, a new dataset of about 18,000 question sequences, each sequence consists of three naturally occurring human-authored questions (totaling around 56,000 unique questions). The sequences themselves are also naturally occurring (i.e., we do not artificially combine individually-authored questions to form sequences), which allows us to focus more on the important connections between questions that should be incorporated to improve the end-to-end question answering accuracy. QBLink is based on the bonus questions of Quiz Bowl tournaments. Unlike previous work that only uses the starter (or tossup) questions, bonus questions are not interruptable (players always hear the complete question) and have greater variability in difficulty. Bonus questions start with a lead-in text, which sets the stage for the rest of the question, followed by a sequence of related questions.
"
Question answering,MQALD: Evaluating the impact of modifiers in question answering over knowledge graphs,"Lucia Siciliani, Pierpaolo Basile, Pasquale Lops, Giovanni Semeraro
","Question Answering (QA) over Knowledge Graphs (KG) aims to develop a system that is capable of answering users’ questions using the information coming from one or multiple Knowledge Graphs, like DBpedia, Wikidata, and so on. Question Answering systems need to translate the user’s question, written using natural language, into a query formulated through a specific data query language that is compliant with the underlying KG. This translation process is already non-trivial when trying to answer simple questions that involve a single triple pattern. It becomes even more troublesome when trying to cope with questions that require modifiers in the final query, i.e., aggregate functions, query forms, and so on. The attention over this last aspect is growing but has never been thoroughly addressed by the existing literature. Starting from the latest advances in this field, we want to further step in this direction. This work aims to provide a publicly available dataset designed for evaluating the performance of a QA system in translating articulated questions into a specific data query language. This dataset has also been used to evaluate three QA systems available at the state of the art.

",MQALD,97,1,MQALD,"Lucia Siciliani, Pierpaolo Basile, Pasquale Lops, Giovanni Semeraro
","Question Answering (QA) over Knowledge Graphs (KG) has the aim of developing a system that is capable of answering users' questions using the information coming from one or multiple Knowledge Graphs, like DBpedia, Wikidata and so on.
Question Answering systems need to translate the question of the user, written using natural language, into a query formulated through a specific data query language that is compliant with the underlying KG.
This translation process is already non-trivial when trying to answer simple questions that involve a single triple pattern and becomes even more troublesome when trying to cope with questions that require the presence of modifiers in the final query, i.e. aggregate functions, query forms, and so on.
The attention over this last aspect is growing but has never been thoroughly addressed by the existing literature.
Starting from the latest advances in this field, we want to make a further step towards this direction.
The aim of this work is to provide a publicly available dataset designed for evaluating the performance of a QA system in translating articulated questions into a specific data query language.
This dataset has also been used to evaluate three QA systems available at the state of the art.
"
Question answering,"Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis","Aleksandr Perevalov, Xi Yan, Liubov Kovriguina, Longquan Jiang, Andreas Both, Ricardo Usbeck","Data-driven systems need to be evaluated to establish trust in the scientific approach and its applicability. In particular, this is true for Knowledge Graph (KG) Question Answering (QA), where complex data structures are made accessible via natural-language interfaces. Evaluating the capabilities of these systems has been a driver for the community for more than ten years while establishing different KGQA benchmark datasets. However, comparing different approaches is cumbersome. The lack of existing and curated leaderboards leads to a missing global view over the research field and could inject mistrust into the results. In particular, the latest and most-used datasets in the KGQA community, LC-QuAD and QALD, miss providing central and up-to-date points of trust. In this paper, we survey and analyze a wide range of evaluation results with significant coverage of 100 publications and 98 systems from the last decade. We provide a new central and open leaderboard for any KGQA benchmark dataset as a focal point for the community - this https URL. Our analysis highlights existing problems during the evaluation of KGQA systems. Thus, we will point to possible improvements for future evaluations.",MQALD,97,0,,,
Question answering,"RuBQ 2.0: An Innovated Russian Question Answering Dataset","Ivan Rybin, Vladislav Korablinov, Pavel Efimov,  Pavel Braslavski ","The paper describes the second version of RuBQ, a Russian dataset for knowledge base question answering (KBQA) over Wikidata. Whereas the first version builds on Q&A pairs harvested online, the extension is based on questions obtained through search engine query suggestion services. The questions underwent crowdsourced and in-house annotation in a quite different fashion compared to the first edition. The dataset doubled in size: RuBQ 2.0 contains 2,910 questions along with the answers and SPARQL queries. The dataset also incorporates answer-bearing paragraphs from Wikipedia for the majority of questions. The dataset is suitable for the evaluation of KBQA, machine reading comprehension (MRC), hybrid questions answering, as well as semantic parsing. We provide the analysis of the dataset and report several KBQA and MRC baseline results. The dataset is freely available under the CC-BY-4.0 license.","A Russian Knowledge Base Question Answering Data Set",98,1,"A Russian Knowledge Base Question Answering Data Set
","Valentin Biryukov","RuBQ 1.0: A Russian Knowledge Base Question Answering Data Set
Introduction
We present RuBQ (pronounced [`rubik]) -- Russian Knowledge Base Questions, a KBQA dataset that consists of 1,500 Russian questions of varying complexity along with their English machine translations, corresponding SPARQL queries, answers, as well as a subset of Wikidata covering entities with Russian labels. 300 RuBQ questions are unanswerable, which poses a new challenge for KBQA systems and makes the task more realistic. The dataset is based on a collection of quiz questions. The data generation pipeline combines automatic processing, crowdsourced and in-house verification, see details in the paper. To the best of our knowledge, this is the first Russian KBQA and semantic parsing dataset."
Question answering,"A Survey on non-English Question Answering Dataset","Andreas Chandra, Affandy Fahrizain, Ibrahim, Simon Willyanto Laufried","Research in question answering datasets and models has gained a lot of attention in the research community. Many of them release their own question answering datasets as well as the models. There is tremendous progress that we have seen in this area of research. The aim of this survey is to recognize, summarize and analyze the existing datasets that have been released by many researchers, especially in non-English datasets as well as resources such as research code, and evaluation metrics. In this paper, we review question answering datasets that are available in common languages other than English such as French, German, Japanese, Chinese, Arabic, Russian, as well as the multilingual and cross-lingual question-answering datasets.","A Russian Knowledge Base Question Answering Data Set",98,0,,,
Question answering,"Can Machine Translation be a Reasonable Alternative for Multilingual Question Answering Systems over Knowledge Graphs?","Aleksandr Perevalov, Andreas Both, Dennis Diefenbach, Axel-Cyrille Ngonga Ngomo

","Providing access to information is the main and most important purpose of the Web. However, despite available easy-to-use tools (e.g., search engines, chatbots, question answering) the accessibility is typically limited by the capability of using the English language. This excludes a huge amount of people. In this work, we discuss Knowledge Graph Question Answering (KGQA) systems that aim at providing natural language access to data stored in Knowledge Graphs (KG). While several KGQA systems have been proposed, only very few have dealt with a language other than English. In this work, we follow our research agenda of enabling speakers of any language to access the knowledge stored in KGs. Because of the lack of native support for many languages, we use machine translation (MT) tools to evaluate KGQA systems regarding questions in languages that are unsupported by a KGQA system. In total, our evaluation is based on 8 different languages (including some that never were evaluated before). For the intensive evaluation, we extend the QALD-9 dataset for KGQA with Wikidata queries and high-quality translations. The extension was done in a crowdsourcing manner by native speakers of the different languages. By using multiple KGQA systems for the evaluation, we were enabled to investigate and answer the main research question: “Can MT be an alternative for multilingual KGQA systems?”. The evaluation results demonstrated that the monolingual KGQA systems can be effectively ported to the new languages with MT tools
","A Russian Knowledge Base Question Answering Data Set",98,0,,,
Question answering,"QA4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation","Anselmo Peñas, Eduard Hovy, Pamela Forner, Álvaro Rodrigo, Richard Sutcliffe,  Roser Morante ","This paper describes the methodology for testing the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. This was the attempt of the QA4MRE challenge which was run as a Lab at CLEF 2011–2013. The traditional QA task was replaced by a new Machine Reading task, whose intention was to ask questions that required a deep knowledge of individual short texts and in which systems were required to choose one answer, by analysing the corresponding test document in conjunction with background text collections provided by the organization. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer’s disease, and Entrance Exams. This paper describes their motivation, their goals, their methodology for preparing the data sets, their background collections, their metrics used for the evaluation, and the lessons learned along these three years.",QA4MRE ,99,1,QA4MRE ,"Anselmo Peñas, Eduard Hovy, Pamela Forner, Álvaro Rodrigo, Richard Sutcliffe,  Roser Morante ","QA4MRE dataset was created for the CLEF 2011/2012/2013 shared tasks to promote research in question answering and reading comprehension. The dataset contains a supporting passage and a set of questions corresponding to the passage. Multiple options for answers are provided for each question, of which only one is correct. The training and test datasets are available for the main track. Additional gold standard documents are available for two pilot studies: one on alzheimers data, and the other on entrance exams data.

To use this dataset:

import tensorflow_datasets as tfds

ds = tfds.load('qa4mre', split='train')
for ex in ds.take(4):
 print(ex)
See the guide for more informations on tensorflow_datasets."
Question answering,"Machine Comprehension with Syntax, Frames, and Semantics","Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester","We demonstrate significant improvement
on the MCTest question answering task
(Richardson et al., 2013) by augmenting
baseline features with features based on
syntax, frame semantics, coreference, and
word embeddings, and combining them in
a max-margin learning framework. We
achieve the best results we are aware of on
this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient
for the use of linguistic structure in machine comprehension",QA4MRE ,99,0, ,,
Question answering,"Automatic question generation and answer assessment: a survey
","Bidyut Das, Mukta Majumder, Santanu Phadikar,  Arif Ahmed Sekh ","Learning through the internet becomes popular that facilitates learners to learn anything, anytime, anywhere from the web resources. Assessment is most important in any learning system. An assessment system can find the self-learning gaps of learners and improve the progress of learning. The manual question generation takes much time and labor. Therefore, automatic question generation from learning resources is the primary task of an automated assessment system. This paper presents a survey of automatic question generation and assessment strategies from textual and pictorial learning resources. The purpose of this survey is to summarize the state-of-the-art techniques for generating questions and evaluating their answers automatically.

","30M Factoid Question-Answer Corpus (30MQA)",100,1,"30M Factoid Question-Answer Corpus (30MQA)
","Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio
","The 30M Factoid Question-Answer Corpus consists of 30M natural language questions in English and their corresponding facts in the knowledge base Freebase. The dataset is formatted as a text file, where each line contains:

"
Question answering,"Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension","Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bingsheng Yao, Tongshuang Wu, Zheng Zhang, Toby Jia-Jun Li, Nora Bradford, Branda Sun, Tran Bao Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, Mark Warschauer","Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models' fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.","30M Factoid Question-Answer Corpus (30MQA)",100,0, ,,
Question answering,"Multi-Hop Question Generation Using Hierarchical Encoding-Decoding and Context Switch Mechanism","Tianbo Ji,Chenyang Lyu, Zhichao Cao, Peng Cheng","Neural auto-regressive sequence-to-sequence models have been dominant in text generation tasks, especially the question generation task. However, neural generation models suffer from the global and local semantic semantic drift problems. Hence, we propose the hierarchical encoding–decoding mechanism that aims at encoding rich structure information of the input passages and reducing the variance in the decoding phase. In the encoder, we hierarchically encode the input passages according to its structure at four granularity-levels: [word, chunk, sentence, document]-level. Second, we progressively select the context vector from the document-level representations to the word-level representations at each decoding time step. At each time-step in the decoding phase, we progressively select the context vector from the document-level representations to word-level. We also propose the context switch mechanism that enables the decoder to use the context vector from the last step when generating the current word at each time-step.It provides a means of improving the stability of the text generation process during the decoding phase when generating a set of consecutive words. Additionally, we inject syntactic parsing knowledge to enrich the word representations. Experimental results show that our proposed model substantially improves the performance and outperforms previous baselines according to both automatic and human evaluation. Besides, we implement a deep and comprehensive analysis of generated questions based on their types. View Full-Text","30M Factoid Question-Answer Corpus (30MQA)",100,0,,,
Question answering,"Automated assessment of subjective assignments: A hybrid approach","NaynaBirla, ManojKumar Jain, AvinashPanwar ","Machine learning (ML) has recently gained popularity in a variety of domains for automating tasks. This has also been used in the evaluation of student responses/ assignments. This paper presents a qualitatively enhanced methodology for automated score prediction of subjective assignments. To consider all the major aspects of a human grader, 21 linguistic features related to syntactical, grammatical, sentimental, and readability are analyzed from the assignments. This study makes use of the ASAP competition dataset from Kaggle. The evaluation metric used is Quadratic Weighted Kappa (QWK), which measures the agreement between the human graded score and the predicted score. The effect of appropriate feature selection has been observed using Mutual Information Regression. Four ML algorithms are investigated on identified linguistic features. 3 Layer Neural Network with feature selection performed well among all chosen ML algorithms with average QWK of 0.678. To include the benefits of deep learning, a new hybrid model (LF-BiLSTM-att-FS) is proposed that combines a higher level deep neural network (DNN) with the selected features. Pre-trained Glove embedding is used to include contextual information of the text. The proposed model results demonstrated an improvement in overall accuracy, with an average QWK value of 0.768.

","30M Factoid Question-Answer Corpus (30MQA)",100,0,,,
Question answering,"Increasing Student Engagement in Lessons and Assessing MOOC Participants Through Artificial Intelligence
","Younes-aziz Bachiri,  Hicham Mouncif ","In today’s generation of MOOCs, videos are fundamental to the student learning experience. Due to the prominence of video content in MOOCs, production staff and instructional designers invest significant time and resources in creating these videos. With instructional videos, they want to increase student involvement. Without formative assessment, however, actual engagement is hard to quantify. Nonetheless, a large number of students necessitates a larger pool of questions; to address this issue, we considered mixing machine-learning methods with automatic natural language processing in order to expand the number of questions evaluated and ensure their validity. To accomplish this, we implemented a methodology that generates questions automatically from video transcripts. Following each course comes an evaluation issue, which is typically a multiple-choice question designed to measure a student's comprehension of the video's material. machine-generated questions performed comparably to human-generated questions when it came to judging skill and resemblance. Additionally, the findings indicate that the majority of the questions generated improve e-assessment when the new technology is applied.
","30M Factoid Question-Answer Corpus (30MQA)
",100,0,,,
Question answering,"Parallel Construction: A Parallel Corpus Approach for
Automatic Question Generation in Non-English
Languages","Benny G. Johnson, Jeffrey S. Dittel, Rachel Van Campenhout, Rodrigo Bistolfi, Aida Maeda, Bill Jerome","Automatic question generation (AQG) has many diverse applications
in educational contexts. To bring these benefits to as many students as possible,
it is prudent to expand AQG capabilities in as many languages as possible. However, English remains the dominant language in AQG research, and the required
natural language processing tools for other languages are often under-resourced
relative to English, which can make developing AQG pipelines difficult or impractical altogether. An approach called parallel construction has been developed to leverage existing English AQG systems for AQG in other languages. The
benefits of this parallel construction approach are described, and examples of
questions generated from Spanish and Brazilian Portuguese textbooks using the
parallel construction method are presented and discussed.","30M Factoid Question-Answer Corpus (30MQA)",100,0,,,